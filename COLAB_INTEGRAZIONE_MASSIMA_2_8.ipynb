{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k1lte16eeJzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbb6cf6d-03e5-4c39-d999-7e4c6cfbfcb1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests SPARQLWrapper Flask\n",
        "!pip install transformers torch\n",
        "!pip install nltk transformers\n",
        "!pip install transformers torch nltk\n",
        "!pip install certifi --upgrade\n",
        "!pip install requests SPARQLWrapper Flask urllib3\n",
        "!pip install requests SPARQLWrapper Flask urllib3 pandas\n",
        "!pip install -r requirements.txt\n",
        "!pip install spacy\n",
        "!pip install simplemma stanza\n",
        "!pip install nltk\n",
        "!pip install transformers[torch] tokenizers==0.13.3\n",
        "!pip install transformers torch nltk\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJoVE4minOer",
        "outputId": "382471cc-d349-453e-99d3-c5b3eccf92fe"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.1.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: simplemma in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from stanza) (2.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (4.25.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting tokenizers==0.13.3\n",
            "  Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of transformers[torch] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Using cached transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "  Using cached transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Using cached transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "  Using cached transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Using cached transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "INFO: pip is still looking at multiple versions of transformers[torch] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Using cached transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "  Using cached transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Using cached transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "  Using cached transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "  Using cached transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Using cached transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "Collecting numpy<2.0,>=1.17 (from transformers[torch])\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting transformers[torch]\n",
            "  Using cached transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "  Using cached transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "  Using cached transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "  Using cached transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "  Using cached transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "  Using cached transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
            "  Using cached transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
            "  Using cached transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
            "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "  Using cached transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
            "  Using cached transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
            "  Using cached transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "  Using cached transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
            "  Using cached transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
            "  Using cached transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "  Using cached transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
            "  Using cached transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "  Using cached transformers-4.35.1-py3-none-any.whl.metadata (123 kB)\n",
            "  Using cached transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
            "  Using cached transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
            "  Using cached transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
            "  Using cached transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (3.0.2)\n",
            "Using cached tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "Using cached transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.49.0\n",
            "    Uninstalling transformers-4.49.0:\n",
            "      Successfully uninstalled transformers-4.49.0\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.33.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting transformers\n",
            "  Using cached transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
            "  Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Using cached transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "Using cached tokenizers-0.21.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution ~ransformers (/usr/local/lib/python3.11/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.13.3\n",
            "    Uninstalling tokenizers-0.13.3:\n",
            "      Successfully uninstalled tokenizers-0.13.3\n",
            "Successfully installed tokenizers-0.21.1 transformers-4.49.0\n",
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m49.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting it-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting it-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.8.0/it_core_news_lg-3.8.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m707.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Cloning into 'dbpedia-spotlight-wrapper'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "Cloning into 'dbpedia-spotlight-wrapper'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica se un URI esiste\n",
        "uri = \"http://dbpedia.org/resource/Rome\"\n",
        "exists = wrapper._check_uri_exists(uri)\n",
        "print(f\"L'URI {uri} {'esiste' if exists else 'non esiste'}\")"
      ],
      "metadata": {
        "id": "bwVUyrQw3cnp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ottieni l'URI aggiornato per una label\n",
        "label = \"Roma\"\n",
        "old_uri = \"http://it.dbpedia.org/resource/Roma\"\n",
        "updated_uri = wrapper._get_updated_uri(label, old_uri)\n",
        "print(f\"URI aggiornato: {updated_uri}\")"
      ],
      "metadata": {
        "id": "kpCHc7WM3hct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IKkl226cgUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhq34lPbcgKF",
        "outputId": "24632e2f-5189-44f1-ad33-b5485db921b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting it-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.8.0/it_core_news_lg-3.8.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-lg\n",
            "Successfully installed it-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per mostrare tutte le entità spaCy\n",
        "def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, original_text=None):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Mostra tutte le entità riconosciute da spaCy, non solo quelle uniche.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy da utilizzare (opzionale)\n",
        "        original_text: Testo originale da analizzare con spaCy (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback per compatibilità con la versione precedente\n",
        "        spacy_only_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "\n",
        "        if not spacy_only_entities:\n",
        "            # Se non ci sono entità spaCy uniche, segnala che è necessario passare il riconoscitore\n",
        "            if spacy_recognizer is None or original_text is None:\n",
        "                lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "                lines.append(\"Modificare la chiamata a export_entities_to_txt includendo i parametri spacy_recognizer e original_text.\")\n",
        "            else:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "        else:\n",
        "            # Mostra le entità spaCy uniche\n",
        "            for entity in spacy_only_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Assicurati che questa riga venga eseguita per generare il file\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,  # Passa il riconoscitore spaCy\n",
        "    original_text=text  # Passa il testo originale\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita_italia.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita_italia.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQE5NAFrwAcf",
        "outputId": "54887850-7653-4197-f597-ea1ecb40906d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "\n",
            "Trovate 37 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 37\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "\n",
            "Verifica file: 'entita_italia.txt' esiste e ha dimensione 7612 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy e il riconoscitore italiano.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati del riconoscitore italiano.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi la sezione del riconoscitore italiano ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "        entities,\n",
        "        stats,\n",
        "        \"entita.txt\",\n",
        "        spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "        italian_recognizer=linker.italian_recognizer,\n",
        "        original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "99b2c7037a41432b9c4ca5082c3d7a04",
            "d3086d26b5924bbdae57674d26283897",
            "4c900e21b11b411ab7c455f04e1b85dc",
            "7a5cdc20c07e4816a3a0fd19a61e804b",
            "56047f68127946569aada5452404e4e1",
            "d2d6713ad9e34197b91a68f69132c349",
            "79392c1fd4734882828b6c5da77dd33f",
            "15364e7e9ef242e9b8951c76da22f619",
            "3571070609e241efb626541028b1e09c",
            "ef78ffb6569f44be8b40aed193d3ac2b",
            "46d4463a40ab48749959d0a2f8f0c044",
            "befca2b67b274a72a2f5213ae964fb98",
            "fb8d2a11d3a84dd79a4f3f308aa465d4",
            "857f3bcab111422f9e30442ba135460d",
            "acdc3319110d496ab68ac4b0b9e0d638",
            "727d64886002405a923d4407af0c4209",
            "43fef5bc8a9b4d1abda35a54026e44bb",
            "06315793acb0496b9fe3946783d10b04",
            "d6f61c84b8604421a831513077391927",
            "32b257cc71514839b256ca7e3830f83f",
            "3607370f349249d6b8acfd001ff8b198",
            "909766e612dd453cbfa79cdf15fbb2be",
            "dc2fc4c8b2ab410eb95dfa58bd086335",
            "37eb79cbd8504854b848074be4a59921",
            "f84fab5f290045bbbada5b979cdc3cd5",
            "2016dd4b64694c78a4eb7bc816aed6ad",
            "6a293547cc7b47fcb6c0dab36c12c7c7",
            "a8fe12d941444d04b202ff08e772526f",
            "ef3b30f1c661461d9c63dc9561bcb9ef",
            "bb82a3d5031942ce84a5c714270036b5",
            "3793086f93704ad292403ead367c6073",
            "e80ef0d369f04de199b2352c26b61111",
            "987be07332014d6a8c1a27c07d5e7876"
          ]
        },
        "id": "lBJ_xLa86p0T",
        "outputId": "ad071bd6-4e48-4f49-9f7e-5cec5c5cd4f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99b2c7037a41432b9c4ca5082c3d7a04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/default.zip:   0%|          | …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "befca2b67b274a72a2f5213ae964fb98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc2fc4c8b2ab410eb95dfa58bd086335"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Trovate 38 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 38\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8575 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita_trovate.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Aggiungi questa parte nella funzione export_entities_to_txt\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "    if transformers_entities:\n",
        "        for entity in transformers_entities:\n",
        "            lines.append(f\"- '{entity['text']}' → Etichetta: {entity.get('label', 'N/A')}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "    else:\n",
        "        lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "            # Carica tokenizer e modello\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Crea pipeline NER\n",
        "            self.ner_pipeline = pipeline(\n",
        "                \"ner\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            self.is_available = True\n",
        "            print(f\"Modello Transformer NER caricato: {self.model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"dbmdz/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer'):\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile\")\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo da Transformer NER\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "        print(f\"DEBUG: Riconoscitore Transformer NER ha trovato {len(transformers_entities)} entità nel testo\")\n",
        "\n",
        "        # Stampa dettagli delle entità trovate\n",
        "        for entity in transformers_entities:\n",
        "            print(f\"DEBUG: Entità trovata - Testo: {entity['text']}, Etichetta: {entity['label']}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        # Stampa testi esistenti\n",
        "        print(f\"DEBUG: Testi esistenti: {existing_texts}\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            print(f\"DEBUG: Verifica entità - Testo: {entity_text}, Lowercase: {entity_lower}\")\n",
        "\n",
        "            # Rimuovi la condizione di validità temporaneamente\n",
        "            if entity_lower not in existing_texts:\n",
        "                print(f\"DEBUG: Entità unica dal riconoscitore Transformer NER: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                transformers_type = self._map_transformers_label_to_type(entity['label'])\n",
        "\n",
        "                transformers_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [transformers_type],\n",
        "                    'source': 'transformers_ner',\n",
        "                    'label': entity['label'],\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "                transformers_only.append(transformers_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"DEBUG: Entità Transformer NER uniche trovate: {len(transformers_only)}\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "56fa5b786c9547beae467638a6887b03",
            "f253de36031a487688b8c3394b8fecb3",
            "950a15c315e54f4a8e7d9f26ee9c72f7",
            "914580895a3a4d3c8bdc2216ed9737a3",
            "34aaf234ebba4e4e84d3e49c0f14789d",
            "cfb7edb43a3d4c22ae587cfc90eab650",
            "9e36db42b58b4b8fa1faf1a8310b83a1",
            "ea16871ee4a94f68aeab9203eea49451",
            "6875c961e6b142f7842ba4c5869e2d25",
            "ecee372f9da74648b0d002e985f28a72",
            "289ef29f74814dc1aac2908cd6759870",
            "e25e37f7743148909e366968eed138f8",
            "421fd2ada2214240810db5d0e431e451",
            "6233535aaeab483b8f7023b51457d184",
            "4b5b13c7cbab40429f32ddaed90de202",
            "54b605a2890f4be1b855797f7efdb598",
            "db9dddea903d430184b57088cae426b2",
            "998c3e99bb254ff4bb5087c66aee0b95",
            "68042e42b1e44e65a8627da0aec3f560",
            "c9a99e66755c4c11a74e253389dd7c9d",
            "4824d4d85ebb4b13a244df954788e965",
            "9560df99c2094f7cb63dd5975c69124f"
          ]
        },
        "id": "DrgAp6oKnT5f",
        "outputId": "38bb9cf6-d0fb-4d7b-9add-7f822420e787"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56fa5b786c9547beae467638a6887b03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e25e37f7743148909e366968eed138f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errore nel caricamento del modello Transformer NER: dbmdz/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Assicurati di avere installato transformers e torch.\n",
            "Prova a installare i modelli con:\n",
            "pip install transformers torch\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: False\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Riconoscitore Transformer NER non disponibile\n",
            "\n",
            "Trovate 38 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: False\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Riconoscitore Transformer NER non disponibile\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 38\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8688 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Lista di modelli italiani alternativi\n",
        "            alternative_models = [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
        "            ]\n",
        "\n",
        "            # Prova a caricare uno dei modelli\n",
        "            for model_name in alternative_models:\n",
        "                try:\n",
        "                    # Determina il device\n",
        "                    device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "                    # Carica tokenizer e modello\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "                    # Crea pipeline NER\n",
        "                    self.ner_pipeline = pipeline(\n",
        "                        \"ner\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    self.is_available = True\n",
        "                    self.model_name = model_name\n",
        "                    print(f\"Modello Transformer NER caricato: {model_name}\")\n",
        "                    return\n",
        "\n",
        "                except Exception as inner_e:\n",
        "                    print(f\"Tentativo fallito con modello {model_name}: {inner_e}\")\n",
        "\n",
        "            # Se nessun modello funziona\n",
        "            raise Exception(\"Nessun modello NER disponibile\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"dbmdz/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer'):\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile\")\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo da Transformer NER\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "        print(f\"DEBUG: Riconoscitore Transformer NER ha trovato {len(transformers_entities)} entità nel testo\")\n",
        "\n",
        "        # Stampa dettagli delle entità trovate\n",
        "        for entity in transformers_entities:\n",
        "            print(f\"DEBUG: Entità trovata - Testo: {entity['text']}, Etichetta: {entity['label']}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        # Stampa testi esistenti\n",
        "        print(f\"DEBUG: Testi esistenti: {existing_texts}\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            print(f\"DEBUG: Verifica entità - Testo: {entity_text}, Lowercase: {entity_lower}\")\n",
        "\n",
        "            # Rimuovi la condizione di validità temporaneamente\n",
        "            if entity_lower not in existing_texts:\n",
        "                print(f\"DEBUG: Entità unica dal riconoscitore Transformer NER: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                transformers_type = self._map_transformers_label_to_type(entity['label'])\n",
        "\n",
        "                transformers_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [transformers_type],\n",
        "                    'source': 'transformers_ner',\n",
        "                    'label': entity['label'],\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "                transformers_only.append(transformers_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"DEBUG: Entità Transformer NER uniche trovate: {len(transformers_only)}\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "125a82f05f1d446a9acbc905b5920f2d",
            "376f7c94992a4c0d8c30132ff6b60582",
            "bfdd0cd7d31a493bbba82ee1c915c322",
            "cad65ef1c7e04c878a61fdd3738f72f9",
            "03858db06c134d53bb64d57ebf41061f",
            "f6a7275240914264a137e6fd494494af",
            "6f5f418350f24b1c93a8d40940e9631a",
            "45dd27ca7da64abba701bac781ecb2b2",
            "7292ec3cc67e478c87a78d50cd20210d",
            "c0e792397b884b94a60ae279896e4744",
            "b4665fa2a28041558db4578cd0ccacc6",
            "d7628c1a518e448cb3b6831288bc118a",
            "8350b5345a924735ba99c6ad08573790",
            "f4aee7509d4843438d5b1dc129c87ba1",
            "80cae6ab93784136bf089d82493e5dc4",
            "4eaeabcf6c174d728bb9ecb7cebfdc7e",
            "c713c7a97b4f49cf9f1ff5fd57a38c1a",
            "7bf31623893a4b5ba6858456abc1b43c",
            "895f8bbee858458b8911b61ae1a44f86",
            "38ab564917a24a22967b1ff42e6858a6",
            "8968f10d49b0492da351dfecff981720",
            "f90521b4fed84656becb8e3770be405c"
          ]
        },
        "id": "F_TV5LftrL_o",
        "outputId": "7a4d807f-fadd-4492-8f48-0b28049eb919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "125a82f05f1d446a9acbc905b5920f2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7628c1a518e448cb3b6831288bc118a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "Tentativo fallito con modello 5had3/bert-base-italian-cased-ner: 5had3/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Tentativo fallito con modello MilaNLProc/bert-italian-cased-ner: MilaNLProc/bert-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello Transformer NER caricato: Davlan/bert-base-multilingual-cased-ner-hrl\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Riconoscitore Transformer NER ha trovato 17 entità nel testo\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998433589935303\n",
            "DEBUG: Entità trovata - Testo: Roma, Etichetta: LOC, Punteggio: 0.9995030164718628\n",
            "DEBUG: Entità trovata - Testo: Vaticano, Etichetta: LOC, Punteggio: 0.6520248651504517\n",
            "DEBUG: Entità trovata - Testo: Firenze, Etichetta: LOC, Punteggio: 0.9997918009757996\n",
            "DEBUG: Entità trovata - Testo: Milano, Etichetta: LOC, Punteggio: 0.9998107552528381\n",
            "DEBUG: Entità trovata - Testo: Napoli, Etichetta: LOC, Punteggio: 0.9998220801353455\n",
            "DEBUG: Entità trovata - Testo: Sapienza, Etichetta: ORG, Punteggio: 0.9991324543952942\n",
            "DEBUG: Entità trovata - Testo: PolitecnicodiMilano, Etichetta: ORG, Punteggio: 0.9991716146469116\n",
            "DEBUG: Entità trovata - Testo: Europa, Etichetta: LOC, Punteggio: 0.9998190999031067\n",
            "DEBUG: Entità trovata - Testo: Ferrari, Etichetta: ORG, Punteggio: 0.5556326508522034\n",
            "DEBUG: Entità trovata - Testo: Lamborghini, Etichetta: ORG, Punteggio: 0.665400505065918\n",
            "DEBUG: Entità trovata - Testo: Mediterraneo, Etichetta: LOC, Punteggio: 0.9982662796974182\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998801946640015\n",
            "DEBUG: Entità trovata - Testo: DanteAlighieri, Etichetta: PER, Punteggio: 0.999840497970581\n",
            "DEBUG: Entità trovata - Testo: LeonardodaVinci, Etichetta: PER, Punteggio: 0.9998635053634644\n",
            "DEBUG: Entità trovata - Testo: GalileoGalilei, Etichetta: PER, Punteggio: 0.9998313188552856\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998799562454224\n",
            "DEBUG: Testi esistenti: {'mediterraneo', 'dante', 'galileo', 'galilei', 'italia', 'sapienza', 'europa', 'vaticano', 'pizza', 'firenze', 'politecnico', 'alighieri', 'galileo galilei', 'napoli', 'il mediterraneo', 'vinci', 'colosseo', 'dante alighieri', 'la ferrari', 'barolo', 'milano', 'la sapienza', 'leonardo', 'ferrari', 'roma', 'politecnico di milano', 'chianti', 'rinascimento', 'lamborghini', 'vesuvio', 'leonardo da vinci'}\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: Roma, Lowercase: roma\n",
            "DEBUG: Verifica entità - Testo: Vaticano, Lowercase: vaticano\n",
            "DEBUG: Verifica entità - Testo: Firenze, Lowercase: firenze\n",
            "DEBUG: Verifica entità - Testo: Milano, Lowercase: milano\n",
            "DEBUG: Verifica entità - Testo: Napoli, Lowercase: napoli\n",
            "DEBUG: Verifica entità - Testo: Sapienza, Lowercase: sapienza\n",
            "DEBUG: Verifica entità - Testo: PolitecnicodiMilano, Lowercase: politecnicodimilano\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: PolitecnicodiMilano (ORG)\n",
            "DEBUG: Verifica entità - Testo: Europa, Lowercase: europa\n",
            "DEBUG: Verifica entità - Testo: Ferrari, Lowercase: ferrari\n",
            "DEBUG: Verifica entità - Testo: Lamborghini, Lowercase: lamborghini\n",
            "DEBUG: Verifica entità - Testo: Mediterraneo, Lowercase: mediterraneo\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: DanteAlighieri, Lowercase: dantealighieri\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: DanteAlighieri (PER)\n",
            "DEBUG: Verifica entità - Testo: LeonardodaVinci, Lowercase: leonardodavinci\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: LeonardodaVinci (PER)\n",
            "DEBUG: Verifica entità - Testo: GalileoGalilei, Lowercase: galileogalilei\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: GalileoGalilei (PER)\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Entità Transformer NER uniche trovate: 4\n",
            "DEBUG: Entità Transformer NER trovate: 4\n",
            "\n",
            "Trovate 42 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "- 'PolitecnicodiMilano' → N/A\n",
            "  Tipo: Organization\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'DanteAlighieri' → N/A\n",
            "  Tipo: Person\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'LeonardodaVinci' → N/A\n",
            "  Tipo: Person\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'GalileoGalilei' → N/A\n",
            "  Tipo: Person\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Riconoscitore Transformer NER ha trovato 17 entità nel testo\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998433589935303\n",
            "DEBUG: Entità trovata - Testo: Roma, Etichetta: LOC, Punteggio: 0.9995030164718628\n",
            "DEBUG: Entità trovata - Testo: Vaticano, Etichetta: LOC, Punteggio: 0.6520248651504517\n",
            "DEBUG: Entità trovata - Testo: Firenze, Etichetta: LOC, Punteggio: 0.9997918009757996\n",
            "DEBUG: Entità trovata - Testo: Milano, Etichetta: LOC, Punteggio: 0.9998107552528381\n",
            "DEBUG: Entità trovata - Testo: Napoli, Etichetta: LOC, Punteggio: 0.9998220801353455\n",
            "DEBUG: Entità trovata - Testo: Sapienza, Etichetta: ORG, Punteggio: 0.9991324543952942\n",
            "DEBUG: Entità trovata - Testo: PolitecnicodiMilano, Etichetta: ORG, Punteggio: 0.9991716146469116\n",
            "DEBUG: Entità trovata - Testo: Europa, Etichetta: LOC, Punteggio: 0.9998190999031067\n",
            "DEBUG: Entità trovata - Testo: Ferrari, Etichetta: ORG, Punteggio: 0.5556326508522034\n",
            "DEBUG: Entità trovata - Testo: Lamborghini, Etichetta: ORG, Punteggio: 0.665400505065918\n",
            "DEBUG: Entità trovata - Testo: Mediterraneo, Etichetta: LOC, Punteggio: 0.9982662796974182\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998801946640015\n",
            "DEBUG: Entità trovata - Testo: DanteAlighieri, Etichetta: PER, Punteggio: 0.999840497970581\n",
            "DEBUG: Entità trovata - Testo: LeonardodaVinci, Etichetta: PER, Punteggio: 0.9998635053634644\n",
            "DEBUG: Entità trovata - Testo: GalileoGalilei, Etichetta: PER, Punteggio: 0.9998313188552856\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998799562454224\n",
            "DEBUG: Testi esistenti: {'mediterraneo', 'dante', 'galileo', 'galilei', 'italia', 'sapienza', 'europa', 'vaticano', 'pizza', 'firenze', 'politecnico', 'alighieri', 'galileo galilei', 'napoli', 'il mediterraneo', 'vinci', 'colosseo', 'dante alighieri', 'la ferrari', 'barolo', 'milano', 'la sapienza', 'leonardo', 'ferrari', 'roma', 'politecnico di milano', 'chianti', 'rinascimento', 'lamborghini', 'vesuvio', 'leonardo da vinci'}\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: Roma, Lowercase: roma\n",
            "DEBUG: Verifica entità - Testo: Vaticano, Lowercase: vaticano\n",
            "DEBUG: Verifica entità - Testo: Firenze, Lowercase: firenze\n",
            "DEBUG: Verifica entità - Testo: Milano, Lowercase: milano\n",
            "DEBUG: Verifica entità - Testo: Napoli, Lowercase: napoli\n",
            "DEBUG: Verifica entità - Testo: Sapienza, Lowercase: sapienza\n",
            "DEBUG: Verifica entità - Testo: PolitecnicodiMilano, Lowercase: politecnicodimilano\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: PolitecnicodiMilano (ORG)\n",
            "DEBUG: Verifica entità - Testo: Europa, Lowercase: europa\n",
            "DEBUG: Verifica entità - Testo: Ferrari, Lowercase: ferrari\n",
            "DEBUG: Verifica entità - Testo: Lamborghini, Lowercase: lamborghini\n",
            "DEBUG: Verifica entità - Testo: Mediterraneo, Lowercase: mediterraneo\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: DanteAlighieri, Lowercase: dantealighieri\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: DanteAlighieri (PER)\n",
            "DEBUG: Verifica entità - Testo: LeonardodaVinci, Lowercase: leonardodavinci\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: LeonardodaVinci (PER)\n",
            "DEBUG: Verifica entità - Testo: GalileoGalilei, Lowercase: galileogalilei\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: GalileoGalilei (PER)\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Entità Transformer NER uniche trovate: 4\n",
            "DEBUG: Entità Transformer NER trovate: 4\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 42\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- transformers_only_entities: 4\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8731 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita_trovate.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Aggiungi questa parte nella funzione export_entities_to_txt\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "    if transformers_entities:\n",
        "        for entity in transformers_entities:\n",
        "            lines.append(f\"- '{entity['text']}' → Etichetta: {entity.get('label', 'N/A')}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "    else:\n",
        "        lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "            # Carica tokenizer e modello\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Crea pipeline NER\n",
        "            self.ner_pipeline = pipeline(\n",
        "                \"ner\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            self.is_available = True\n",
        "            print(f\"Modello Transformer NER caricato: {self.model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"dbmdz/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo da Transformer NER\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore Transformer NER ha trovato {len(transformers_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore Transformer NER: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                transformers_type = self._map_transformers_label_to_type(entity['label'])\n",
        "\n",
        "                transformers_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [transformers_type],\n",
        "                    'source': 'transformers_ner',\n",
        "                    'label': entity['label'],\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "                transformers_only.append(transformers_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(transformers_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "33c27647f7e74193a37d241be940571b",
            "462f06ca806e4365a30e2fcb079cf026",
            "ff5adce707d34e948e8aec223b80c23b",
            "7cb541b4f6024f2e966a8e37a5cd093c",
            "538ccc648a2f40228e1e588a6249fd87",
            "c6e6b85d44d940848c9d95cc24f814dd",
            "660b6030ddfa4acd806517b4a8ba2da9",
            "c3c37a4607044b4998041e6a55cbba1c",
            "ac5636907393468d90ae5ca40c9b67cb",
            "6f925dc41b3641a5ad94244732dad5cc",
            "43a347010fc745c7a13638d4a53fbf95",
            "9dd45e1efb824a9fa64eed0a2cb28683",
            "734d841bcfe944bfa28d479df00b624b",
            "3269d9ef5be0400da3d8f232c799c7b6",
            "3c5ba092d043479d843c97314af67e4c",
            "effb0092e34941188768b5a8765ec472",
            "584be2e248a94313b814df876b1e32e1",
            "c3a15d887b38402c8a84769b0d9663cc",
            "3508ede777ae4e11a83ff6e80415a3fd",
            "9ad5e73b7cef402e96a15f5b36bb4c51",
            "9bb827146b8649719ca3a0961e997ced",
            "d026a7d56aac4d549e8c900b8e789975"
          ]
        },
        "id": "VR0ujPN4A2O2",
        "outputId": "b56655d3-d51c-44b1-a19f-166e629706b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33c27647f7e74193a37d241be940571b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dd45e1efb824a9fa64eed0a2cb28683"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "Errore nel caricamento del modello Transformer NER: dbmdz/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Assicurati di avere installato transformers e torch.\n",
            "Prova a installare i modelli con:\n",
            "pip install transformers torch\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Trovate 38 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 38\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8688 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica caricamento del modello\n",
        "recognizer = TransformersNERRecognizer(language=\"it\", use_gpu=False)\n",
        "\n",
        "# Testo di test più esplicito\n",
        "test_text = \"Mario Rossi lavora per la Banca d'Italia a Roma e ha fondato una startup tecnologica a Milano.\"\n",
        "entities = recognizer.recognize_entities(test_text)\n",
        "\n",
        "print(\"Entità trovate:\")\n",
        "for entity in entities:\n",
        "    print(f\"- {entity['text']} (Tipo: {entity['label']}, Punteggio: {entity['score']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "c90dde9098534fea853d095ccb84eb19",
            "0c1dc55bdcdf4e41b5d013902b8e9d39",
            "403c1e63a4ea437f99d9ec89f11cc0a3",
            "d96bf999477e4ef9b8bc234b98145c8a",
            "a5433829f47442fd94c5684af30193aa",
            "eae40eb33c5a470c88b258d55b1c813f",
            "5a332f3c39294dd1a6e630e98801cc05",
            "f6ab92969e164615b1ee83b07e712ab6",
            "698382eaa87749a6bdebfdc5b1b4750b",
            "fcbbba22dd4f488fa17c3833095487ef",
            "183fd4438f53425fb48e09b774b659f6",
            "4f3a66cfa9284ebaaa9b725a3c441481",
            "76f6bff5db694fc199a9cac79e9ee059",
            "0adcacf5046248e2a2755ccfae8a81dd",
            "329e85b08c974eab8489919c4f19f779",
            "a3e64a9a84374c6aa882c3d392787de4",
            "ece0841d4413440882c3f9ade6c96aa5",
            "a411a57a8c664ab292ba61db4b1c8b97",
            "6f1f94c3f06249a5a5ddba8612a4540f",
            "e62bc58dc5314d8f8ce3d281414f5ef6",
            "29961dda9b2340c2910dae9a7362ef95",
            "45f7c233516c44ed92fdf1b827824b5f",
            "52edbf406d994662a0f9b2d502c50e05",
            "25fb7569935d45c492b1e40f42beb924",
            "4499685d5da4456dba01c7ba715fb2fd",
            "9a2ae8819e1847ad86ca1e2b6854fa63",
            "ac7c72e2e80547dc983b8b525959001f",
            "ff9cdc3ea5324b0fb6ea24ac464918af",
            "357d96cf6a8b49d6835b3c3561405289",
            "3a48d2f8041b4548bd144c53d5388615",
            "e50f2ccb731f49079f1e5686f044b514",
            "2355602a97024827b88855f72f8b0999",
            "f9073c358088495c8421f0052e52445f",
            "ba1b64a2f4214ebdb9fd8e6656f20c95",
            "5d7b2bc39f014c9c871bddbd5270a21a",
            "223bf4622ffa492c985d665cf36cc0dd",
            "0260d2c7f71149bd88c35a938eda887d",
            "85ff0058b3f842608d587c28b68662c8",
            "5bff531258344601927d91bca35dd246",
            "351680af59cd420da6f79816f68b80e4",
            "99173a3ae2024f5982ac860d30c9402d",
            "a312de900c68435d9a6e2e6ffc7c4936",
            "9fbf602a21744e188a063083a75ba833",
            "130db07b81e64547b13fb78641f1fb31",
            "7a198d9f0cac418389ef3405d35a3fb1",
            "92d5f9952d9b4688ab646d0f630c67f6",
            "7ec6c714d5834b5f990bbd8d561e1943",
            "a8cb44ae119c44be807fc3acac3921dd",
            "5adae7dcf5d54bdb97edd4e549deb2f9",
            "e9484de7528449058d309e1e0def2999",
            "157039af7a8d44299f97f27dd8ccd7f1",
            "0d92e96f491b442194aa70e8a6b4ccee",
            "664d04ccfef8460984418f04e71a9007",
            "c17b7744158a448896aad8812b327bda",
            "52931570e243479896f8f791693f9bf8",
            "dda7a5a2a3c04dc4aed55cc9bc7bd5db",
            "19f96645145c46de863d24f62802ee89",
            "45c4627054544a7abe82a232dbbdb270",
            "cb5596c133094bbb9518ba4754fd5844",
            "293b590c9d10454f89c619c9ea7a032d",
            "ef39645116f2433dafbdc067f58cb9e2",
            "2a4498b52be64694b221a361a7a88b8d",
            "4bff3ae090b442f68932a09076df7a66",
            "533fce5ea7324727ade621adb06f4855",
            "6e590f0043064a06b7e1bda995b6336d",
            "6111b1be06644e06b1e798e23e804a9d"
          ]
        },
        "id": "fO36zAXeNZae",
        "outputId": "76be2b16-8ca1-4a60-c7d7-2c677f5e8bc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c90dde9098534fea853d095ccb84eb19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f3a66cfa9284ebaaa9b725a3c441481"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52edbf406d994662a0f9b2d502c50e05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba1b64a2f4214ebdb9fd8e6656f20c95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a198d9f0cac418389ef3405d35a3fb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dda7a5a2a3c04dc4aed55cc9bc7bd5db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello Transformer NER caricato: dslim/bert-base-NER\n",
            "Entità trovate:\n",
            "- MarioRossi (Tipo: PER, Punteggio: 0.9997310042381287)\n",
            "- Roma (Tipo: LOC, Punteggio: 0.9841041564941406)\n",
            "- Milano (Tipo: LOC, Punteggio: 0.9881179928779602)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers[torch] tokenizers==0.13.3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qU02BL_FX69l",
        "outputId": "12232f33-630c-47b2-ecb1-d102f4822382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tokenizers==0.13.3\n",
            "  Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.32.3)\n",
            "INFO: pip is looking at multiple versions of transformers[torch] to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting transformers[torch]\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.48.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.46.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is still looking at multiple versions of transformers[torch] to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading transformers-4.46.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (0.5.3)\n",
            "  Downloading transformers-4.46.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.1/44.1 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.2-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.45.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading transformers-4.44.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.44.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.44.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.43.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.4-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0,>=1.17 (from transformers[torch])\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers[torch]\n",
            "  Downloading transformers-4.42.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.42.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.2-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.1-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.41.0-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.2-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.0/138.0 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.40.0-py3-none-any.whl.metadata (137 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.6/137.6 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.3-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.2-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.1-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.39.0-py3-none-any.whl.metadata (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.7/130.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.1-py3-none-any.whl.metadata (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.2-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.1-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.37.0-py3-none-any.whl.metadata (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.4/129.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.36.2-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.36.1-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.36.0-py3-none-any.whl.metadata (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.5/123.5 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.1-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.35.0-py3-none-any.whl.metadata (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.1/123.1 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.34.1-py3-none-any.whl.metadata (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.34.0-py3-none-any.whl.metadata (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.5/121.5 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading transformers-4.33.3-py3-none-any.whl.metadata (119 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (4.67.1)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.10 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.11/dist-packages (from transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.20.3->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers[torch]) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers[torch]) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (3.0.2)\n",
            "Downloading tokenizers-0.13.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m62.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading transformers-4.33.3-py3-none-any.whl (7.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tokenizers, transformers\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.21.1\n",
            "    Uninstalling tokenizers-0.21.1:\n",
            "      Successfully uninstalled tokenizers-0.21.1\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sentence-transformers 3.4.1 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.33.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed tokenizers-0.13.3 transformers-4.33.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tokenizers",
                  "transformers"
                ]
              },
              "id": "4e13f1d798a741c0b200280ca4c37cb8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,\n",
        "    original_text=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy, dal riconoscitore italiano e dal riconoscitore Transformer NER.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        transformers_recognizer: Riconoscitore Transformer NER (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']  # Assicurati che questa riga non sia commentata\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in entities:  # Iterate through all entities\n",
        "        if entity.get('source') in ['wikidata', 'wikidata+spacy']:  # Check if the entity is from Wikidata\n",
        "            lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")  # Access Wikidata-specific attributes only if present\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore Transformer e un testo originale, ottieni tutte le entità\n",
        "    if transformers_recognizer and original_text and hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "        try:\n",
        "            print(\"DEBUG: Tentativo di ottenere entità dal riconoscitore Transformer\")\n",
        "            transformers_all_entities = transformers_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not transformers_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "            else:\n",
        "                print(f\"DEBUG: Trovate {len(transformers_all_entities)} entità da Transformer\")\n",
        "                # Rimuovi duplicati mantenendo l'ordine originale\n",
        "                unique_transformers_entities = []\n",
        "                seen = set()\n",
        "                for entity in transformers_all_entities:\n",
        "                    if entity['text'] not in seen:\n",
        "                        unique_transformers_entities.append(entity)\n",
        "                        seen.add(entity['text'])\n",
        "\n",
        "                for entity in unique_transformers_entities:\n",
        "                    # Usa il primo tipo disponibile o 'N/A'\n",
        "                    label = entity.get('label', entity.get('types', ['N/A'])[0] if entity.get('types') else 'N/A')\n",
        "                    score = entity.get('score', 'N/A')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore Transformer: {e}\")\n",
        "            print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità Transformer già filtrate nel metodo find_entities\n",
        "        if not transformers_entities:\n",
        "            if not transformers_recognizer:\n",
        "                lines.append(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            elif not hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "                lines.append(\"Il riconoscitore Transformer NER non ha il metodo recognize_entities.\")\n",
        "            else:\n",
        "                lines.append(\"Per mostrare tutte le entità del riconoscitore Transformer, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Usando {len(transformers_entities)} entità Transformer già filtrate\")\n",
        "            for entity in transformers_entities:\n",
        "                label = entity.get('label', 'N/A')\n",
        "                score = entity.get('score', 'N/A')\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Lista di fallback globale (modelli che funzionano in molte lingue)\n",
        "        self.fallback_models = [\n",
        "            \"dslim/bert-base-NER\",               # Ottimo supporto multilingua\n",
        "            \"dbmdz/bert-large-cased-finetuned-conll03-english\",  # Molto robusto\n",
        "            \"Babelscape/wikineural-multilingual-ner\"  # Supporto multilingua avanzato\n",
        "        ]\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG: Selezionando modello per lingua: {self.language}\")\n",
        "\n",
        "        # Prova prima i modelli specifici per la lingua\n",
        "        models = self.language_models.get(self.language, self.language_models.get(\"en\", []))\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Importa solo il tokenizer per verificare la disponibilità\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Verifica disponibilità modello: {model}\")\n",
        "\n",
        "                # Verifica la disponibilità con caricamento offline\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=False)\n",
        "                print(f\"DEBUG: Modello {model} disponibile e selezionato\")\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Modello {model} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Se nessun modello specifico per lingua funziona, prova i fallback\n",
        "        print(\"DEBUG: Nessun modello specifico per lingua disponibile, provo fallback globali\")\n",
        "        for fallback in self.fallback_models:\n",
        "            try:\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Provo fallback modello: {fallback}\")\n",
        "                tokenizer = AutoTokenizer.from_pretrained(fallback, local_files_only=False)\n",
        "                print(f\"DEBUG: Fallback {fallback} disponibile e selezionato\")\n",
        "                return fallback\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Fallback {fallback} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Ultimo tentativo: dslim/bert-base-NER (molto comune)\n",
        "        print(\"DEBUG: Ultimo tentativo con dslim/bert-base-NER\")\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Assicurati che le dipendenze siano disponibili\n",
        "            import torch\n",
        "            from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "            print(f\"DEBUG: Tentativo di caricare il modello: {self.model_name}\")\n",
        "\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "            print(f\"DEBUG: Usando device {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "            # Prova a caricare con questo modello specifico\n",
        "            try:\n",
        "                # Carica tokenizer e modello\n",
        "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "                model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "                # Crea pipeline NER\n",
        "                self.ner_pipeline = pipeline(\n",
        "                    \"ner\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                self.is_available = True\n",
        "                print(f\"DEBUG: Modello {self.model_name} caricato con successo!\")\n",
        "\n",
        "                # Test rapido per verificare il funzionamento\n",
        "                test_text = \"Mario Rossi vive a Roma.\"\n",
        "                test_result = self.ner_pipeline(test_text)\n",
        "                print(f\"DEBUG: Test modello superato - entità trovate: {len(test_result)}\")\n",
        "                return\n",
        "\n",
        "            except Exception as model_error:\n",
        "                print(f\"DEBUG: Errore nel caricamento del modello specifico: {str(model_error)}\")\n",
        "\n",
        "                # Se il modello specifico fallisce, prova i modelli di fallback dalla lista\n",
        "                for fallback in self.fallback_models:\n",
        "                    if fallback != self.model_name:  # Evita di riprovare lo stesso modello\n",
        "                        try:\n",
        "                            print(f\"DEBUG: Provo modello di fallback: {fallback}\")\n",
        "                            tokenizer = AutoTokenizer.from_pretrained(fallback)\n",
        "                            model = AutoModelForTokenClassification.from_pretrained(fallback)\n",
        "\n",
        "                            self.ner_pipeline = pipeline(\n",
        "                                \"ner\",\n",
        "                                model=model,\n",
        "                                tokenizer=tokenizer,\n",
        "                                device=device\n",
        "                            )\n",
        "\n",
        "                            self.is_available = True\n",
        "                            self.model_name = fallback  # Aggiorna il nome del modello\n",
        "                            print(f\"DEBUG: Modello fallback {fallback} caricato con successo!\")\n",
        "                            return\n",
        "\n",
        "                        except Exception as fallback_error:\n",
        "                            print(f\"DEBUG: Fallback {fallback} fallito: {str(fallback_error)}\")\n",
        "\n",
        "                # Se tutti i fallback falliscono, solleva l'errore originale\n",
        "                raise model_error\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: Errore di importazione: {str(e)}\")\n",
        "            print(\"È necessario installare 'transformers' e 'torch':\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generale nel caricamento del modello: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> list:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            print(f\"DEBUG: Analisi testo con modello {self.model_name}\")\n",
        "\n",
        "            # Esegui NER con gestione di testi lunghi\n",
        "            # Alcuni modelli hanno limiti sulla lunghezza del testo\n",
        "            if len(text) > 500:\n",
        "                # Dividi il testo in blocchi di 500 caratteri con sovrapposizione\n",
        "                chunks = []\n",
        "                chunk_size = 450  # Usa 450 per avere sovrapposizione\n",
        "                for i in range(0, len(text), chunk_size):\n",
        "                    # Assicurati che l'ultimo chunk non vada oltre la fine del testo\n",
        "                    end = min(i + chunk_size + 50, len(text))  # +50 per sovrapposizione\n",
        "                    chunks.append(text[i:end])\n",
        "\n",
        "                print(f\"DEBUG: Testo diviso in {len(chunks)} chunks per l'analisi\")\n",
        "\n",
        "                # Analizza ogni chunk separatamente\n",
        "                all_results = []\n",
        "                for i, chunk in enumerate(chunks):\n",
        "                    print(f\"DEBUG: Analisi chunk {i+1}/{len(chunks)}\")\n",
        "                    chunk_results = self.ner_pipeline(chunk)\n",
        "                    # Aggiungi offset per i chunk successivi al primo\n",
        "                    if i > 0:\n",
        "                        offset = i * chunk_size\n",
        "                        for result in chunk_results:\n",
        "                            result['start'] += offset\n",
        "                            result['end'] += offset\n",
        "                    all_results.extend(chunk_results)\n",
        "\n",
        "                ner_results = all_results\n",
        "            else:\n",
        "                # Per testi brevi, usa direttamente il pipeline\n",
        "                ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            print(f\"DEBUG: Trovati {len(ner_results)} token di entità\")\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],  # Rimuovi il prefisso \"B-\"\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente se il tipo corrisponde\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        # Gestione speciale per i token tokenizzati che iniziano con ##\n",
        "                        if result['word'].startswith('##'):\n",
        "                            current_entity['text'] += result['word'][2:]  # Rimuovi ##\n",
        "                        else:\n",
        "                            # Potrebbe essere necessario aggiungere uno spazio\n",
        "                            space_needed = not (result['start'] == current_entity['end_char'])\n",
        "                            if space_needed:\n",
        "                                current_entity['text'] += ' ' + result['word']\n",
        "                            else:\n",
        "                                current_entity['text'] += result['word']\n",
        "\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        # Aggiorna lo score con la media o il massimo\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                elif result['entity'] != 'O':  # Altra etichetta (non \"Other\")\n",
        "                    # Gestisci entità che non hanno markup B-/I-\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    # Crea una nuova entità\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'].replace('B-', '').replace('I-', ''),\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                else:  # Token classificato come \"O\" (Other)\n",
        "                    # Finalizza l'entità corrente, se esiste\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi l'ultima entità in elaborazione\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            print(f\"DEBUG: Raggruppate in {len(grouped_entities)} entità\")\n",
        "\n",
        "            # Aggiungi source e crea lista finale\n",
        "            for entity in grouped_entities:\n",
        "                # Pulizia finale del testo (rimuovi spazi extra)\n",
        "                entity['text'] = entity['text'].strip()\n",
        "                # Aggiungi la fonte\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "            # Aggiungi post-processing alle entità\n",
        "            print(f\"DEBUG: Applicazione post-processing a {len(entities)} entità\")\n",
        "            entities = self.process_entities(entities, text)\n",
        "            return entities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nel riconoscimento entità: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return []\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, pipeline  # Verifica disponibilità di transformers\n",
        "            import torch  # Verifica disponibilità di torch\n",
        "\n",
        "            print(\"DEBUG: Moduli transformers e torch disponibili. Inizializzazione riconoscitore Transformer NER...\")\n",
        "\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"5had3/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "            # Verifica se il riconoscitore è stato inizializzato correttamente\n",
        "            if not hasattr(self.transformers_recognizer, 'is_available') or not self.transformers_recognizer.is_available:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER non disponibile dopo l'inizializzazione\")\n",
        "            else:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER inizializzato con successo\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - Transformer NER non disponibile. {e}\")\n",
        "            print(\"Installare 'transformers' e 'torch' con 'pip install transformers torch'\")\n",
        "            self.transformers_recognizer = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione del riconoscitore Transformer NER: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer:\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiorna il set di testi esistenti dopo aver aggiunto le entità italiane\n",
        "        for entity in italian_only_entities:\n",
        "            if 'text' in entity:\n",
        "                existing_texts.add(entity['text'].lower())\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        transformers_only_entities = []\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            try:\n",
        "                # Usa un metodo separato per ottenere le entità\n",
        "                transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "                print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "\n",
        "                # Aggiungi solo se ci sono entità\n",
        "                if transformers_only_entities:\n",
        "                    enriched_entities.extend(transformers_only_entities)\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nel trovare entità Transformer: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile per find_entities\")\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "          \"\"\"\n",
        "          Ottiene entità riconosciute dal riconoscitore Transformer NER.\n",
        "          Includendo sia entità nuove che quelle già trovate da altre fonti.\n",
        "          \"\"\"\n",
        "          # Verifica che il riconoscitore Transformer sia disponibile\n",
        "          if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "              print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "              return []\n",
        "\n",
        "          # Stampa informazioni sul riconoscitore\n",
        "          print(f\"DEBUG: Modello Transformer NER in uso: {self.transformers_recognizer.model_name}\")\n",
        "\n",
        "          try:\n",
        "              # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "              transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "\n",
        "              # Debug dettagliato delle entità\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER trovate: {len(transformers_entities)}\")\n",
        "\n",
        "              # Filtra e mappa le entità - IMPORTANTE: non filtriamo per existing_texts\n",
        "              # per assicurarci che tutte le entità Transformer siano incluse\n",
        "              transformers_mapped = []\n",
        "              for entity in transformers_entities:\n",
        "                  if self._is_valid_entity(entity.get('text', '')):\n",
        "                      # Aggiungi sempre il source 'transformers_ner'\n",
        "                      mapped_entity = {\n",
        "                          'text': entity.get('text', ''),\n",
        "                          'label': entity.get('label', ''),\n",
        "                          'types': [entity.get('label', 'N/A')],\n",
        "                          'source': 'transformers_ner',\n",
        "                          'score': entity.get('score', 0)\n",
        "                      }\n",
        "                      transformers_mapped.append(mapped_entity)\n",
        "\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER mappate: {len(transformers_mapped)}\")\n",
        "              return transformers_mapped\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "              import traceback\n",
        "              traceback.print_exc()\n",
        "              return []\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Importazioni necessarie\n",
        "    import os\n",
        "    import sys\n",
        "    import traceback\n",
        "\n",
        "    # Imposta il livello di log per transformers\n",
        "    os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"info\"\n",
        "\n",
        "    # Verifica le dipendenze\n",
        "    try:\n",
        "        import torch\n",
        "        import transformers\n",
        "        print(f\"PyTorch versione: {torch.__version__}\")\n",
        "        print(f\"Transformers versione: {transformers.__version__}\")\n",
        "        print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"ATTENZIONE: Libreria mancante - {e}\")\n",
        "\n",
        "    try:\n",
        "        # Crea un'istanza del linker di entità migliorato con debug aggiuntivo\n",
        "        print(\"Inizializzazione EntityLinkerItalian...\")\n",
        "        linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "        # Verifica disponibilità dei riconoscitori\n",
        "        print(f\"SpaCy riconoscitore disponibile: {linker.wikidata.spacy_recognizer.is_available}\")\n",
        "        print(f\"Italian NLP riconoscitore disponibile: {linker.italian_recognizer.is_available}\")\n",
        "        if hasattr(linker, 'transformers_recognizer') and linker.transformers_recognizer:\n",
        "            print(f\"Transformer NER riconoscitore disponibile: {linker.transformers_recognizer.is_available}\")\n",
        "            print(f\"Transformer NER modello: {linker.transformers_recognizer.model_name}\")\n",
        "        else:\n",
        "            print(\"Transformer NER riconoscitore non disponibile\")\n",
        "\n",
        "        # Testo di esempio\n",
        "        text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "               \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "               \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "               \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "               \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "               \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "               \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "               \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "               \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "               \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "        print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "        # Test esplicito del riconoscitore Transformer NER, se disponibile\n",
        "        if hasattr(linker, 'transformers_recognizer') and linker.transformers_recognizer and linker.transformers_recognizer.is_available:\n",
        "            print(\"\\nTest diretto del riconoscitore Transformer NER:\")\n",
        "            test_sample = \"Mario Rossi vive a Roma e lavora per l'Università La Sapienza.\"\n",
        "            try:\n",
        "                transformer_test_entities = linker.transformers_recognizer.recognize_entities(test_sample)\n",
        "                print(f\"Entità trovate dal riconoscitore Transformer in test: {len(transformer_test_entities)}\")\n",
        "                for entity in transformer_test_entities:\n",
        "                    print(f\"- '{entity['text']}' → Etichetta: {entity.get('label')}, Score: {entity.get('score')}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Errore nel test diretto del riconoscitore Transformer: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "        # Trova entità\n",
        "        print(\"\\nRicerca entità nel testo principale...\")\n",
        "        entities, stats = linker.find_entities(text)\n",
        "\n",
        "        # Verifica entità Transformer\n",
        "        transformer_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "        print(f\"\\nEntità Transformer NER trovate: {len(transformer_entities)}\")\n",
        "        for entity in transformer_entities[:10]:  # Mostra solo le prime 10 per brevità\n",
        "            print(f\"- '{entity['text']}' → Etichetta: {entity.get('label')}, Score: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nTrovate {stats['total_entities']} entità totali:\")\n",
        "        for k, v in stats.items():\n",
        "            print(f\"- {k}: {v}\")\n",
        "\n",
        "        # Esporta le entità in un file\n",
        "        print(\"\\nEsportazione entità in file...\")\n",
        "        output_file_path = export_entities_to_txt(\n",
        "            entities,\n",
        "            stats,\n",
        "            \"entita_complete.txt\",  # Nuovo nome file per distinguerlo dal precedente\n",
        "            spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "            italian_recognizer=linker.italian_recognizer,\n",
        "            transformers_recognizer=linker.transformers_recognizer,\n",
        "            original_text=text\n",
        "        )\n",
        "\n",
        "        print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "        # Verifica il file di output\n",
        "        import os\n",
        "        if os.path.exists(output_file_path):\n",
        "            file_size = os.path.getsize(output_file_path)\n",
        "            print(f\"Verifica file: '{output_file_path}' esiste e ha dimensione {file_size} bytes\")\n",
        "\n",
        "            # Leggi il file per verificare il contenuto\n",
        "            with open(output_file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Cerca la sezione Transformer nel file\n",
        "            if \"TRANSFORMER NER ENTITIES:\" in content:\n",
        "                transformer_section = content.split(\"TRANSFORMER NER ENTITIES:\")[1].split(\"\\n\\n\")[0]\n",
        "                transformer_lines = [line for line in transformer_section.split(\"\\n\") if line.strip() and \"-----\" not in line]\n",
        "                print(f\"\\nSezione Transformer nel file di output contiene {len(transformer_lines)} righe\")\n",
        "                if len(transformer_lines) > 1:  # La prima riga è l'intestazione\n",
        "                    print(\"Le prime 5 entità Transformer nel file:\")\n",
        "                    for line in transformer_lines[:6]:\n",
        "                        print(f\"  {line}\")\n",
        "                else:\n",
        "                    print(\"ATTENZIONE: Nessuna entità Transformer trovata nel file di output\")\n",
        "            else:\n",
        "                print(\"ATTENZIONE: Sezione Transformer non trovata nel file di output\")\n",
        "        else:\n",
        "            print(f\"ATTENZIONE: Il file '{output_file_path}' non è stato creato!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore generale: {e}\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "lB58aO7tSgZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in entities:  # Iterate through all entities\n",
        "        if entity.get('source') in ['wikidata', 'wikidata+spacy']:  # Check if the entity is from Wikidata\n",
        "            lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")  # Access Wikidata-specific attributes only if present\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Aggiungi questa sezione PRIMA delle ultime righe di scrittura del file\n",
        "        # Sezione entità del riconoscitore Transformer NER\n",
        "#        transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "        \"\"\"\n",
        "        lines.append(\"\\nTRANSFORMER NER ENTITIES:\")\n",
        "        lines.append(\"-------------------------\")\n",
        "        if transformers_entities:\n",
        "            # Rimuovi duplicati mantenendo l'ordine originale\n",
        "            unique_transformers_entities = []\n",
        "            seen = set()\n",
        "            for entity in transformers_entities:\n",
        "                if entity['text'] not in seen:\n",
        "                    unique_transformers_entities.append(entity)\n",
        "                    seen.add(entity['text'])\n",
        "\n",
        "            for entity in unique_transformers_entities:\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {entity.get('label', 'N/A')}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "        else:\n",
        "            lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "        lines.append(\"\")\n",
        "        \"\"\"\n",
        "\n",
        "        # Aggiungi questa parte nella funzione export_entities_to_txt\n",
        "        # Sezione entità del riconoscitore Transformer NER\n",
        "        #transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "        \"\"\"\n",
        "\n",
        "        lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "        lines.append(\"-------------------------\")\n",
        "\n",
        "        if transformers_entities:\n",
        "            for entity in transformers_entities:\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {entity.get('label', 'N/A')}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "        else:\n",
        "            lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "        lines.append(\"\")\n",
        "        \"\"\"\n",
        "\n",
        "        # Sezione entità del riconoscitore Transformer NER\n",
        "        transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "        lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "        lines.append(\"-------------------------\")\n",
        "        if transformers_entities:\n",
        "            # Rimuovi duplicati mantenendo l'ordine originale\n",
        "            unique_transformers_entities = []\n",
        "            seen = set()\n",
        "            for entity in transformers_entities:\n",
        "                if entity['text'] not in seen:\n",
        "                    unique_transformers_entities.append(entity)\n",
        "                    seen.add(entity['text'])\n",
        "\n",
        "            for entity in unique_transformers_entities:\n",
        "                # Usa il primo tipo disponibile o 'N/A'\n",
        "                label = entity.get('label', entity.get('types', ['N/A'])[0] if entity.get('types') else 'N/A')\n",
        "                score = entity.get('score', 'N/A')\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "        else:\n",
        "            lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Lista di modelli italiani alternativi\n",
        "            alternative_models = [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
        "            ]\n",
        "\n",
        "            # Prova a caricare uno dei modelli\n",
        "            for model_name in alternative_models:\n",
        "                try:\n",
        "                    # Determina il device\n",
        "                    device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "                    # Carica tokenizer e modello\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "                    # Crea pipeline NER\n",
        "                    self.ner_pipeline = pipeline(\n",
        "                        \"ner\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    self.is_available = True\n",
        "                    self.model_name = model_name\n",
        "                    print(f\"Modello Transformer NER caricato: {model_name}\")\n",
        "                    return\n",
        "\n",
        "                except Exception as inner_e:\n",
        "                    print(f\"Tentativo fallito con modello {model_name}: {inner_e}\")\n",
        "\n",
        "            # Se nessun modello funziona\n",
        "            raise Exception(\"Nessun modello NER disponibile\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"5had3/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "            # Test di riconoscimento\n",
        "            test_text = \"Mario Rossi lavora per la Banca d'Italia a Roma.\"\n",
        "            test_entities = self.transformers_recognizer.recognize_entities(test_text)\n",
        "            print(\"DEBUG: Test riconoscitore Transformer NER:\")\n",
        "            for entity in test_entities:\n",
        "                print(f\"- {entity['text']} (Tipo: {entity.get('label', 'N/A')})\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "#    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        # ... codice esistente ...\n",
        "\n",
        "        # Test esplicito del riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"5had3/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "            # Test di riconoscimento\n",
        "            test_text = \"Mario Rossi lavora per la Banca d'Italia a Roma.\"\n",
        "            test_entities = self.transformers_recognizer.recognize_entities(test_text)\n",
        "            print(\"DEBUG: Test riconoscitore Transformer NER:\")\n",
        "            for entity in test_entities:\n",
        "                print(f\"- {entity['text']} (Tipo: {entity.get('label', 'N/A')})\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer'):\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile\")\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Stampa informazioni sul riconoscitore\n",
        "        print(f\"DEBUG: Modello Transformer NER in uso: {self.transformers_recognizer.model_name}\")\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Debug dettagliato delle entità\n",
        "        print(\"DEBUG: Dettagli entità Transformer NER:\")\n",
        "        for entity in transformers_entities:\n",
        "            print(f\"- Testo: {entity.get('text')}\")\n",
        "            print(f\"  Label: {entity.get('label')}\")\n",
        "            print(f\"  Score: {entity.get('score')}\")\n",
        "            print(f\"  Attributi disponibili: {list(entity.keys())}\")\n",
        "\n",
        "        # Filtra e mappa le entità\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            # Aggiungi sempre il source 'transformers_ner'\n",
        "            mapped_entity = {\n",
        "                'text': entity.get('text', ''),\n",
        "                'label': entity.get('label', ''),\n",
        "                'types': [entity.get('label', 'N/A')],\n",
        "                'source': 'transformers_ner',\n",
        "                'score': entity.get('score', 0)\n",
        "            }\n",
        "            transformers_only.append(mapped_entity)\n",
        "\n",
        "        print(f\"DEBUG: Numero di entità Transformer NER mappate: {len(transformers_only)}\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    # Debug delle entità\n",
        "    print(\"\\nDEBUG: Entità trovate:\")\n",
        "    for entity in entities:\n",
        "        print(f\"- {entity.get('text')} → Fonte: {entity.get('source')} → Tipi: {entity.get('types')}\")\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4f2b0ed79d38450d90c0f9aaf26b703d",
            "4c9e4b8d082240af91f07e5801d660c6",
            "31f77130c9fc4123924a09cbae7b55cf",
            "b2834fc068b34027b9749dd2e147f110",
            "22b331ae9213459793c664fa8bae9f1b",
            "dfd16ae1d0c94c5a8f1d61dff63c9472",
            "fb53263e2a0046ec84b6af30412e15a6",
            "6b35f95cd0be44ccbe9b86f517b70514",
            "e17b1aece8d74f478c55df692f85064b",
            "2c2ac16576c948cea2357e2e6dc1e1f5",
            "165c03cdec8740298c5fbfbd10e1e7a2",
            "2358c7e72a614e82983c4fa1c1786235",
            "df5948e06714428fa678a301dcfbaec5",
            "f84a770d415b4d0999bc9688ce8c0521",
            "98e1deb8366049bf82dff5fb8aefcf95",
            "58f47b40c87c45eaa1d1b9d4a7ceeec4",
            "869b15e439c54574a6c792d55e44f327",
            "f61f2229c317449783cb8ca91ed90e48",
            "d7b16ac57b8545978dd8410265efd669",
            "d7a8a68428524ff296babb0dec308625",
            "1b370a2c76864358a6799e4dd69a535e",
            "f2b9aac9d7bf4324ba1734bed66c94f5",
            "6f59f8769d1d463b8dcc2ad600f31acd",
            "546f7060068d45aaa7ddb7482d788c53",
            "359dfed2e58e4c728d2719a78dc0ca6e",
            "375b13829cd4422f802c426cf72d6bd9",
            "a651b3dba8714c40ac320519aca87f3e",
            "f26d0a03a31e414d827d4b2a120355e8",
            "2aa366237ee9438cad91323109003d84",
            "a2ddb8ba063441c4bdf6548fd78ebc31",
            "3913e2a3cf3c4e76b0a116275a44b8c8",
            "c23b007e16fd4af8bb78d5c56726fbf7",
            "95b5a74373e640e1a3dddaf0f79545fa",
            "f6dec49fa3e74e0497b56dec5f5bfe39",
            "6efd2dac3abb4bb28a58a7e786b0dbc7",
            "e22ba9648e8b4ef49196fab4286e23fb",
            "c48776fd91484b0f9a3152d5c95ace2f",
            "a5b1eab5922449afba948ded1cf8dfa1",
            "70ad6580689b4760b3c66ff3bd1adce5",
            "501795b33408438fac1f3aaaa0c58d0d",
            "d5a24a0a192845a79b5a393ff8ff066a",
            "35d3b2b222c24210986f1a20394f8619",
            "0bf15c9b0dc446e59665349f35607105",
            "c689d16707c2499ead910ede868bb5f6",
            "c2b38b3f77f54006bc037e348fca994d",
            "58053e8c67a34ee9ab89520f44934b8d",
            "e30e5d2b680a41cd8a1ca08968094c98",
            "66b0e5c8da4d4beb8e396c33d594aff4",
            "08bb325b93ec4dbf864689b75fdc63e3",
            "6105e1a68b614eaebaca27cf77f56b51",
            "de6b44938d104b59bd9be3a796a95d93",
            "6b7cd39f8b2b4bec859daded93bda415",
            "8d23af6bffe6422faf28114b62901406",
            "f6e6729756ad4f99a1ad7d32a67990c7",
            "8238bd5ce4624ad89cbdfff5ef84365c",
            "5a5c0abaf49c4a0e893601b3a64ee12c",
            "18dec80a771a4909b9819c1adf1a8a12",
            "2b7dedf4572946938aaddb039a383da1",
            "82d5ca52b4f94fa8919cdb08acd80046",
            "00d66d2f3392429a87f882fc6caab54b",
            "ae688afe18c145d7829bc362bb2cf06b",
            "e9b5e018eda14deb8463a1455160cc5b",
            "1cad711a4cff47a988ca297b60236ac0",
            "1b46a91fc9a3473fb459341e2e99b034",
            "3f7fcc7fb65348a49e35d1b9f651ee32",
            "9d411f11d4ee4ba6abb4f0f5180d240d",
            "bb2e9b2ac76d4207ab295806486bf080",
            "e23e4ed8366748939d6a865fc363124e",
            "7a012e7dd292449c9b109d30212da5c9",
            "1e01bd647553482eb3851758584d4e70",
            "17e3070ad9b64992bbaf6d9601a07274",
            "e6e7dd6104db44729caeb98a486a424d",
            "53e2a50e482b42a5ac5310bffd01eedc",
            "9c7ba345679144329b58e68017d8db7b",
            "e78cc9c96e57420f8024f6d344dd070d",
            "9c023e240a4b415881adc31955425e49",
            "82ff4d1886af40fa9cee92ee66bfdcfc",
            "7329c230cce8492a8e0a1eceffec1682",
            "63fa36456e0941cd8e88bd6501225552",
            "6d21f049e1c04df99a24041b02618871",
            "85afdc7d15bf4b47965ea941d04b0c90",
            "2145217afc964b8fb5aebcbdbd89004e",
            "904f34f10356404099636325e07cb5e6",
            "6e095b12de39400f902044fba18809e1",
            "6ea8a45add2d4b90b6ea06d0ad224107",
            "01a5dc851c234e19b4e6884e79cba1d5",
            "c1202b3dfd9749e8bfbc7384bf0f00dc",
            "0a83484412274610a5d2716c688a54a2"
          ]
        },
        "id": "ib3r2kyHEJ01",
        "outputId": "b2d4358a-aa5a-478c-8173-2fb751252bb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f2b0ed79d38450d90c0f9aaf26b703d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/default.zip:   0%|          | …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2358c7e72a614e82983c4fa1c1786235"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f59f8769d1d463b8dcc2ad600f31acd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tentativo fallito con modello 5had3/bert-base-italian-cased-ner: 5had3/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Tentativo fallito con modello MilaNLProc/bert-italian-cased-ner: MilaNLProc/bert-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/264 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6dec49fa3e74e0497b56dec5f5bfe39"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.10k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2b38b3f77f54006bc037e348fca994d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a5c0abaf49c4a0e893601b3a64ee12c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb2e9b2ac76d4207ab295806486bf080"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/709M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7329c230cce8492a8e0a1eceffec1682"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello Transformer NER caricato: Davlan/bert-base-multilingual-cased-ner-hrl\n",
            "DEBUG: Test riconoscitore Transformer NER:\n",
            "- MarioRossi (Tipo: PER)\n",
            "- Bancad'Italia (Tipo: ORG)\n",
            "- Roma (Tipo: LOC)\n",
            "Tentativo fallito con modello 5had3/bert-base-italian-cased-ner: 5had3/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Tentativo fallito con modello MilaNLProc/bert-italian-cased-ner: MilaNLProc/bert-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Modello Transformer NER caricato: Davlan/bert-base-multilingual-cased-ner-hrl\n",
            "DEBUG: Test riconoscitore Transformer NER:\n",
            "- MarioRossi (Tipo: PER)\n",
            "- Bancad'Italia (Tipo: ORG)\n",
            "- Roma (Tipo: LOC)\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Modello Transformer NER in uso: Davlan/bert-base-multilingual-cased-ner-hrl\n",
            "DEBUG: Dettagli entità Transformer NER:\n",
            "- Testo: Italia\n",
            "  Label: LOC\n",
            "  Score: 0.9998433589935303\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Roma\n",
            "  Label: LOC\n",
            "  Score: 0.9995030164718628\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Vaticano\n",
            "  Label: LOC\n",
            "  Score: 0.6520251631736755\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Firenze\n",
            "  Label: LOC\n",
            "  Score: 0.9997918009757996\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Milano\n",
            "  Label: LOC\n",
            "  Score: 0.9998107552528381\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Napoli\n",
            "  Label: LOC\n",
            "  Score: 0.9998220801353455\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Sapienza\n",
            "  Label: ORG\n",
            "  Score: 0.9991324543952942\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: PolitecnicodiMilano\n",
            "  Label: ORG\n",
            "  Score: 0.9991716146469116\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Europa\n",
            "  Label: LOC\n",
            "  Score: 0.9998190999031067\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Ferrari\n",
            "  Label: ORG\n",
            "  Score: 0.5556336045265198\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Lamborghini\n",
            "  Label: ORG\n",
            "  Score: 0.6654011607170105\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Mediterraneo\n",
            "  Label: LOC\n",
            "  Score: 0.9982662796974182\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Italia\n",
            "  Label: LOC\n",
            "  Score: 0.9998801946640015\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: DanteAlighieri\n",
            "  Label: PER\n",
            "  Score: 0.999840497970581\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: LeonardodaVinci\n",
            "  Label: PER\n",
            "  Score: 0.9998635053634644\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: GalileoGalilei\n",
            "  Label: PER\n",
            "  Score: 0.9998313188552856\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Italia\n",
            "  Label: LOC\n",
            "  Score: 0.9998799562454224\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "DEBUG: Numero di entità Transformer NER mappate: 17\n",
            "DEBUG: Entità Transformer NER trovate: 17\n",
            "\n",
            "DEBUG: Entità trovate:\n",
            "- Italia → Fonte: wikidata+spacy → Tipi: ['paese', 'stato sovrano', 'stato sociale', 'Stato unitario', 'repubblica', 'paese mediterraneo', 'grande potenza']\n",
            "- Roma → Fonte: wikidata+spacy → Tipi: ['città di confine', 'comune italiano soppresso', 'destinazione turistica', 'metropoli', 'città più grande', 'città universitaria', 'grande città', 'comune italiano', 'capitale di Stato']\n",
            "- Colosseo → Fonte: wikidata+spacy → Tipi: ['anfiteatro romano', 'sito archeologico', 'attrazione turistica', 'stadio', 'manufatto archeologico museo', 'museo nazionale italiano', 'edificio civile storico museo', 'museo del Ministero della Cultura italiano']\n",
            "- Vaticano → Fonte: wikidata+spacy → Tipi: ['stato sovrano', 'città-Stato', 'enclave', 'Paese senza affaccio al mare', 'attrazione turistica', 'paese mediterraneo', 'paese', 'complesso religioso', 'complesso istituzionale', 'area urbana', 'destinazione turistica', 'Stato confessionale', 'Q7396640']\n",
            "- Firenze → Fonte: wikidata+spacy → Tipi: ['comune italiano', 'grande città', 'capitale o capoluogo', 'Città-stato italiane']\n",
            "- Rinascimento → Fonte: wikidata+spacy → Tipi: ['movimento artistico', 'movimento culturale']\n",
            "- Milano → Fonte: wikidata+spacy → Tipi: ['città', 'capoluogo', 'capoluogo', 'capoluogo', 'Città-stato italiane', 'grande città', 'metropoli', 'comune italiano']\n",
            "- Napoli → Fonte: wikidata+spacy → Tipi: ['città', 'comune italiano', 'grande città']\n",
            "- Vesuvio → Fonte: wikidata+spacy → Tipi: ['vulcano attivo', 'stratovulcano', 'attrazione turistica', 'montagna']\n",
            "- la Sapienza → Fonte: wikidata+spacy → Tipi: ['università statale', 'organizzazione']\n",
            "- Politecnico di Milano → Fonte: wikidata+spacy → Tipi: ['politecnico', 'organizzazione']\n",
            "- Europa → Fonte: wikidata+spacy → Tipi: ['area continentale e isole limitrofe', 'continente', 'regione geografica']\n",
            "- Chianti → Fonte: wikidata+spacy → Tipi: ['vino rosso', 'vino da tavola']\n",
            "- Barolo → Fonte: wikidata+spacy → Tipi: ['comune italiano']\n",
            "- Ferrari → Fonte: wikidata+spacy → Tipi: ['scuderia di Formula 1']\n",
            "- Lamborghini → Fonte: wikidata+spacy → Tipi: ['casa automobilistica', 'impresa', 'società controllata']\n",
            "- Mediterraneo → Fonte: wikidata+spacy → Tipi: ['mare interno', 'mare mediterraneo', 'bacino idrografico']\n",
            "- Dante Alighieri → Fonte: wikidata+spacy → Tipi: ['umano']\n",
            "- Leonardo da Vinci → Fonte: wikidata+spacy → Tipi: ['umano']\n",
            "- Galileo Galilei → Fonte: wikidata+spacy → Tipi: ['umano']\n",
            "- La Ferrari → Fonte: wikidata+spacy → Tipi: ['singolo']\n",
            "- Il Mediterraneo → Fonte: wikidata+spacy → Tipi: ['quotidiano']\n",
            "- Sapienza → Fonte: wikidata+spacy → Tipi: ['università statale', 'organizzazione']\n",
            "- Politecnico → Fonte: wikidata+spacy → Tipi: []\n",
            "- Alighieri → Fonte: wikidata+spacy → Tipi: ['famiglia nobile italiana']\n",
            "- Leonardo → Fonte: wikidata+spacy → Tipi: ['umano']\n",
            "- Vinci → Fonte: wikidata+spacy → Tipi: ['comune italiano']\n",
            "- Galileo → Fonte: wikidata+spacy → Tipi: ['umano']\n",
            "- Galilei → Fonte: wikidata+spacy → Tipi: ['umano']\n",
            "- Politecnico di Milano → Fonte: local → Tipi: ['University']\n",
            "- Lamborghini → Fonte: local → Tipi: ['Company']\n",
            "- Sapienza → Fonte: local → Tipi: ['University']\n",
            "- Vesuvio → Fonte: local → Tipi: ['Volcano']\n",
            "- Ferrari → Fonte: local → Tipi: ['Company']\n",
            "- Chianti → Fonte: local → Tipi: ['Wine']\n",
            "- Barolo → Fonte: local → Tipi: ['Wine']\n",
            "- pizza → Fonte: local → Tipi: ['Food']\n",
            "- Dante → Fonte: italian_nlp → Tipi: ['ProperNoun']\n",
            "- Italia → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Roma → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Vaticano → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Firenze → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Milano → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Napoli → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Sapienza → Fonte: transformers_ner → Tipi: ['ORG']\n",
            "- PolitecnicodiMilano → Fonte: transformers_ner → Tipi: ['ORG']\n",
            "- Europa → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Ferrari → Fonte: transformers_ner → Tipi: ['ORG']\n",
            "- Lamborghini → Fonte: transformers_ner → Tipi: ['ORG']\n",
            "- Mediterraneo → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- Italia → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "- DanteAlighieri → Fonte: transformers_ner → Tipi: ['PER']\n",
            "- LeonardodaVinci → Fonte: transformers_ner → Tipi: ['PER']\n",
            "- GalileoGalilei → Fonte: transformers_ner → Tipi: ['PER']\n",
            "- Italia → Fonte: transformers_ner → Tipi: ['LOC']\n",
            "\n",
            "Trovate 55 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "- 'Italia' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Roma' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Vaticano' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Firenze' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Milano' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Napoli' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Sapienza' → N/A\n",
            "  Tipo: ORG\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'PolitecnicodiMilano' → N/A\n",
            "  Tipo: ORG\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Europa' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Ferrari' → N/A\n",
            "  Tipo: ORG\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Lamborghini' → N/A\n",
            "  Tipo: ORG\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Mediterraneo' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Italia' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'DanteAlighieri' → N/A\n",
            "  Tipo: PER\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'LeonardodaVinci' → N/A\n",
            "  Tipo: PER\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'GalileoGalilei' → N/A\n",
            "  Tipo: PER\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'Italia' → N/A\n",
            "  Tipo: LOC\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Modello Transformer NER in uso: Davlan/bert-base-multilingual-cased-ner-hrl\n",
            "DEBUG: Dettagli entità Transformer NER:\n",
            "- Testo: Italia\n",
            "  Label: LOC\n",
            "  Score: 0.9998433589935303\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Roma\n",
            "  Label: LOC\n",
            "  Score: 0.9995030164718628\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Vaticano\n",
            "  Label: LOC\n",
            "  Score: 0.6520251631736755\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Firenze\n",
            "  Label: LOC\n",
            "  Score: 0.9997918009757996\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Milano\n",
            "  Label: LOC\n",
            "  Score: 0.9998107552528381\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Napoli\n",
            "  Label: LOC\n",
            "  Score: 0.9998220801353455\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Sapienza\n",
            "  Label: ORG\n",
            "  Score: 0.9991324543952942\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: PolitecnicodiMilano\n",
            "  Label: ORG\n",
            "  Score: 0.9991716146469116\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Europa\n",
            "  Label: LOC\n",
            "  Score: 0.9998190999031067\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Ferrari\n",
            "  Label: ORG\n",
            "  Score: 0.5556336045265198\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Lamborghini\n",
            "  Label: ORG\n",
            "  Score: 0.6654011607170105\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Mediterraneo\n",
            "  Label: LOC\n",
            "  Score: 0.9982662796974182\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Italia\n",
            "  Label: LOC\n",
            "  Score: 0.9998801946640015\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: DanteAlighieri\n",
            "  Label: PER\n",
            "  Score: 0.999840497970581\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: LeonardodaVinci\n",
            "  Label: PER\n",
            "  Score: 0.9998635053634644\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: GalileoGalilei\n",
            "  Label: PER\n",
            "  Score: 0.9998313188552856\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "- Testo: Italia\n",
            "  Label: LOC\n",
            "  Score: 0.9998799562454224\n",
            "  Attributi disponibili: ['text', 'label', 'start_char', 'end_char', 'score', 'source']\n",
            "DEBUG: Numero di entità Transformer NER mappate: 17\n",
            "DEBUG: Entità Transformer NER trovate: 17\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 55\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- transformers_only_entities: 17\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 9053 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,\n",
        "    original_text=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy, dal riconoscitore italiano e dal riconoscitore Transformer NER.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        transformers_recognizer: Riconoscitore Transformer NER (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']  # Assicurati che questa riga non sia commentata\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in entities:  # Iterate through all entities\n",
        "        if entity.get('source') in ['wikidata', 'wikidata+spacy']:  # Check if the entity is from Wikidata\n",
        "            lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")  # Access Wikidata-specific attributes only if present\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore Transformer e un testo originale, ottieni tutte le entità\n",
        "    if transformers_recognizer and original_text and hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "        try:\n",
        "            print(\"DEBUG: Tentativo di ottenere entità dal riconoscitore Transformer\")\n",
        "            transformers_all_entities = transformers_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not transformers_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "            else:\n",
        "                print(f\"DEBUG: Trovate {len(transformers_all_entities)} entità da Transformer\")\n",
        "                # Rimuovi duplicati mantenendo l'ordine originale\n",
        "                unique_transformers_entities = []\n",
        "                seen = set()\n",
        "                for entity in transformers_all_entities:\n",
        "                    if entity['text'] not in seen:\n",
        "                        unique_transformers_entities.append(entity)\n",
        "                        seen.add(entity['text'])\n",
        "\n",
        "                for entity in unique_transformers_entities:\n",
        "                    # Usa il primo tipo disponibile o 'N/A'\n",
        "                    label = entity.get('label', entity.get('types', ['N/A'])[0] if entity.get('types') else 'N/A')\n",
        "                    score = entity.get('score', 'N/A')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore Transformer: {e}\")\n",
        "            print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità Transformer già filtrate nel metodo find_entities\n",
        "        if not transformers_entities:\n",
        "            if not transformers_recognizer:\n",
        "                lines.append(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            elif not hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "                lines.append(\"Il riconoscitore Transformer NER non ha il metodo recognize_entities.\")\n",
        "            else:\n",
        "                lines.append(\"Per mostrare tutte le entità del riconoscitore Transformer, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Usando {len(transformers_entities)} entità Transformer già filtrate\")\n",
        "            for entity in transformers_entities:\n",
        "                label = entity.get('label', 'N/A')\n",
        "                score = entity.get('score', 'N/A')\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Lista di fallback globale (modelli che funzionano in molte lingue)\n",
        "        self.fallback_models = [\n",
        "            \"dslim/bert-base-NER\",               # Ottimo supporto multilingua\n",
        "            \"dbmdz/bert-large-cased-finetuned-conll03-english\",  # Molto robusto\n",
        "            \"Babelscape/wikineural-multilingual-ner\"  # Supporto multilingua avanzato\n",
        "        ]\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG: Selezionando modello per lingua: {self.language}\")\n",
        "\n",
        "        # Prova prima i modelli specifici per la lingua\n",
        "        models = self.language_models.get(self.language, self.language_models.get(\"en\", []))\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Importa solo il tokenizer per verificare la disponibilità\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Verifica disponibilità modello: {model}\")\n",
        "\n",
        "                # Verifica la disponibilità con caricamento offline\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=False)\n",
        "                print(f\"DEBUG: Modello {model} disponibile e selezionato\")\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Modello {model} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Se nessun modello specifico per lingua funziona, prova i fallback\n",
        "        print(\"DEBUG: Nessun modello specifico per lingua disponibile, provo fallback globali\")\n",
        "        for fallback in self.fallback_models:\n",
        "            try:\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Provo fallback modello: {fallback}\")\n",
        "                tokenizer = AutoTokenizer.from_pretrained(fallback, local_files_only=False)\n",
        "                print(f\"DEBUG: Fallback {fallback} disponibile e selezionato\")\n",
        "                return fallback\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Fallback {fallback} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Ultimo tentativo: dslim/bert-base-NER (molto comune)\n",
        "        print(\"DEBUG: Ultimo tentativo con dslim/bert-base-NER\")\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Assicurati che le dipendenze siano disponibili\n",
        "            import torch\n",
        "            from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "            print(f\"DEBUG: Tentativo di caricare il modello: {self.model_name}\")\n",
        "\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "            print(f\"DEBUG: Usando device {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "            # Prova a caricare con questo modello specifico\n",
        "            try:\n",
        "                # Carica tokenizer e modello\n",
        "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "                model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "                # Crea pipeline NER\n",
        "                self.ner_pipeline = pipeline(\n",
        "                    \"ner\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                self.is_available = True\n",
        "                print(f\"DEBUG: Modello {self.model_name} caricato con successo!\")\n",
        "\n",
        "                # Test rapido per verificare il funzionamento\n",
        "                test_text = \"Mario Rossi vive a Roma.\"\n",
        "                test_result = self.ner_pipeline(test_text)\n",
        "                print(f\"DEBUG: Test modello superato - entità trovate: {len(test_result)}\")\n",
        "                return\n",
        "\n",
        "            except Exception as model_error:\n",
        "                print(f\"DEBUG: Errore nel caricamento del modello specifico: {str(model_error)}\")\n",
        "\n",
        "                # Se il modello specifico fallisce, prova i modelli di fallback dalla lista\n",
        "                for fallback in self.fallback_models:\n",
        "                    if fallback != self.model_name:  # Evita di riprovare lo stesso modello\n",
        "                        try:\n",
        "                            print(f\"DEBUG: Provo modello di fallback: {fallback}\")\n",
        "                            tokenizer = AutoTokenizer.from_pretrained(fallback)\n",
        "                            model = AutoModelForTokenClassification.from_pretrained(fallback)\n",
        "\n",
        "                            self.ner_pipeline = pipeline(\n",
        "                                \"ner\",\n",
        "                                model=model,\n",
        "                                tokenizer=tokenizer,\n",
        "                                device=device\n",
        "                            )\n",
        "\n",
        "                            self.is_available = True\n",
        "                            self.model_name = fallback  # Aggiorna il nome del modello\n",
        "                            print(f\"DEBUG: Modello fallback {fallback} caricato con successo!\")\n",
        "                            return\n",
        "\n",
        "                        except Exception as fallback_error:\n",
        "                            print(f\"DEBUG: Fallback {fallback} fallito: {str(fallback_error)}\")\n",
        "\n",
        "                # Se tutti i fallback falliscono, solleva l'errore originale\n",
        "                raise model_error\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: Errore di importazione: {str(e)}\")\n",
        "            print(\"È necessario installare 'transformers' e 'torch':\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generale nel caricamento del modello: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> list:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            print(f\"DEBUG: Analisi testo con modello {self.model_name}\")\n",
        "\n",
        "            # Esegui NER con gestione di testi lunghi\n",
        "            # Alcuni modelli hanno limiti sulla lunghezza del testo\n",
        "            if len(text) > 500:\n",
        "                # Dividi il testo in blocchi di 500 caratteri con sovrapposizione\n",
        "                chunks = []\n",
        "                chunk_size = 450  # Usa 450 per avere sovrapposizione\n",
        "                for i in range(0, len(text), chunk_size):\n",
        "                    # Assicurati che l'ultimo chunk non vada oltre la fine del testo\n",
        "                    end = min(i + chunk_size + 50, len(text))  # +50 per sovrapposizione\n",
        "                    chunks.append(text[i:end])\n",
        "\n",
        "                print(f\"DEBUG: Testo diviso in {len(chunks)} chunks per l'analisi\")\n",
        "\n",
        "                # Analizza ogni chunk separatamente\n",
        "                all_results = []\n",
        "                for i, chunk in enumerate(chunks):\n",
        "                    print(f\"DEBUG: Analisi chunk {i+1}/{len(chunks)}\")\n",
        "                    chunk_results = self.ner_pipeline(chunk)\n",
        "                    # Aggiungi offset per i chunk successivi al primo\n",
        "                    if i > 0:\n",
        "                        offset = i * chunk_size\n",
        "                        for result in chunk_results:\n",
        "                            result['start'] += offset\n",
        "                            result['end'] += offset\n",
        "                    all_results.extend(chunk_results)\n",
        "\n",
        "                ner_results = all_results\n",
        "            else:\n",
        "                # Per testi brevi, usa direttamente il pipeline\n",
        "                ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            print(f\"DEBUG: Trovati {len(ner_results)} token di entità\")\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],  # Rimuovi il prefisso \"B-\"\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente se il tipo corrisponde\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        # Gestione speciale per i token tokenizzati che iniziano con ##\n",
        "                        if result['word'].startswith('##'):\n",
        "                            current_entity['text'] += result['word'][2:]  # Rimuovi ##\n",
        "                        else:\n",
        "                            # Potrebbe essere necessario aggiungere uno spazio\n",
        "                            space_needed = not (result['start'] == current_entity['end_char'])\n",
        "                            if space_needed:\n",
        "                                current_entity['text'] += ' ' + result['word']\n",
        "                            else:\n",
        "                                current_entity['text'] += result['word']\n",
        "\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        # Aggiorna lo score con la media o il massimo\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                elif result['entity'] != 'O':  # Altra etichetta (non \"Other\")\n",
        "                    # Gestisci entità che non hanno markup B-/I-\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    # Crea una nuova entità\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'].replace('B-', '').replace('I-', ''),\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                else:  # Token classificato come \"O\" (Other)\n",
        "                    # Finalizza l'entità corrente, se esiste\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi l'ultima entità in elaborazione\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            print(f\"DEBUG: Raggruppate in {len(grouped_entities)} entità\")\n",
        "\n",
        "            # Aggiungi source e crea lista finale\n",
        "            for entity in grouped_entities:\n",
        "                # Pulizia finale del testo (rimuovi spazi extra)\n",
        "                entity['text'] = entity['text'].strip()\n",
        "                # Aggiungi la fonte\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "            return entities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nel riconoscimento entità: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return []\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, pipeline  # Verifica disponibilità di transformers\n",
        "            import torch  # Verifica disponibilità di torch\n",
        "\n",
        "            print(\"DEBUG: Moduli transformers e torch disponibili. Inizializzazione riconoscitore Transformer NER...\")\n",
        "\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"5had3/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "            # Verifica se il riconoscitore è stato inizializzato correttamente\n",
        "            if not hasattr(self.transformers_recognizer, 'is_available') or not self.transformers_recognizer.is_available:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER non disponibile dopo l'inizializzazione\")\n",
        "            else:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER inizializzato con successo\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - Transformer NER non disponibile. {e}\")\n",
        "            print(\"Installare 'transformers' e 'torch' con 'pip install transformers torch'\")\n",
        "            self.transformers_recognizer = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione del riconoscitore Transformer NER: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer:\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiorna il set di testi esistenti dopo aver aggiunto le entità italiane\n",
        "        for entity in italian_only_entities:\n",
        "            if 'text' in entity:\n",
        "                existing_texts.add(entity['text'].lower())\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        transformers_only_entities = []\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            try:\n",
        "                # Usa un metodo separato per ottenere le entità\n",
        "                transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "                print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "\n",
        "                # Aggiungi solo se ci sono entità\n",
        "                if transformers_only_entities:\n",
        "                    enriched_entities.extend(transformers_only_entities)\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nel trovare entità Transformer: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile per find_entities\")\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "          \"\"\"\n",
        "          Ottiene entità riconosciute dal riconoscitore Transformer NER.\n",
        "          Includendo sia entità nuove che quelle già trovate da altre fonti.\n",
        "          \"\"\"\n",
        "          # Verifica che il riconoscitore Transformer sia disponibile\n",
        "          if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "              print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "              return []\n",
        "\n",
        "          # Stampa informazioni sul riconoscitore\n",
        "          print(f\"DEBUG: Modello Transformer NER in uso: {self.transformers_recognizer.model_name}\")\n",
        "\n",
        "          try:\n",
        "              # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "              transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "\n",
        "              # Debug dettagliato delle entità\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER trovate: {len(transformers_entities)}\")\n",
        "\n",
        "              # Filtra e mappa le entità - IMPORTANTE: non filtriamo per existing_texts\n",
        "              # per assicurarci che tutte le entità Transformer siano incluse\n",
        "              transformers_mapped = []\n",
        "              for entity in transformers_entities:\n",
        "                  if self._is_valid_entity(entity.get('text', '')):\n",
        "                      # Aggiungi sempre il source 'transformers_ner'\n",
        "                      mapped_entity = {\n",
        "                          'text': entity.get('text', ''),\n",
        "                          'label': entity.get('label', ''),\n",
        "                          'types': [entity.get('label', 'N/A')],\n",
        "                          'source': 'transformers_ner',\n",
        "                          'score': entity.get('score', 0)\n",
        "                      }\n",
        "                      transformers_mapped.append(mapped_entity)\n",
        "\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER mappate: {len(transformers_mapped)}\")\n",
        "              return transformers_mapped\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "              import traceback\n",
        "              traceback.print_exc()\n",
        "              return []\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Importazioni necessarie\n",
        "    import os\n",
        "    import sys\n",
        "    import traceback\n",
        "\n",
        "    # Imposta il livello di log per transformers\n",
        "    os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"info\"\n",
        "\n",
        "    # Verifica le dipendenze\n",
        "    try:\n",
        "        import torch\n",
        "        import transformers\n",
        "        print(f\"PyTorch versione: {torch.__version__}\")\n",
        "        print(f\"Transformers versione: {transformers.__version__}\")\n",
        "        print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"ATTENZIONE: Libreria mancante - {e}\")\n",
        "\n",
        "    try:\n",
        "        # Crea un'istanza del linker di entità migliorato con debug aggiuntivo\n",
        "        print(\"Inizializzazione EntityLinkerItalian...\")\n",
        "        linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "        # Verifica disponibilità dei riconoscitori\n",
        "        print(f\"SpaCy riconoscitore disponibile: {linker.wikidata.spacy_recognizer.is_available}\")\n",
        "        print(f\"Italian NLP riconoscitore disponibile: {linker.italian_recognizer.is_available}\")\n",
        "        if hasattr(linker, 'transformers_recognizer') and linker.transformers_recognizer:\n",
        "            print(f\"Transformer NER riconoscitore disponibile: {linker.transformers_recognizer.is_available}\")\n",
        "            print(f\"Transformer NER modello: {linker.transformers_recognizer.model_name}\")\n",
        "        else:\n",
        "            print(\"Transformer NER riconoscitore non disponibile\")\n",
        "\n",
        "        # Testo di esempio\n",
        "        text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "               \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "               \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "               \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "               \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "               \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "               \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "               \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "               \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "               \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "        print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "        # Test esplicito del riconoscitore Transformer NER, se disponibile\n",
        "        if hasattr(linker, 'transformers_recognizer') and linker.transformers_recognizer and linker.transformers_recognizer.is_available:\n",
        "            print(\"\\nTest diretto del riconoscitore Transformer NER:\")\n",
        "            test_sample = \"Mario Rossi vive a Roma e lavora per l'Università La Sapienza.\"\n",
        "            try:\n",
        "                transformer_test_entities = linker.transformers_recognizer.recognize_entities(test_sample)\n",
        "                print(f\"Entità trovate dal riconoscitore Transformer in test: {len(transformer_test_entities)}\")\n",
        "                for entity in transformer_test_entities:\n",
        "                    print(f\"- '{entity['text']}' → Etichetta: {entity.get('label')}, Score: {entity.get('score')}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Errore nel test diretto del riconoscitore Transformer: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "        # Trova entità\n",
        "        print(\"\\nRicerca entità nel testo principale...\")\n",
        "        entities, stats = linker.find_entities(text)\n",
        "\n",
        "        # Verifica entità Transformer\n",
        "        transformer_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "        print(f\"\\nEntità Transformer NER trovate: {len(transformer_entities)}\")\n",
        "        for entity in transformer_entities[:10]:  # Mostra solo le prime 10 per brevità\n",
        "            print(f\"- '{entity['text']}' → Etichetta: {entity.get('label')}, Score: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nTrovate {stats['total_entities']} entità totali:\")\n",
        "        for k, v in stats.items():\n",
        "            print(f\"- {k}: {v}\")\n",
        "\n",
        "        # Esporta le entità in un file\n",
        "        print(\"\\nEsportazione entità in file...\")\n",
        "        output_file_path = export_entities_to_txt(\n",
        "            entities,\n",
        "            stats,\n",
        "            \"entita_complete.txt\",  # Nuovo nome file per distinguerlo dal precedente\n",
        "            spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "            italian_recognizer=linker.italian_recognizer,\n",
        "            transformers_recognizer=linker.transformers_recognizer,\n",
        "            original_text=text\n",
        "        )\n",
        "\n",
        "        print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "        # Verifica il file di output\n",
        "        import os\n",
        "        if os.path.exists(output_file_path):\n",
        "            file_size = os.path.getsize(output_file_path)\n",
        "            print(f\"Verifica file: '{output_file_path}' esiste e ha dimensione {file_size} bytes\")\n",
        "\n",
        "            # Leggi il file per verificare il contenuto\n",
        "            with open(output_file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Cerca la sezione Transformer nel file\n",
        "            if \"TRANSFORMER NER ENTITIES:\" in content:\n",
        "                transformer_section = content.split(\"TRANSFORMER NER ENTITIES:\")[1].split(\"\\n\\n\")[0]\n",
        "                transformer_lines = [line for line in transformer_section.split(\"\\n\") if line.strip() and \"-----\" not in line]\n",
        "                print(f\"\\nSezione Transformer nel file di output contiene {len(transformer_lines)} righe\")\n",
        "                if len(transformer_lines) > 1:  # La prima riga è l'intestazione\n",
        "                    print(\"Le prime 5 entità Transformer nel file:\")\n",
        "                    for line in transformer_lines[:6]:\n",
        "                        print(f\"  {line}\")\n",
        "                else:\n",
        "                    print(\"ATTENZIONE: Nessuna entità Transformer trovata nel file di output\")\n",
        "            else:\n",
        "                print(\"ATTENZIONE: Sezione Transformer non trovata nel file di output\")\n",
        "        else:\n",
        "            print(f\"ATTENZIONE: Il file '{output_file_path}' non è stato creato!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore generale: {e}\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "15ce6034796a49a3b6e7c9e7a2f03706",
            "5f3d65901d194eafa68c57ac5f70e884",
            "62bb192f2bbc48d4b97ee4e4c0734d5f",
            "6ce8d1d9ce1f47cd9eb579659fbadfc9",
            "a1fc843819284c98899aeec4fb0aa9a1",
            "f057cc731996435d9f58f6e1c0f1d531",
            "ca471935e409406e82cba346e94868a3",
            "c95842da4d71426293cc3e2301bd5f28",
            "6658a810cf0b44c89c339a1a9ca2d749",
            "8a4387ada3b2444cbd91074a6bf1adf4",
            "94e1cdc39f374f57a4a471fb284fcd45",
            "bf24cbaaa71b49d79216a262d86ed746",
            "98459161f55d4b26bc3e109f28ea2da8",
            "3f62967e70344ec892105a60e2679864",
            "c1e85f7a125d4013a307e13b1eb136cc",
            "462518975075445c9def79715409bc3d",
            "00ba219608104d638f56e79da271165b",
            "79c5b09923d945929bcf1539d79794b4",
            "b85a277075a447388cb8aef59159ab6d",
            "9c66679093de48f6accb49e52afa80bd",
            "3d36c6ace5914809b8cccd911b29e8f2",
            "8d71414f037549e0b129cc7c6139d786"
          ]
        },
        "id": "5qXKUS1Zg_64",
        "outputId": "3e8a1c5e-c75d-4406-a3fd-212ea3daa5bf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch versione: 2.6.0+cu124\n",
            "Transformers versione: 4.33.3\n",
            "CUDA disponibile: True\n",
            "Inizializzazione EntityLinkerItalian...\n",
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15ce6034796a49a3b6e7c9e7a2f03706"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bf24cbaaa71b49d79216a262d86ed746"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "DEBUG: Moduli transformers e torch disponibili. Inizializzazione riconoscitore Transformer NER...\n",
            "DEBUG: Tentativo di caricare il modello: 5had3/bert-base-italian-cased-ner\n",
            "DEBUG: Usando device CPU\n",
            "DEBUG: Errore nel caricamento del modello specifico: 5had3/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "DEBUG: Provo modello di fallback: dslim/bert-base-NER\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Modello fallback dslim/bert-base-NER caricato con successo!\n",
            "DEBUG: Riconoscitore Transformer NER inizializzato con successo\n",
            "SpaCy riconoscitore disponibile: True\n",
            "Italian NLP riconoscitore disponibile: True\n",
            "Transformer NER riconoscitore disponibile: True\n",
            "Transformer NER modello: dslim/bert-base-NER\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "\n",
            "Test diretto del riconoscitore Transformer NER:\n",
            "DEBUG: Analisi testo con modello dslim/bert-base-NER\n",
            "DEBUG: Trovati 12 token di entità\n",
            "DEBUG: Raggruppate in 2 entità\n",
            "Entità trovate dal riconoscitore Transformer in test: 2\n",
            "- 'Mario Rossi' → Etichetta: PER, Score: 0.9997902512550354\n",
            "- 'Romavor Università La Sapienza' → Etichetta: ORG, Score: 0.9977136850357056\n",
            "\n",
            "Ricerca entità nel testo principale...\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Entità unica da spaCy: Vesuvio (LOC)\n",
            "Di cui 1 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Modello Transformer NER in uso: dslim/bert-base-NER\n",
            "DEBUG: Analisi testo con modello dslim/bert-base-NER\n",
            "DEBUG: Testo diviso in 3 chunks per l'analisi\n",
            "DEBUG: Analisi chunk 1/3\n",
            "DEBUG: Analisi chunk 2/3\n",
            "DEBUG: Analisi chunk 3/3\n",
            "DEBUG: Trovati 39 token di entità\n",
            "DEBUG: Raggruppate in 11 entità\n",
            "DEBUG: Numero di entità Transformer NER trovate: 11\n",
            "DEBUG: Numero di entità Transformer NER mappate: 11\n",
            "DEBUG: Entità Transformer NER trovate: 11\n",
            "\n",
            "Entità Transformer NER trovate: 11\n",
            "- 'Italia' → Etichetta: ORG, Score: 0.6448531150817871\n",
            "- 'Romaoss' → Etichetta: LOC, Score: 0.889772355556488\n",
            "- 'Milano' → Etichetta: LOC, Score: 0.9906013011932373\n",
            "- 'Napoli Sa Polite Milanoite Milano' → Etichetta: ORG, Score: 0.9531297087669373\n",
            "- 'Europaolo' → Etichetta: LOC, Score: 0.7394434809684753\n",
            "- 'La Ferrari e la Lamborghini' → Etichetta: ORG, Score: 0.9696776866912842\n",
            "- 'Italia' → Etichetta: LOC, Score: 0.5453237295150757\n",
            "- 'Dante Alighieri' → Etichetta: PER, Score: 0.9995166063308716\n",
            "- 'Leonardo da Vinci' → Etichetta: PER, Score: 0.9990577101707458\n",
            "- 'Galileo Galile' → Etichetta: PER, Score: 0.9984367489814758\n",
            "\n",
            "Trovate 49 entità totali:\n",
            "- total_entities: 49\n",
            "- wikidata_entities: 36\n",
            "- spacy_only_entities: 1\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 36\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "- transformers_only_entities: 11\n",
            "\n",
            "Esportazione entità in file...\n",
            "DEBUG: Tentativo di ottenere entità dal riconoscitore Transformer\n",
            "DEBUG: Analisi testo con modello dslim/bert-base-NER\n",
            "DEBUG: Testo diviso in 3 chunks per l'analisi\n",
            "DEBUG: Analisi chunk 1/3\n",
            "DEBUG: Analisi chunk 2/3\n",
            "DEBUG: Analisi chunk 3/3\n",
            "DEBUG: Trovati 39 token di entità\n",
            "DEBUG: Raggruppate in 11 entità\n",
            "DEBUG: Trovate 11 entità da Transformer\n",
            "File salvato in: /content/entita_complete.txt\n",
            "\n",
            "File di output salvato in: /content/entita_complete.txt\n",
            "Verifica file: '/content/entita_complete.txt' esiste e ha dimensione 9444 bytes\n",
            "\n",
            "Sezione Transformer nel file di output contiene 9 righe\n",
            "Le prime 5 entità Transformer nel file:\n",
            "  - 'Italia' → Etichetta: ORG, Punteggio: 0.6448531150817871\n",
            "  - 'Romaoss' → Etichetta: LOC, Punteggio: 0.889772355556488\n",
            "  - 'Milano' → Etichetta: LOC, Punteggio: 0.9906013011932373\n",
            "  - 'Napoli Sa Polite Milanoite Milano' → Etichetta: ORG, Punteggio: 0.9531297087669373\n",
            "  - 'Europaolo' → Etichetta: LOC, Punteggio: 0.7394434809684753\n",
            "  - 'La Ferrari e la Lamborghini' → Etichetta: ORG, Punteggio: 0.9696776866912842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "codice corretto"
      ],
      "metadata": {
        "id": "sGubHDE-Seoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,\n",
        "    original_text=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy, dal riconoscitore italiano e dal riconoscitore Transformer NER.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        transformers_recognizer: Riconoscitore Transformer NER (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']  # Assicurati che questa riga non sia commentata\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in entities:  # Iterate through all entities\n",
        "        if entity.get('source') in ['wikidata', 'wikidata+spacy']:  # Check if the entity is from Wikidata\n",
        "            lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")  # Access Wikidata-specific attributes only if present\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore Transformer e un testo originale, ottieni tutte le entità\n",
        "    if transformers_recognizer and original_text and hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "        try:\n",
        "            print(\"DEBUG: Tentativo di ottenere entità dal riconoscitore Transformer\")\n",
        "            transformers_all_entities = transformers_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not transformers_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "            else:\n",
        "                print(f\"DEBUG: Trovate {len(transformers_all_entities)} entità da Transformer\")\n",
        "                # Rimuovi duplicati mantenendo l'ordine originale\n",
        "                unique_transformers_entities = []\n",
        "                seen = set()\n",
        "                for entity in transformers_all_entities:\n",
        "                    if entity['text'] not in seen:\n",
        "                        unique_transformers_entities.append(entity)\n",
        "                        seen.add(entity['text'])\n",
        "\n",
        "                for entity in unique_transformers_entities:\n",
        "                    # Usa il primo tipo disponibile o 'N/A'\n",
        "                    label = entity.get('label', entity.get('types', ['N/A'])[0] if entity.get('types') else 'N/A')\n",
        "                    score = entity.get('score', 'N/A')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore Transformer: {e}\")\n",
        "            print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità Transformer già filtrate nel metodo find_entities\n",
        "        if not transformers_entities:\n",
        "            if not transformers_recognizer:\n",
        "                lines.append(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            elif not hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "                lines.append(\"Il riconoscitore Transformer NER non ha il metodo recognize_entities.\")\n",
        "            else:\n",
        "                lines.append(\"Per mostrare tutte le entità del riconoscitore Transformer, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Usando {len(transformers_entities)} entità Transformer già filtrate\")\n",
        "            for entity in transformers_entities:\n",
        "                label = entity.get('label', 'N/A')\n",
        "                score = entity.get('score', 'N/A')\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Lista di fallback globale (modelli che funzionano in molte lingue)\n",
        "        self.fallback_models = [\n",
        "            \"dslim/bert-base-NER\",               # Ottimo supporto multilingua\n",
        "            \"dbmdz/bert-large-cased-finetuned-conll03-english\",  # Molto robusto\n",
        "            \"Babelscape/wikineural-multilingual-ner\"  # Supporto multilingua avanzato\n",
        "        ]\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG: Selezionando modello per lingua: {self.language}\")\n",
        "\n",
        "        # Prova prima i modelli specifici per la lingua\n",
        "        models = self.language_models.get(self.language, self.language_models.get(\"en\", []))\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Importa solo il tokenizer per verificare la disponibilità\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Verifica disponibilità modello: {model}\")\n",
        "\n",
        "                # Verifica la disponibilità con caricamento offline\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=False)\n",
        "                print(f\"DEBUG: Modello {model} disponibile e selezionato\")\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Modello {model} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Se nessun modello specifico per lingua funziona, prova i fallback\n",
        "        print(\"DEBUG: Nessun modello specifico per lingua disponibile, provo fallback globali\")\n",
        "        for fallback in self.fallback_models:\n",
        "            try:\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Provo fallback modello: {fallback}\")\n",
        "                tokenizer = AutoTokenizer.from_pretrained(fallback, local_files_only=False)\n",
        "                print(f\"DEBUG: Fallback {fallback} disponibile e selezionato\")\n",
        "                return fallback\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Fallback {fallback} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Ultimo tentativo: dslim/bert-base-NER (molto comune)\n",
        "        print(\"DEBUG: Ultimo tentativo con dslim/bert-base-NER\")\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Assicurati che le dipendenze siano disponibili\n",
        "            import torch\n",
        "            from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "            print(f\"DEBUG: Tentativo di caricare il modello: {self.model_name}\")\n",
        "\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "            print(f\"DEBUG: Usando device {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "            # Prova a caricare con questo modello specifico\n",
        "            try:\n",
        "                # Carica tokenizer e modello\n",
        "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "                model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "                # Crea pipeline NER\n",
        "                self.ner_pipeline = pipeline(\n",
        "                    \"ner\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                self.is_available = True\n",
        "                print(f\"DEBUG: Modello {self.model_name} caricato con successo!\")\n",
        "\n",
        "                # Test rapido per verificare il funzionamento\n",
        "                test_text = \"Mario Rossi vive a Roma.\"\n",
        "                test_result = self.ner_pipeline(test_text)\n",
        "                print(f\"DEBUG: Test modello superato - entità trovate: {len(test_result)}\")\n",
        "                return\n",
        "\n",
        "            except Exception as model_error:\n",
        "                print(f\"DEBUG: Errore nel caricamento del modello specifico: {str(model_error)}\")\n",
        "\n",
        "                # Se il modello specifico fallisce, prova i modelli di fallback dalla lista\n",
        "                for fallback in self.fallback_models:\n",
        "                    if fallback != self.model_name:  # Evita di riprovare lo stesso modello\n",
        "                        try:\n",
        "                            print(f\"DEBUG: Provo modello di fallback: {fallback}\")\n",
        "                            tokenizer = AutoTokenizer.from_pretrained(fallback)\n",
        "                            model = AutoModelForTokenClassification.from_pretrained(fallback)\n",
        "\n",
        "                            self.ner_pipeline = pipeline(\n",
        "                                \"ner\",\n",
        "                                model=model,\n",
        "                                tokenizer=tokenizer,\n",
        "                                device=device\n",
        "                            )\n",
        "\n",
        "                            self.is_available = True\n",
        "                            self.model_name = fallback  # Aggiorna il nome del modello\n",
        "                            print(f\"DEBUG: Modello fallback {fallback} caricato con successo!\")\n",
        "                            return\n",
        "\n",
        "                        except Exception as fallback_error:\n",
        "                            print(f\"DEBUG: Fallback {fallback} fallito: {str(fallback_error)}\")\n",
        "\n",
        "                # Se tutti i fallback falliscono, solleva l'errore originale\n",
        "                raise model_error\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: Errore di importazione: {str(e)}\")\n",
        "            print(\"È necessario installare 'transformers' e 'torch':\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generale nel caricamento del modello: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> list:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer,\n",
        "        con miglioramenti alla tokenizzazione per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            print(f\"DEBUG: Analisi testo con modello {self.model_name}\")\n",
        "\n",
        "            # Strategia 1: Analisi parola per parola (per migliorare la tokenizzazione)\n",
        "            # Dividere il testo in frasi per mantenere un po' di contesto\n",
        "            import re\n",
        "            sentences = re.split(r'[.!?]\\s+', text)\n",
        "\n",
        "            print(f\"DEBUG: Testo diviso in {len(sentences)} frasi\")\n",
        "            all_ner_results = []\n",
        "\n",
        "            for sentence_idx, sentence in enumerate(sentences):\n",
        "                # Calcola l'offset di caratteri per questa frase nel testo originale\n",
        "                sentence_offset = text.find(sentence)\n",
        "                if sentence_offset == -1:  # Se non trova esattamente la frase (a causa di spazi, ecc.)\n",
        "                    # Fai un'approssimazione basata sulle frasi precedenti\n",
        "                    sentence_offset = sum(len(s) + 2 for s in sentences[:sentence_idx])  # +2 per punteggiatura e spazio\n",
        "\n",
        "                print(f\"DEBUG: Analisi frase {sentence_idx+1}/{len(sentences)}\")\n",
        "\n",
        "                # Divide la frase in insiemi di parole (per mantenere un po' di contesto)\n",
        "                words = sentence.split()\n",
        "\n",
        "                # Analizza la frase in piccoli gruppi di parole per mantenere contesto ma migliorare tokenizzazione\n",
        "                window_size = 3  # Numero di parole da analizzare insieme\n",
        "                for i in range(0, len(words), window_size):\n",
        "                    word_group = ' '.join(words[i:i+window_size])\n",
        "\n",
        "                    # Skip gruppi vuoti\n",
        "                    if not word_group.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Calcola offset all'interno della frase\n",
        "                    word_group_offset = sentence.find(word_group)\n",
        "                    if word_group_offset == -1:\n",
        "                        # Approssimazione se non trova esattamente\n",
        "                        word_group_offset = sum(len(w) + 1 for w in words[:i])\n",
        "\n",
        "                    # Offset totale nel testo originale\n",
        "                    total_offset = sentence_offset + word_group_offset\n",
        "\n",
        "                    try:\n",
        "                        # Analizza questo gruppo di parole\n",
        "                        group_results = self.ner_pipeline(word_group)\n",
        "\n",
        "                        # Aggiungi l'offset al risultato\n",
        "                        for result in group_results:\n",
        "                            result['start'] += total_offset\n",
        "                            result['end'] += total_offset\n",
        "                            # Verifica che il contenuto corrisponda al testo originale\n",
        "                            expected_text = text[result['start']:result['end']]\n",
        "                            if expected_text != result['word']:\n",
        "                                print(f\"DEBUG: Mismatch - tokenizer ha '{result['word']}' ma il testo ha '{expected_text}'\")\n",
        "                                # Correzione\n",
        "                                result['word'] = expected_text\n",
        "\n",
        "                        all_ner_results.extend(group_results)\n",
        "                    except Exception as e:\n",
        "                        print(f\"DEBUG: Errore nell'analisi del gruppo di parole '{word_group}': {e}\")\n",
        "\n",
        "            print(f\"DEBUG: Totale token di entità trovati: {len(all_ner_results)}\")\n",
        "\n",
        "            # Ordinamento per posizione per assicurare la sequenza corretta\n",
        "            all_ner_results.sort(key=lambda x: x['start'])\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            # Fix per gestire entità sovrapposte - usa una strategia differente\n",
        "            i = 0\n",
        "            while i < len(all_ner_results):\n",
        "                result = all_ner_results[i]\n",
        "                # Ignora token classificati come \"O\" (Other)\n",
        "                if result['entity'] == 'O':\n",
        "                    i += 1\n",
        "                    continue\n",
        "\n",
        "                # Estrai tipo di entità (rimuovi prefissi B- e I-)\n",
        "                entity_type = result['entity']\n",
        "                if entity_type.startswith('B-') or entity_type.startswith('I-'):\n",
        "                    entity_type = entity_type[2:]\n",
        "\n",
        "                # Inizia una nuova entità\n",
        "                start_pos = result['start']\n",
        "                end_pos = result['end']\n",
        "                entity_text = result['word']\n",
        "                entity_score = result['score']\n",
        "\n",
        "                # Cerca token consecutivi dello stesso tipo\n",
        "                j = i + 1\n",
        "                while j < len(all_ner_results):\n",
        "                    next_result = all_ner_results[j]\n",
        "                    next_type = next_result['entity']\n",
        "                    if next_type.startswith('B-') or next_type.startswith('I-'):\n",
        "                        next_type = next_type[2:]\n",
        "\n",
        "                    # Se è lo stesso tipo ed è adiacente o vicino\n",
        "                    if next_type == entity_type and next_result['start'] <= end_pos + 3:  # +3 per gestire spazi\n",
        "                        # Estendi l'entità\n",
        "                        # Gestisci lo spazio tra token\n",
        "                        if next_result['start'] > end_pos:\n",
        "                            # C'è uno spazio tra i token\n",
        "                            entity_text += ' ' + next_result['word']\n",
        "                        else:\n",
        "                            # Token adiacenti o sovrapposti\n",
        "                            entity_text += next_result['word'][end_pos - next_result['start']:]\n",
        "\n",
        "                        end_pos = next_result['end']\n",
        "                        entity_score = max(entity_score, next_result['score'])\n",
        "                        j += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # Crea l'entità completa\n",
        "                full_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'label': entity_type,\n",
        "                    'start_char': start_pos,\n",
        "                    'end_char': end_pos,\n",
        "                    'score': entity_score,\n",
        "                    'source': 'transformers_ner'\n",
        "                }\n",
        "\n",
        "                # Post-processing per correggere errori comuni nei modelli italiani\n",
        "                full_entity = self._post_process_entity(full_entity, text)\n",
        "\n",
        "                # Aggiungi l'entità se non vuota dopo post-processing\n",
        "                if full_entity and full_entity['text'].strip():\n",
        "                    grouped_entities.append(full_entity)\n",
        "\n",
        "                # Avanza all'indice dopo l'ultimo token usato in questa entità\n",
        "                i = j\n",
        "\n",
        "            print(f\"DEBUG: Raggruppate in {len(grouped_entities)} entità\")\n",
        "            return grouped_entities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nel riconoscimento entità: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return []\n",
        "\n",
        "\n",
        "\n",
        "    def _post_process_entity(self, entity, original_text):\n",
        "        \"\"\"\n",
        "        Applica correzioni post-processing alle entità riconosciute.\n",
        "\n",
        "        Args:\n",
        "            entity: Entità da correggere\n",
        "            original_text: Testo originale per verifica\n",
        "\n",
        "        Returns:\n",
        "            Entità corretta o None se l'entità deve essere scartata\n",
        "        \"\"\"\n",
        "        text = entity['text']\n",
        "\n",
        "        # 1. Correzioni specifiche per errori di tokenizzazione italiani\n",
        "        corrections = {\n",
        "            \"Romaoss\": \"Roma\",\n",
        "            \"Milanoite\": \"Milano\",\n",
        "            \"Europaolo\": \"Europa\",\n",
        "            \"Barolog\": \"Barolo\",\n",
        "            \"Fiorentina\": \"Firenze\",\n",
        "            \"Sa Polite\": \"\",  # Rimuovi completamente\n",
        "            \"Rinascimentolo\": \"Rinascimento\",\n",
        "            \"Vesuviana\": \"Vesuvio\",\n",
        "            \"Napoli Sa\": \"Napoli\",\n",
        "            \"Polite Milano\": \"Milano\",\n",
        "            \"Milanoite Milano\": \"Milano\",\n",
        "            \"Mediterraneano\": \"Mediterraneo\",\n",
        "            \"Lamborghinini\": \"Lamborghini\"\n",
        "        }\n",
        "\n",
        "        # Applica correzioni esatte\n",
        "        for wrong, correct in corrections.items():\n",
        "            if wrong in text:\n",
        "                text = text.replace(wrong, correct)\n",
        "\n",
        "        # 2. Correggi tokenizzazioni errate conosciute (suffissi e pattern comuni)\n",
        "        if \"oss\" in text:\n",
        "            text = text.replace(\"oss\", \"\")\n",
        "\n",
        "        if \"aolo\" in text and \"Europa\" in text:\n",
        "            text = \"Europa\"\n",
        "\n",
        "        if \"ite\" in text and any(city in text for city in [\"Milano\", \"Napoli\", \"Roma\"]):\n",
        "            for city in [\"Milano\", \"Napoli\", \"Roma\"]:\n",
        "                if city in text:\n",
        "                    text = city\n",
        "                    break\n",
        "\n",
        "        # 3. Rimuovi suffissi comuni errati\n",
        "        suffixes = [\"oss\", \"olog\", \"ologna\", \"oni\", \"olino\", \"aolo\", \"ite\", \"inte\"]\n",
        "        for suffix in suffixes:\n",
        "            if text.endswith(suffix):\n",
        "                text = text[:-len(suffix)]\n",
        "\n",
        "        # 4. Gestisci entità con lettere ripetute erroneamente\n",
        "        if any(c*3 in text for c in \"abcdefghijklmnopqrstuvwxyz\"):\n",
        "            # Troppi caratteri ripetuti, probabilmente un errore\n",
        "            return None\n",
        "\n",
        "        # 5. Separa entità erroneamente unite\n",
        "        # Esempio: \"NapoliMilano\" -> \"Napoli\"\n",
        "        cities = [\"Roma\", \"Milano\", \"Napoli\", \"Firenze\", \"Venezia\", \"Torino\", \"Bologna\", \"Palermo\"]\n",
        "        for i, city1 in enumerate(cities):\n",
        "            for city2 in cities[i+1:]:\n",
        "                if city1 in text and city2 in text and len(text) < len(city1) + len(city2) + 3:\n",
        "                    # Probabilmente città unite erroneamente - prendi la prima\n",
        "                    if text.find(city1) < text.find(city2):\n",
        "                        text = city1\n",
        "                    else:\n",
        "                        text = city2\n",
        "                    break\n",
        "\n",
        "        # 6. Gestisci casi specifici di entità concatenate con articoli o preposizioni\n",
        "        # Es: \"LaFerrari\" -> \"Ferrari\"\n",
        "        articles_preps = [\"La\", \"Il\", \"Lo\", \"Gli\", \"Le\", \"Di\", \"Del\", \"Della\", \"Dello\", \"Dei\", \"Degli\", \"Delle\"]\n",
        "        for article in articles_preps:\n",
        "            if text.startswith(article) and len(article) + 1 < len(text):\n",
        "                # Verifica se c'è una lettera maiuscola dopo l'articolo/preposizione\n",
        "                if text[len(article)].isupper():\n",
        "                    text = text[len(article):]  # Rimuovi l'articolo/preposizione\n",
        "\n",
        "        # 7. Verifica che il testo risultante sia effettivamente nel testo originale\n",
        "        if text and text not in original_text:\n",
        "            # Prova a trovare la corrispondenza più vicina nel testo originale\n",
        "            from difflib import get_close_matches\n",
        "            words = original_text.split()\n",
        "            matches = get_close_matches(text, words, n=1)\n",
        "            if matches:\n",
        "                text = matches[0]\n",
        "            else:\n",
        "                # Ancora non trovato, prova una corrispondenza parziale\n",
        "                potential_matches = []\n",
        "                for word in words:\n",
        "                    if len(word) >= 4 and (text in word or word in text):\n",
        "                        potential_matches.append(word)\n",
        "\n",
        "                if potential_matches:\n",
        "                    # Scegli la corrispondenza più lunga (probabilmente la più accurata)\n",
        "                    text = max(potential_matches, key=len)\n",
        "\n",
        "        # 8. Rimuovi testo residuo non significativo\n",
        "        # Se dopo tutte le correzioni, l'entità è troppo breve o è un articolo/preposizione\n",
        "        stop_words = [\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\", \"di\", \"a\", \"da\", \"in\", \"con\", \"su\", \"per\", \"tra\", \"fra\"]\n",
        "        if text.lower() in stop_words or len(text) < 3:\n",
        "            return None\n",
        "\n",
        "        # 9. Pulisci spazi e aggiorna l'entità\n",
        "        text = text.strip()\n",
        "        entity['text'] = text\n",
        "\n",
        "        # 10. Se il testo è diventato vuoto, scarta l'entità\n",
        "        if not entity['text']:\n",
        "            return None\n",
        "\n",
        "        return entity\n",
        "\n",
        "    def process_entities(self, entities, original_text):\n",
        "        \"\"\"\n",
        "        Applica post-processing a tutte le entità riconosciute.\n",
        "\n",
        "        Args:\n",
        "            entities: Lista di entità da correggere\n",
        "            original_text: Testo originale per verifica\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità corrette\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG: Inizio post-processing di {len(entities)} entità\")\n",
        "        corrected_entities = []\n",
        "\n",
        "        for entity in entities:\n",
        "            # Applica post-processing all'entità\n",
        "            corrected = self._post_process_entity(entity, original_text)\n",
        "            if corrected:\n",
        "                # Verifica per evitare duplicati\n",
        "                if not any(e['text'] == corrected['text'] and e['label'] == corrected['label'] for e in corrected_entities):\n",
        "                    corrected_entities.append(corrected)\n",
        "\n",
        "        print(f\"DEBUG: Post-processing completato, {len(corrected_entities)} entità valide rimaste\")\n",
        "        return corrected_entities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP, Transformer NER,\n",
        "    Zero-Shot Classifier e WordNet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, pipeline  # Verifica disponibilità di transformers\n",
        "            import torch  # Verifica disponibilità di torch\n",
        "\n",
        "            print(\"DEBUG: Moduli transformers e torch disponibili. Inizializzazione riconoscitore Transformer NER...\")\n",
        "\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"5had3/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "            # Verifica se il riconoscitore è stato inizializzato correttamente\n",
        "            if not hasattr(self.transformers_recognizer, 'is_available') or not self.transformers_recognizer.is_available:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER non disponibile dopo l'inizializzazione\")\n",
        "            else:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER inizializzato con successo\")\n",
        "\n",
        "            print(\"DEBUG: Inizializzazione classificatore zero-shot...\")\n",
        "\n",
        "            # Verifica che pipeline sia disponibile\n",
        "            from transformers import pipeline\n",
        "            print(\"DEBUG: Transformers pipeline disponibile\")\n",
        "\n",
        "            # Verifica che i modelli per zero-shot siano accessibili\n",
        "            print(f\"DEBUG: Tentativo di accesso a modello zero-shot: facebook/bart-large-mnli\")\n",
        "\n",
        "            self.zero_shot_classifier = ZeroShotClassifier(language=language, use_gpu=use_gpu)\n",
        "\n",
        "            if hasattr(self.zero_shot_classifier, 'is_available'):\n",
        "                print(f\"DEBUG: Zero-shot classifier disponibilità: {self.zero_shot_classifier.is_available}\")\n",
        "                if not self.zero_shot_classifier.is_available:\n",
        "                    print(\"DEBUG: Classificatore zero-shot non disponibile dopo l'inizializzazione\")\n",
        "                else:\n",
        "                    print(f\"DEBUG: Classificatore zero-shot inizializzato con successo: {self.zero_shot_classifier.model_name}\")\n",
        "            else:\n",
        "                print(\"DEBUG: L'attributo is_available non esiste nel classificatore\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - Classificatore zero-shot non disponibile. {e}\")\n",
        "            print(\"Installare 'transformers' e 'torch' con 'pip install transformers torch'\")\n",
        "            self.zero_shot_classifier = None\n",
        "        except Exception as e:\n",
        "          print(f\"DEBUG: Errore generico nell'inizializzazione del classificatore zero-shot: {e}\")\n",
        "          import traceback\n",
        "          traceback.print_exc()\n",
        "          self.zero_shot_classifier = None\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - Transformer NER non disponibile. {e}\")\n",
        "            print(\"Installare 'transformers' e 'torch' con 'pip install transformers torch'\")\n",
        "            self.transformers_recognizer = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione del riconoscitore Transformer NER: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "        # Inizializza il classificatore zero-shot\n",
        "        try:\n",
        "            print(\"DEBUG: Inizializzazione classificatore zero-shot...\")\n",
        "            self.zero_shot_classifier = ZeroShotClassifier(language=language, use_gpu=use_gpu)\n",
        "\n",
        "            if not self.zero_shot_classifier.is_available:\n",
        "                print(\"DEBUG: Classificatore zero-shot non disponibile\")\n",
        "            else:\n",
        "                print(\"DEBUG: Classificatore zero-shot inizializzato con successo\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - Classificatore zero-shot non disponibile. {e}\")\n",
        "            print(\"Installare 'transformers' e 'torch' con 'pip install transformers torch'\")\n",
        "            self.zero_shot_classifier = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione del classificatore zero-shot: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.zero_shot_classifier = None\n",
        "\n",
        "        # Inizializza WordNet knowledge base\n",
        "        try:\n",
        "            print(\"DEBUG: Inizializzazione WordNet knowledge base...\")\n",
        "            self.wordnet_kb = WordNetKnowledgeBase(language=\"it\")\n",
        "\n",
        "            if not self.wordnet_kb.is_available:\n",
        "                print(\"DEBUG: WordNet knowledge base non disponibile\")\n",
        "            else:\n",
        "                print(\"DEBUG: WordNet knowledge base inizializzata con successo\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - WordNet non disponibile. {e}\")\n",
        "            print(\"Installare 'nltk' con 'pip install nltk'\")\n",
        "            self.wordnet_kb = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione di WordNet: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.wordnet_kb = None\n",
        "\n",
        "\n",
        "        def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando tutti i riconoscitori disponibili\n",
        "        e arricchisce con WordNet e zero-shot classification.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug dello stato dei riconoscitori\n",
        "        print(f\"DEBUG: Stato riconoscitori:\")\n",
        "        print(f\"- SpaCy: {self.wikidata.spacy_recognizer.is_available}\")\n",
        "        print(f\"- Italian NLP: {self.italian_recognizer.is_available}\")\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer:\n",
        "            print(f\"- Transformer NER: {self.transformers_recognizer.is_available}\")\n",
        "        if hasattr(self, 'zero_shot_classifier') and self.zero_shot_classifier:\n",
        "            print(f\"- Zero-Shot: {self.zero_shot_classifier.is_available}\")\n",
        "        if hasattr(self, 'wordnet_kb') and self.wordnet_kb:\n",
        "            print(f\"- WordNet KB: {self.wordnet_kb.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiorna il set di testi esistenti\n",
        "        for entity in italian_only_entities:\n",
        "            if 'text' in entity:\n",
        "                existing_texts.add(entity['text'].lower())\n",
        "\n",
        "       # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = []\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Riconoscimento entità con Transformer NER...\")\n",
        "            try:\n",
        "                transformers_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "                print(f\"DEBUG: {len(transformers_entities)} entità trovate da Transformer NER\")\n",
        "\n",
        "                # Aggiungi solo se ci sono entità\n",
        "                if transformers_entities:\n",
        "                    enriched_entities.extend(transformers_entities)\n",
        "\n",
        "                    # Aggiorna il set di testi esistenti\n",
        "                    for entity in transformers_entities:\n",
        "                        if 'text' in entity:\n",
        "                            existing_texts.add(entity['text'].lower())\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nel riconoscimento entità con Transformer NER: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile\")\n",
        "\n",
        "        # Raccogli statistiche base\n",
        "        stats_base = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': stats.get('wikidata_entities', 0),\n",
        "            'spacy_only_entities': stats.get('spacy_only_entities', 0),\n",
        "            'italian_only_entities': len(italian_only_entities),\n",
        "            'transformers_only_entities': len(transformers_entities),\n",
        "            'local_entities': stats.get('local_entities', 0),\n",
        "            'entities_with_dbpedia': stats.get('entities_with_dbpedia', 0),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        # Applica classificazione zero-shot se disponibile\n",
        "        if hasattr(self, 'zero_shot_classifier') and self.zero_shot_classifier and self.zero_shot_classifier.is_available:\n",
        "            print(\"DEBUG: Applicazione classificazione zero-shot...\")\n",
        "            try:\n",
        "                # Classifica le entità che non hanno un tipo\n",
        "                enriched_entities = self.zero_shot_classifier.classify_entities(enriched_entities, text)\n",
        "\n",
        "                # Aggiorna statistiche\n",
        "                stats_base['entities_with_zero_shot'] = sum(1 for e in enriched_entities if 'zero_shot_classification' in e)\n",
        "\n",
        "                print(f\"DEBUG: {stats_base['entities_with_zero_shot']} entità classificate con zero-shot\")\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nella classificazione zero-shot: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "        # Applica arricchimento semantico con WordNet se disponibile\n",
        "        if hasattr(self, 'wordnet_kb') and self.wordnet_kb and self.wordnet_kb.is_available:\n",
        "            print(\"DEBUG: Applicazione arricchimento semantico con WordNet...\")\n",
        "            try:\n",
        "                # Arricchisci le entità con WordNet\n",
        "                enriched_entities = self.wordnet_kb.enrich_entities(enriched_entities)\n",
        "\n",
        "                # Aggiorna statistiche\n",
        "                stats_base['entities_with_wordnet'] = sum(1 for e in enriched_entities if 'semantic_info' in e)\n",
        "\n",
        "                print(f\"DEBUG: {stats_base['entities_with_wordnet']} entità arricchite con WordNet\")\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nell'arricchimento semantico con WordNet: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "        return enriched_entities, stats_base\n",
        "\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "          \"\"\"\n",
        "          Ottiene entità riconosciute dal riconoscitore Transformer NER.\n",
        "          Includendo sia entità nuove che quelle già trovate da altre fonti.\n",
        "          \"\"\"\n",
        "          # Verifica che il riconoscitore Transformer sia disponibile\n",
        "          if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "              print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "              return []\n",
        "\n",
        "          # Stampa informazioni sul riconoscitore\n",
        "          print(f\"DEBUG: Modello Transformer NER in uso: {self.transformers_recognizer.model_name}\")\n",
        "\n",
        "          try:\n",
        "              # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "              transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "\n",
        "              # Debug dettagliato delle entità\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER trovate: {len(transformers_entities)}\")\n",
        "\n",
        "              # Filtra e mappa le entità - IMPORTANTE: non filtriamo per existing_texts\n",
        "              # per assicurarci che tutte le entità Transformer siano incluse\n",
        "              transformers_mapped = []\n",
        "              for entity in transformers_entities:\n",
        "                  if self._is_valid_entity(entity.get('text', '')):\n",
        "                      # Aggiungi sempre il source 'transformers_ner'\n",
        "                      mapped_entity = {\n",
        "                          'text': entity.get('text', ''),\n",
        "                          'label': entity.get('label', ''),\n",
        "                          'types': [entity.get('label', 'N/A')],\n",
        "                          'source': 'transformers_ner',\n",
        "                          'score': entity.get('score', 0)\n",
        "                      }\n",
        "                      transformers_mapped.append(mapped_entity)\n",
        "\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER mappate: {len(transformers_mapped)}\")\n",
        "              return transformers_mapped\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "              import traceback\n",
        "              traceback.print_exc()\n",
        "              return []\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Importazioni necessarie\n",
        "    import os\n",
        "    import sys\n",
        "    import traceback\n",
        "\n",
        "    # Imposta il livello di log per transformers\n",
        "    os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"info\"\n",
        "\n",
        "    # Verifica le dipendenze\n",
        "    try:\n",
        "        import torch\n",
        "        import transformers\n",
        "        print(f\"PyTorch versione: {torch.__version__}\")\n",
        "        print(f\"Transformers versione: {transformers.__version__}\")\n",
        "        print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"ATTENZIONE: Libreria mancante - {e}\")\n",
        "\n",
        "    try:\n",
        "        # Crea un'istanza del linker di entità migliorato con debug aggiuntivo\n",
        "        print(\"Inizializzazione EntityLinkerItalian...\")\n",
        "        linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "        # Verifica disponibilità dei riconoscitori\n",
        "        print(f\"SpaCy riconoscitore disponibile: {linker.wikidata.spacy_recognizer.is_available}\")\n",
        "        print(f\"Italian NLP riconoscitore disponibile: {linker.italian_recognizer.is_available}\")\n",
        "        if hasattr(linker, 'transformers_recognizer') and linker.transformers_recognizer:\n",
        "            print(f\"Transformer NER riconoscitore disponibile: {linker.transformers_recognizer.is_available}\")\n",
        "            print(f\"Transformer NER modello: {linker.transformers_recognizer.model_name}\")\n",
        "        else:\n",
        "            print(\"Transformer NER riconoscitore non disponibile\")\n",
        "\n",
        "        # Testo di esempio\n",
        "        text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "               \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "               \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "               \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "               \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "               \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "               \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "               \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "               \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "               \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "        print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "        # Test esplicito del riconoscitore Transformer NER, se disponibile\n",
        "        if hasattr(linker, 'transformers_recognizer') and linker.transformers_recognizer and linker.transformers_recognizer.is_available:\n",
        "            print(\"\\nTest diretto del riconoscitore Transformer NER:\")\n",
        "            test_sample = \"Mario Rossi vive a Roma e lavora per l'Università La Sapienza.\"\n",
        "            try:\n",
        "                transformer_test_entities = linker.transformers_recognizer.recognize_entities(test_sample)\n",
        "                print(f\"Entità trovate dal riconoscitore Transformer in test: {len(transformer_test_entities)}\")\n",
        "                for entity in transformer_test_entities:\n",
        "                    print(f\"- '{entity['text']}' → Etichetta: {entity.get('label')}, Score: {entity.get('score')}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Errore nel test diretto del riconoscitore Transformer: {e}\")\n",
        "                traceback.print_exc()\n",
        "\n",
        "        # Trova entità\n",
        "        print(\"\\nRicerca entità nel testo principale...\")\n",
        "        entities, stats = linker.find_entities(text)\n",
        "\n",
        "        # Verifica entità Transformer\n",
        "        transformer_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "        print(f\"\\nEntità Transformer NER trovate: {len(transformer_entities)}\")\n",
        "        for entity in transformer_entities[:10]:  # Mostra solo le prime 10 per brevità\n",
        "            print(f\"- '{entity['text']}' → Etichetta: {entity.get('label')}, Score: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        print(f\"\\nTrovate {stats['total_entities']} entità totali:\")\n",
        "        for k, v in stats.items():\n",
        "            print(f\"- {k}: {v}\")\n",
        "\n",
        "        # Esporta le entità in un file\n",
        "        print(\"\\nEsportazione entità in file...\")\n",
        "        output_file_path = export_entities_to_txt(\n",
        "            entities,\n",
        "            stats,\n",
        "            \"entita_complete.txt\",  # Nuovo nome file per distinguerlo dal precedente\n",
        "            spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "            italian_recognizer=linker.italian_recognizer,\n",
        "            transformers_recognizer=linker.transformers_recognizer,\n",
        "            original_text=text\n",
        "        )\n",
        "\n",
        "        print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "        # Verifica il file di output\n",
        "        import os\n",
        "        if os.path.exists(output_file_path):\n",
        "            file_size = os.path.getsize(output_file_path)\n",
        "            print(f\"Verifica file: '{output_file_path}' esiste e ha dimensione {file_size} bytes\")\n",
        "\n",
        "            # Leggi il file per verificare il contenuto\n",
        "            with open(output_file_path, 'r', encoding='utf-8') as f:\n",
        "                content = f.read()\n",
        "\n",
        "            # Cerca la sezione Transformer nel file\n",
        "            if \"TRANSFORMER NER ENTITIES:\" in content:\n",
        "                transformer_section = content.split(\"TRANSFORMER NER ENTITIES:\")[1].split(\"\\n\\n\")[0]\n",
        "                transformer_lines = [line for line in transformer_section.split(\"\\n\") if line.strip() and \"-----\" not in line]\n",
        "                print(f\"\\nSezione Transformer nel file di output contiene {len(transformer_lines)} righe\")\n",
        "                if len(transformer_lines) > 1:  # La prima riga è l'intestazione\n",
        "                    print(\"Le prime 5 entità Transformer nel file:\")\n",
        "                    for line in transformer_lines[:6]:\n",
        "                        print(f\"  {line}\")\n",
        "                else:\n",
        "                    print(\"ATTENZIONE: Nessuna entità Transformer trovata nel file di output\")\n",
        "            else:\n",
        "                print(\"ATTENZIONE: Sezione Transformer non trovata nel file di output\")\n",
        "        else:\n",
        "            print(f\"ATTENZIONE: Il file '{output_file_path}' non è stato creato!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore generale: {e}\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "id": "pVNV_WKu7DiU",
        "outputId": "e704cd5b-faf4-49fe-e2ee-17cb4896818b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "expected an indented block after function definition on line 1958 (<ipython-input-11-4e12beceb7d2>, line 1959)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-11-4e12beceb7d2>\"\u001b[0;36m, line \u001b[0;32m1959\u001b[0m\n\u001b[0;31m    \"\"\"\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 1958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "variazione ultima"
      ],
      "metadata": {
        "id": "BIk8LdtEBszV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')  # Open Multilingual WordNet\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,\n",
        "    zero_shot_classifier=None,  # Nuovo parametro\n",
        "    wordnet_kb=None,            # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da tutti i riconoscitori e gli arricchitori.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        transformers_recognizer: Riconoscitore Transformer NER (opzionale)\n",
        "        zero_shot_classifier: Classificatore zero-shot (opzionale)\n",
        "        wordnet_kb: Knowledge base WordNet (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "    zero_shot_entities = [e for e in entities if 'zero_shot_classification' in e]\n",
        "    wordnet_entities = [e for e in entities if 'semantic_info' in e]\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in entities:\n",
        "        if entity.get('source') in ['wikidata', 'wikidata+spacy']:\n",
        "            lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore Transformer e un testo originale, ottieni tutte le entità\n",
        "    if transformers_recognizer and original_text and hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "        try:\n",
        "            print(\"DEBUG: Tentativo di ottenere entità dal riconoscitore Transformer\")\n",
        "            transformers_all_entities = transformers_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not transformers_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "            else:\n",
        "                print(f\"DEBUG: Trovate {len(transformers_all_entities)} entità da Transformer\")\n",
        "                # Rimuovi duplicati mantenendo l'ordine originale\n",
        "                unique_transformers_entities = []\n",
        "                seen = set()\n",
        "                for entity in transformers_all_entities:\n",
        "                    if entity['text'] not in seen:\n",
        "                        unique_transformers_entities.append(entity)\n",
        "                        seen.add(entity['text'])\n",
        "\n",
        "                for entity in unique_transformers_entities:\n",
        "                    # Usa il primo tipo disponibile o 'N/A'\n",
        "                    label = entity.get('label', entity.get('types', ['N/A'])[0] if entity.get('types') else 'N/A')\n",
        "                    score = entity.get('score', 'N/A')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore Transformer: {e}\")\n",
        "            print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità Transformer già filtrate nel metodo find_entities\n",
        "        if not transformers_entities:\n",
        "            if not transformers_recognizer:\n",
        "                lines.append(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            elif not hasattr(transformers_recognizer, 'recognize_entities'):\n",
        "                lines.append(\"Il riconoscitore Transformer NER non ha il metodo recognize_entities.\")\n",
        "            else:\n",
        "                lines.append(\"Per mostrare tutte le entità del riconoscitore Transformer, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            print(f\"DEBUG: Usando {len(transformers_entities)} entità Transformer già filtrate\")\n",
        "            for entity in transformers_entities:\n",
        "                label = entity.get('label', 'N/A')\n",
        "                score = entity.get('score', 'N/A')\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Punteggio: {score}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # NUOVA SEZIONE: Entità classificate con Zero-Shot\n",
        "    lines.append(\"ZERO-SHOT CLASSIFIED ENTITIES:\")\n",
        "    lines.append(\"------------------------------\")\n",
        "    if zero_shot_entities:\n",
        "        for entity in zero_shot_entities:\n",
        "            classification = entity.get('zero_shot_classification', {})\n",
        "            label = classification.get('label', 'N/A')\n",
        "            score = classification.get('score', 'N/A')\n",
        "            lines.append(f\"- '{entity['text']}' → Classe: {label}, Confidenza: {score:.4f}\")\n",
        "\n",
        "            # Mostra anche altri punteggi se disponibili\n",
        "            scores = classification.get('scores', {})\n",
        "            if scores and len(scores) > 1:\n",
        "                lines.append(f\"  Altri punteggi:\")\n",
        "                for score_label, score_value in scores.items():\n",
        "                    if score_label != label:  # Salta l'etichetta principale\n",
        "                        lines.append(f\"  - {score_label}: {score_value:.4f}\")\n",
        "    else:\n",
        "        lines.append(\"Nessuna entità classificata con Zero-Shot.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # NUOVA SEZIONE: Entità arricchite con WordNet\n",
        "    lines.append(\"WORDNET SEMANTIC ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "    if wordnet_entities:\n",
        "        for entity in wordnet_entities:\n",
        "            semantic_info = entity.get('semantic_info', {})\n",
        "            lines.append(f\"- '{entity['text']}':\")\n",
        "\n",
        "            # Definizioni\n",
        "            definitions = semantic_info.get('definitions', [])\n",
        "            if definitions:\n",
        "                lines.append(f\"  Definizioni:\")\n",
        "                for i, definition in enumerate(definitions[:2]):  # Mostra solo le prime 2 definizioni\n",
        "                    lines.append(f\"  {i+1}. {definition}\")\n",
        "\n",
        "            # Sinonimi\n",
        "            synonyms = semantic_info.get('synonyms', [])\n",
        "            if synonyms:\n",
        "                lines.append(f\"  Sinonimi: {', '.join(synonyms[:5])}\")  # Mostra solo i primi 5 sinonimi\n",
        "\n",
        "            # Iperonimi (concetti più generali)\n",
        "            hypernyms = semantic_info.get('hypernyms', [])\n",
        "            if hypernyms:\n",
        "                lines.append(f\"  Concetti generali:\")\n",
        "                for i, hypernym in enumerate(hypernyms[:2]):  # Mostra solo i primi 2 iperonimi\n",
        "                    lines.append(f\"  - {hypernym.get('name', 'N/A')}\")\n",
        "    else:\n",
        "        lines.append(\"Nessuna entità arricchita con WordNet.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "\n",
        "\n",
        "                \"dbmdz/bert-base-italian-xxl-cased\",        # Modello italiano grande\n",
        "                \"Babelscape/wikineural-multilingual-ner\",   # Ottimo per NER multilingua\n",
        "                \"Davlan/xlm-roberta-base-ner-hrl\",          # Buon supporto per italiano\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",        # Altra alternativa italiana\n",
        "                \"5had3/bert-base-italian-cased-ner\",        # Modello che dava problemi - spostato in fondo\n",
        "                \"dslim/bert-base-NER\"\n",
        "\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Lista di fallback globale (modelli che funzionano in molte lingue)\n",
        "        self.fallback_models = [\n",
        "            \"dslim/bert-base-NER\",               # Ottimo supporto multilingua\n",
        "            \"dbmdz/bert-large-cased-finetuned-conll03-english\",  # Molto robusto\n",
        "            \"Babelscape/wikineural-multilingual-ner\"  # Supporto multilingua avanzato\n",
        "        ]\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG: Selezionando modello per lingua: {self.language}\")\n",
        "\n",
        "        # Prova prima i modelli specifici per la lingua\n",
        "        models = self.language_models.get(self.language, self.language_models.get(\"en\", []))\n",
        "        print(f\"DEBUG: Proverò questi modelli nell'ordine: {models}\")\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Importa solo il tokenizer per verificare la disponibilità\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Verifica disponibilità modello: {model}\")\n",
        "\n",
        "                # Verifica la disponibilità con caricamento offline\n",
        "                tokenizer = AutoTokenizer.from_pretrained(model, local_files_only=False)\n",
        "                print(f\"DEBUG: Modello {model} disponibile e selezionato\")\n",
        "                return model\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Modello {model} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Se nessun modello specifico per lingua funziona, prova i fallback\n",
        "        print(\"DEBUG: Nessun modello specifico per lingua disponibile, provo fallback globali\")\n",
        "        for fallback in self.fallback_models:\n",
        "            try:\n",
        "                from transformers import AutoTokenizer\n",
        "                print(f\"DEBUG: Provo fallback modello: {fallback}\")\n",
        "                tokenizer = AutoTokenizer.from_pretrained(fallback, local_files_only=False)\n",
        "                print(f\"DEBUG: Fallback {fallback} disponibile e selezionato\")\n",
        "                return fallback\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Fallback {fallback} non disponibile: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # Ultimo tentativo: dslim/bert-base-NER (molto comune)\n",
        "        print(\"DEBUG: Ultimo tentativo con dslim/bert-base-NER\")\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Assicurati che le dipendenze siano disponibili\n",
        "            import torch\n",
        "            from transformers import pipeline, AutoTokenizer, AutoModelForTokenClassification\n",
        "\n",
        "            print(f\"DEBUG: Tentativo di caricare il modello: {self.model_name}\")\n",
        "\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "            print(f\"DEBUG: Usando device {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "            # Prova a caricare con questo modello specifico\n",
        "            try:\n",
        "                # Carica tokenizer e modello\n",
        "                tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "                model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "                # Crea pipeline NER\n",
        "                self.ner_pipeline = pipeline(\n",
        "                    \"ner\",\n",
        "                    model=model,\n",
        "                    tokenizer=tokenizer,\n",
        "                    device=device\n",
        "                )\n",
        "\n",
        "                self.is_available = True\n",
        "                print(f\"DEBUG: Modello {self.model_name} caricato con successo!\")\n",
        "\n",
        "                # Test rapido per verificare il funzionamento\n",
        "                test_text = \"Mario Rossi vive a Roma.\"\n",
        "                test_result = self.ner_pipeline(test_text)\n",
        "                print(f\"DEBUG: Test modello superato - entità trovate: {len(test_result)}\")\n",
        "                return\n",
        "\n",
        "            except Exception as model_error:\n",
        "                print(f\"DEBUG: Errore nel caricamento del modello specifico: {str(model_error)}\")\n",
        "\n",
        "                # Se il modello specifico fallisce, prova i modelli di fallback dalla lista\n",
        "                for fallback in self.fallback_models:\n",
        "                    if fallback != self.model_name:  # Evita di riprovare lo stesso modello\n",
        "                        try:\n",
        "                            print(f\"DEBUG: Provo modello di fallback: {fallback}\")\n",
        "                            tokenizer = AutoTokenizer.from_pretrained(fallback)\n",
        "                            model = AutoModelForTokenClassification.from_pretrained(fallback)\n",
        "\n",
        "                            self.ner_pipeline = pipeline(\n",
        "                                \"ner\",\n",
        "                                model=model,\n",
        "                                tokenizer=tokenizer,\n",
        "                                device=device\n",
        "                            )\n",
        "\n",
        "                            self.is_available = True\n",
        "                            self.model_name = fallback  # Aggiorna il nome del modello\n",
        "                            print(f\"DEBUG: Modello fallback {fallback} caricato con successo!\")\n",
        "                            return\n",
        "\n",
        "                        except Exception as fallback_error:\n",
        "                            print(f\"DEBUG: Fallback {fallback} fallito: {str(fallback_error)}\")\n",
        "\n",
        "                # Se tutti i fallback falliscono, solleva l'errore originale\n",
        "                raise model_error\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: Errore di importazione: {str(e)}\")\n",
        "            print(\"È necessario installare 'transformers' e 'torch':\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generale nel caricamento del modello: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> list:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer,\n",
        "        con miglioramenti alla tokenizzazione per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            print(f\"DEBUG: Analisi testo con modello {self.model_name}\")\n",
        "\n",
        "            # Strategia 1: Analisi parola per parola (per migliorare la tokenizzazione)\n",
        "            # Dividere il testo in frasi per mantenere un po' di contesto\n",
        "            import re\n",
        "            sentences = re.split(r'[.!?]\\s+', text)\n",
        "\n",
        "            print(f\"DEBUG: Testo diviso in {len(sentences)} frasi\")\n",
        "            all_ner_results = []\n",
        "\n",
        "            for sentence_idx, sentence in enumerate(sentences):\n",
        "                # Calcola l'offset di caratteri per questa frase nel testo originale\n",
        "                sentence_offset = text.find(sentence)\n",
        "                if sentence_offset == -1:  # Se non trova esattamente la frase (a causa di spazi, ecc.)\n",
        "                    # Fai un'approssimazione basata sulle frasi precedenti\n",
        "                    sentence_offset = sum(len(s) + 2 for s in sentences[:sentence_idx])  # +2 per punteggiatura e spazio\n",
        "\n",
        "                print(f\"DEBUG: Analisi frase {sentence_idx+1}/{len(sentences)}\")\n",
        "\n",
        "                # Divide la frase in insiemi di parole (per mantenere un po' di contesto)\n",
        "                words = sentence.split()\n",
        "\n",
        "                # Analizza la frase in piccoli gruppi di parole per mantenere contesto ma migliorare tokenizzazione\n",
        "                window_size = 3  # Numero di parole da analizzare insieme\n",
        "                for i in range(0, len(words), window_size):\n",
        "                    word_group = ' '.join(words[i:i+window_size])\n",
        "\n",
        "                    # Skip gruppi vuoti\n",
        "                    if not word_group.strip():\n",
        "                        continue\n",
        "\n",
        "                    # Calcola offset all'interno della frase\n",
        "                    word_group_offset = sentence.find(word_group)\n",
        "                    if word_group_offset == -1:\n",
        "                        # Approssimazione se non trova esattamente\n",
        "                        word_group_offset = sum(len(w) + 1 for w in words[:i])\n",
        "\n",
        "                    # Offset totale nel testo originale\n",
        "                    total_offset = sentence_offset + word_group_offset\n",
        "\n",
        "                    try:\n",
        "                        # Analizza questo gruppo di parole\n",
        "                        group_results = self.ner_pipeline(word_group)\n",
        "\n",
        "                        # Aggiungi l'offset al risultato\n",
        "                        for result in group_results:\n",
        "                            result['start'] += total_offset\n",
        "                            result['end'] += total_offset\n",
        "                            # Verifica che il contenuto corrisponda al testo originale\n",
        "                            expected_text = text[result['start']:result['end']]\n",
        "                            if expected_text != result['word']:\n",
        "                                print(f\"DEBUG: Mismatch - tokenizer ha '{result['word']}' ma il testo ha '{expected_text}'\")\n",
        "                                # Correzione\n",
        "                                result['word'] = expected_text\n",
        "\n",
        "                        all_ner_results.extend(group_results)\n",
        "                    except Exception as e:\n",
        "                        print(f\"DEBUG: Errore nell'analisi del gruppo di parole '{word_group}': {e}\")\n",
        "\n",
        "            print(f\"DEBUG: Totale token di entità trovati: {len(all_ner_results)}\")\n",
        "\n",
        "            # Ordinamento per posizione per assicurare la sequenza corretta\n",
        "            all_ner_results.sort(key=lambda x: x['start'])\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            # Fix per gestire entità sovrapposte - usa una strategia differente\n",
        "            i = 0\n",
        "            while i < len(all_ner_results):\n",
        "                result = all_ner_results[i]\n",
        "                # Ignora token classificati come \"O\" (Other)\n",
        "                if result['entity'] == 'O':\n",
        "                    i += 1\n",
        "                    continue\n",
        "\n",
        "                # Estrai tipo di entità (rimuovi prefissi B- e I-)\n",
        "                entity_type = result['entity']\n",
        "                if entity_type.startswith('B-') or entity_type.startswith('I-'):\n",
        "                    entity_type = entity_type[2:]\n",
        "\n",
        "                # Inizia una nuova entità\n",
        "                start_pos = result['start']\n",
        "                end_pos = result['end']\n",
        "                entity_text = result['word']\n",
        "                entity_score = result['score']\n",
        "\n",
        "                # Cerca token consecutivi dello stesso tipo\n",
        "                j = i + 1\n",
        "                while j < len(all_ner_results):\n",
        "                    next_result = all_ner_results[j]\n",
        "                    next_type = next_result['entity']\n",
        "                    if next_type.startswith('B-') or next_type.startswith('I-'):\n",
        "                        next_type = next_type[2:]\n",
        "\n",
        "                    # Se è lo stesso tipo ed è adiacente o vicino\n",
        "                    if next_type == entity_type and next_result['start'] <= end_pos + 3:  # +3 per gestire spazi\n",
        "                        # Estendi l'entità\n",
        "                        # Gestisci lo spazio tra token\n",
        "                        if next_result['start'] > end_pos:\n",
        "                            # C'è uno spazio tra i token\n",
        "                            entity_text += ' ' + next_result['word']\n",
        "                        else:\n",
        "                            # Token adiacenti o sovrapposti\n",
        "                            entity_text += next_result['word'][end_pos - next_result['start']:]\n",
        "\n",
        "                        end_pos = next_result['end']\n",
        "                        entity_score = max(entity_score, next_result['score'])\n",
        "                        j += 1\n",
        "                    else:\n",
        "                        break\n",
        "\n",
        "                # Crea l'entità completa\n",
        "                full_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'label': entity_type,\n",
        "                    'start_char': start_pos,\n",
        "                    'end_char': end_pos,\n",
        "                    'score': entity_score,\n",
        "                    'source': 'transformers_ner'\n",
        "                }\n",
        "\n",
        "                # Post-processing per correggere errori comuni nei modelli italiani\n",
        "                full_entity = self._post_process_entity(full_entity, text)\n",
        "\n",
        "                # Aggiungi l'entità se non vuota dopo post-processing\n",
        "                if full_entity and full_entity['text'].strip():\n",
        "                    grouped_entities.append(full_entity)\n",
        "\n",
        "                # Avanza all'indice dopo l'ultimo token usato in questa entità\n",
        "                i = j\n",
        "\n",
        "            print(f\"DEBUG: Raggruppate in {len(grouped_entities)} entità\")\n",
        "            return grouped_entities\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nel riconoscimento entità: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return []\n",
        "\n",
        "\n",
        "\n",
        "    def _post_process_entity(self, entity, original_text):\n",
        "        \"\"\"\n",
        "        Applica correzioni post-processing alle entità riconosciute.\n",
        "\n",
        "        Args:\n",
        "            entity: Entità da correggere\n",
        "            original_text: Testo originale per verifica\n",
        "\n",
        "        Returns:\n",
        "            Entità corretta o None se l'entità deve essere scartata\n",
        "        \"\"\"\n",
        "        text = entity['text']\n",
        "\n",
        "        # 1. Correzioni specifiche per errori di tokenizzazione italiani\n",
        "        corrections = {\n",
        "            \"Romaoss\": \"Roma\",\n",
        "            \"Milanoite\": \"Milano\",\n",
        "            \"Europaolo\": \"Europa\",\n",
        "            \"Barolog\": \"Barolo\",\n",
        "            \"Fiorentina\": \"Firenze\",\n",
        "            \"Sa Polite\": \"\",  # Rimuovi completamente\n",
        "            \"Rinascimentolo\": \"Rinascimento\",\n",
        "            \"Vesuviana\": \"Vesuvio\",\n",
        "            \"Napoli Sa\": \"Napoli\",\n",
        "            \"Polite Milano\": \"Milano\",\n",
        "            \"Milanoite Milano\": \"Milano\",\n",
        "            \"Mediterraneano\": \"Mediterraneo\",\n",
        "            \"Lamborghinini\": \"Lamborghini\"\n",
        "        }\n",
        "\n",
        "        # Applica correzioni esatte\n",
        "        for wrong, correct in corrections.items():\n",
        "            if wrong in text:\n",
        "                text = text.replace(wrong, correct)\n",
        "\n",
        "        # 2. Correggi tokenizzazioni errate conosciute (suffissi e pattern comuni)\n",
        "        if \"oss\" in text:\n",
        "            text = text.replace(\"oss\", \"\")\n",
        "\n",
        "        if \"aolo\" in text and \"Europa\" in text:\n",
        "            text = \"Europa\"\n",
        "\n",
        "        if \"ite\" in text and any(city in text for city in [\"Milano\", \"Napoli\", \"Roma\"]):\n",
        "            for city in [\"Milano\", \"Napoli\", \"Roma\"]:\n",
        "                if city in text:\n",
        "                    text = city\n",
        "                    break\n",
        "\n",
        "        # 3. Rimuovi suffissi comuni errati\n",
        "        suffixes = [\"oss\", \"olog\", \"ologna\", \"oni\", \"olino\", \"aolo\", \"ite\", \"inte\"]\n",
        "        for suffix in suffixes:\n",
        "            if text.endswith(suffix):\n",
        "                text = text[:-len(suffix)]\n",
        "\n",
        "        # 4. Gestisci entità con lettere ripetute erroneamente\n",
        "        if any(c*3 in text for c in \"abcdefghijklmnopqrstuvwxyz\"):\n",
        "            # Troppi caratteri ripetuti, probabilmente un errore\n",
        "            return None\n",
        "\n",
        "        # 5. Separa entità erroneamente unite\n",
        "        # Esempio: \"NapoliMilano\" -> \"Napoli\"\n",
        "        cities = [\"Roma\", \"Milano\", \"Napoli\", \"Firenze\", \"Venezia\", \"Torino\", \"Bologna\", \"Palermo\"]\n",
        "        for i, city1 in enumerate(cities):\n",
        "            for city2 in cities[i+1:]:\n",
        "                if city1 in text and city2 in text and len(text) < len(city1) + len(city2) + 3:\n",
        "                    # Probabilmente città unite erroneamente - prendi la prima\n",
        "                    if text.find(city1) < text.find(city2):\n",
        "                        text = city1\n",
        "                    else:\n",
        "                        text = city2\n",
        "                    break\n",
        "\n",
        "        # 6. Gestisci casi specifici di entità concatenate con articoli o preposizioni\n",
        "        # Es: \"LaFerrari\" -> \"Ferrari\"\n",
        "        articles_preps = [\"La\", \"Il\", \"Lo\", \"Gli\", \"Le\", \"Di\", \"Del\", \"Della\", \"Dello\", \"Dei\", \"Degli\", \"Delle\"]\n",
        "        for article in articles_preps:\n",
        "            if text.startswith(article) and len(article) + 1 < len(text):\n",
        "                # Verifica se c'è una lettera maiuscola dopo l'articolo/preposizione\n",
        "                if text[len(article)].isupper():\n",
        "                    text = text[len(article):]  # Rimuovi l'articolo/preposizione\n",
        "\n",
        "        # 7. Verifica che il testo risultante sia effettivamente nel testo originale\n",
        "        if text and text not in original_text:\n",
        "            # Prova a trovare la corrispondenza più vicina nel testo originale\n",
        "            from difflib import get_close_matches\n",
        "            words = original_text.split()\n",
        "            matches = get_close_matches(text, words, n=1)\n",
        "            if matches:\n",
        "                text = matches[0]\n",
        "            else:\n",
        "                # Ancora non trovato, prova una corrispondenza parziale\n",
        "                potential_matches = []\n",
        "                for word in words:\n",
        "                    if len(word) >= 4 and (text in word or word in text):\n",
        "                        potential_matches.append(word)\n",
        "\n",
        "                if potential_matches:\n",
        "                    # Scegli la corrispondenza più lunga (probabilmente la più accurata)\n",
        "                    text = max(potential_matches, key=len)\n",
        "\n",
        "        # 8. Rimuovi testo residuo non significativo\n",
        "        # Se dopo tutte le correzioni, l'entità è troppo breve o è un articolo/preposizione\n",
        "        stop_words = [\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\", \"di\", \"a\", \"da\", \"in\", \"con\", \"su\", \"per\", \"tra\", \"fra\"]\n",
        "        if text.lower() in stop_words or len(text) < 3:\n",
        "            return None\n",
        "\n",
        "        # 9. Pulisci spazi e aggiorna l'entità\n",
        "        text = text.strip()\n",
        "        entity['text'] = text\n",
        "\n",
        "        # 10. Se il testo è diventato vuoto, scarta l'entità\n",
        "        if not entity['text']:\n",
        "            return None\n",
        "\n",
        "        return entity\n",
        "\n",
        "    def process_entities(self, entities, original_text):\n",
        "        \"\"\"\n",
        "        Applica post-processing a tutte le entità riconosciute.\n",
        "\n",
        "        Args:\n",
        "            entities: Lista di entità da correggere\n",
        "            original_text: Testo originale per verifica\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità corrette\n",
        "        \"\"\"\n",
        "        print(f\"DEBUG: Inizio post-processing di {len(entities)} entità\")\n",
        "        corrected_entities = []\n",
        "\n",
        "        for entity in entities:\n",
        "            # Applica post-processing all'entità\n",
        "            corrected = self._post_process_entity(entity, original_text)\n",
        "            if corrected:\n",
        "                # Verifica per evitare duplicati\n",
        "                if not any(e['text'] == corrected['text'] and e['label'] == corrected['label'] for e in corrected_entities):\n",
        "                    corrected_entities.append(corrected)\n",
        "\n",
        "        print(f\"DEBUG: Post-processing completato, {len(corrected_entities)} entità valide rimaste\")\n",
        "        return corrected_entities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer, pipeline  # Verifica disponibilità di transformers\n",
        "            import torch  # Verifica disponibilità di torch\n",
        "\n",
        "            print(\"DEBUG: Moduli transformers e torch disponibili. Inizializzazione riconoscitore Transformer NER...\")\n",
        "\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                #model_name=\"5had3/bert-base-italian-cased-ner\",\n",
        "                model_name=None,\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "            # Verifica se il riconoscitore è stato inizializzato correttamente\n",
        "            if not hasattr(self.transformers_recognizer, 'is_available') or not self.transformers_recognizer.is_available:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER non disponibile dopo l'inizializzazione\")\n",
        "            else:\n",
        "                print(\"DEBUG: Riconoscitore Transformer NER inizializzato con successo\")\n",
        "\n",
        "\n",
        "            print(\"DEBUG: Inizializzazione WordNet knowledge base...\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - Transformer NER non disponibile. {e}\")\n",
        "            print(\"Installare 'transformers' e 'torch' con 'pip install transformers torch'\")\n",
        "            self.transformers_recognizer = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione del riconoscitore Transformer NER: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    # Aggiungi il classificatore Zero-Shot in un blocco try/except separato\n",
        "        try:\n",
        "            print(\"DEBUG: Inizializzazione Zero-Shot Classifier...\")\n",
        "            self.zero_shot_classifier = ZeroShotClassifier(\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "\n",
        "            # Verifica se il classificatore è stato inizializzato correttamente\n",
        "            if not hasattr(self.zero_shot_classifier, 'is_available') or not self.zero_shot_classifier.is_available:\n",
        "                print(\"DEBUG: Zero-Shot Classifier non disponibile dopo l'inizializzazione\")\n",
        "            else:\n",
        "                print(\"DEBUG: Zero-Shot Classifier inizializzato con successo\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - Zero-Shot Classifier non disponibile. {e}\")\n",
        "            print(\"Installare 'transformers' e 'torch' con 'pip install transformers torch'\")\n",
        "            self.zero_shot_classifier = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione del Zero-Shot Classifier: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.zero_shot_classifier = None\n",
        "\n",
        "        # Dentro il __init__ della classe EntityLinkerItalian\n",
        "        try:\n",
        "            print(\"DEBUG: Inizializzazione WordNet knowledge base...\")\n",
        "\n",
        "            # Verifica che nltk e wordnet siano disponibili\n",
        "            import nltk\n",
        "            from nltk.corpus import wordnet as wn\n",
        "            print(\"DEBUG: NLTK e WordNet importati con successo\")\n",
        "\n",
        "            # Verifica che i dati di wordnet siano stati scaricati\n",
        "            try:\n",
        "                nltk.data.find('corpora/wordnet')\n",
        "                print(\"DEBUG: WordNet corpus trovato\")\n",
        "            except LookupError:\n",
        "                print(\"DEBUG: WordNet corpus non trovato, tentativo di download...\")\n",
        "                nltk.download('wordnet')\n",
        "\n",
        "            try:\n",
        "                nltk.data.find('corpora/omw-1.4')\n",
        "                print(\"DEBUG: Open Multilingual WordNet trovato\")\n",
        "            except LookupError:\n",
        "                print(\"DEBUG: Open Multilingual WordNet non trovato, tentativo di download...\")\n",
        "                nltk.download('omw-1.4')\n",
        "\n",
        "            # Test con una parola italiana\n",
        "            test_word = \"casa\"\n",
        "            synsets = wn.synsets(test_word, lang='ita')\n",
        "            print(f\"DEBUG: Test WordNet in italiano per 'casa': {len(synsets)} synsets trovati\")\n",
        "\n",
        "            self.wordnet_kb = WordNetKnowledgeBase(language=\"it\")\n",
        "\n",
        "            if hasattr(self.wordnet_kb, 'is_available'):\n",
        "                print(f\"DEBUG: WordNet KB disponibilità: {self.wordnet_kb.is_available}\")\n",
        "            else:\n",
        "                print(\"DEBUG: L'attributo is_available non esiste in WordNet KB\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: ImportError - WordNet non disponibile. {e}\")\n",
        "            print(\"Installare 'nltk' con 'pip install nltk'\")\n",
        "            self.wordnet_kb = None\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generico nell'inizializzazione di WordNet: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.wordnet_kb = None\n",
        "\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer:\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiorna il set di testi esistenti dopo aver aggiunto le entità italiane\n",
        "        for entity in italian_only_entities:\n",
        "            if 'text' in entity:\n",
        "                existing_texts.add(entity['text'].lower())\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        transformers_only_entities = []\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            try:\n",
        "                # Usa un metodo separato per ottenere le entità\n",
        "                transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "                print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "\n",
        "                # Aggiungi solo se ci sono entità\n",
        "                if transformers_only_entities:\n",
        "                    enriched_entities.extend(transformers_only_entities)\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nel trovare entità Transformer: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile per find_entities\")\n",
        "\n",
        "\n",
        "        # Dopo aver aggiunto le entità da Transformer NER\n",
        "        # Applica classificazione zero-shot se disponibile\n",
        "        if hasattr(self, 'zero_shot_classifier') and self.zero_shot_classifier and self.zero_shot_classifier.is_available:\n",
        "            print(\"DEBUG: Applicazione classificazione Zero-Shot alle entità\")\n",
        "            try:\n",
        "                # Classifica le entità\n",
        "                enriched_entities = self.zero_shot_classifier.classify_entities(enriched_entities, original_text=text)\n",
        "                print(f\"DEBUG: Classificazione Zero-Shot completata\")\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nella classificazione Zero-Shot: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "\n",
        "         # Arricchisci con WordNet se disponibile\n",
        "        if hasattr(self, 'wordnet_kb') and self.wordnet_kb and self.wordnet_kb.is_available:\n",
        "            print(\"DEBUG: Arricchimento entità con WordNet\")\n",
        "            try:\n",
        "                # Arricchisci le entità con informazioni semantiche\n",
        "                enriched_entities = self.wordnet_kb.enrich_entities(enriched_entities)\n",
        "                print(f\"DEBUG: Arricchimento WordNet completato\")\n",
        "            except Exception as e:\n",
        "                print(f\"DEBUG: Errore nell'arricchimento WordNet: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "          \"\"\"\n",
        "          Ottiene entità riconosciute dal riconoscitore Transformer NER.\n",
        "          Includendo sia entità nuove che quelle già trovate da altre fonti.\n",
        "          \"\"\"\n",
        "          # Verifica che il riconoscitore Transformer sia disponibile\n",
        "          if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "              print(\"DEBUG: Riconoscitore Transformer NER non disponibile.\")\n",
        "              return []\n",
        "\n",
        "          # Stampa informazioni sul riconoscitore\n",
        "          print(f\"DEBUG: Modello Transformer NER in uso: {self.transformers_recognizer.model_name}\")\n",
        "\n",
        "          try:\n",
        "              # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "              transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "\n",
        "              # Debug dettagliato delle entità\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER trovate: {len(transformers_entities)}\")\n",
        "\n",
        "              # Filtra e mappa le entità - IMPORTANTE: non filtriamo per existing_texts\n",
        "              # per assicurarci che tutte le entità Transformer siano incluse\n",
        "              transformers_mapped = []\n",
        "              for entity in transformers_entities:\n",
        "                  if self._is_valid_entity(entity.get('text', '')):\n",
        "                      # Aggiungi sempre il source 'transformers_ner'\n",
        "                      mapped_entity = {\n",
        "                          'text': entity.get('text', ''),\n",
        "                          'label': entity.get('label', ''),\n",
        "                          'types': [entity.get('label', 'N/A')],\n",
        "                          'source': 'transformers_ner',\n",
        "                          'score': entity.get('score', 0)\n",
        "                      }\n",
        "                      transformers_mapped.append(mapped_entity)\n",
        "\n",
        "              print(f\"DEBUG: Numero di entità Transformer NER mappate: {len(transformers_mapped)}\")\n",
        "              return transformers_mapped\n",
        "\n",
        "          except Exception as e:\n",
        "              print(f\"DEBUG: Errore nell'ottenere entità Transformer: {e}\")\n",
        "              import traceback\n",
        "              traceback.print_exc()\n",
        "              return []\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "class ZeroShotClassifier:\n",
        "    \"\"\"\n",
        "    Classificatore zero-shot per il riconoscimento di entità senza addestramento specifico.\n",
        "    Utilizza modelli transformer pre-addestrati per classificare entità in categorie arbitrarie.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"facebook/bart-large-mnli\", language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il classificatore zero-shot.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello per la classificazione zero-shot\n",
        "            language: Lingua del testo da analizzare\n",
        "            use_gpu: Se utilizzare GPU per l'accelerazione\n",
        "        \"\"\"\n",
        "        self.model_name = model_name\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.classifier = None\n",
        "\n",
        "        # Elenco modelli alternativi multilingua per zero-shot\n",
        "        self.alternative_models = [\n",
        "            \"facebook/bart-large-mnli\",              # Inglese\n",
        "            \"joeddav/xlm-roberta-large-xnli\",        # Multilingua, ottimo per italiano\n",
        "            \"MoritzLaurer/mDeBERTa-v3-base-xnli-multilingual-nli-2mil7\", # Ottimo modello multilingua\n",
        "            \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\", # Alta performance\n",
        "            \"cross-encoder/nli-deberta-v3-large\",    # Inglese, alta qualità\n",
        "        ]\n",
        "\n",
        "        # Categorie predefinite per le entità in italiano\n",
        "        self.default_labels = {\n",
        "            'it': {\n",
        "                'persona': \"Questa entità è una persona, un individuo o un personaggio?\",\n",
        "                'luogo': \"Questa entità è un luogo, una località o una posizione geografica?\",\n",
        "                'organizzazione': \"Questa entità è un'organizzazione, un'azienda, un gruppo o un'istituzione?\",\n",
        "                'evento': \"Questa entità è un evento, una manifestazione o un avvenimento storico?\",\n",
        "                'opera': \"Questa entità è un'opera d'arte, un libro, una canzone o un film?\",\n",
        "                'prodotto': \"Questa entità è un prodotto, una marca o un oggetto commerciale?\",\n",
        "                'concetto': \"Questa entità è un concetto astratto, un'idea o una teoria?\"\n",
        "            },\n",
        "            'en': {\n",
        "                'person': \"This entity is a person, individual, or character?\",\n",
        "                'location': \"This entity is a place, location, or geographical area?\",\n",
        "                'organization': \"This entity is an organization, company, group, or institution?\",\n",
        "                'event': \"This entity is an event, happening, or historical occurrence?\",\n",
        "                'work': \"This entity is a work of art, book, song, or movie?\",\n",
        "                'product': \"This entity is a product, brand, or commercial item?\",\n",
        "                'concept': \"This entity is an abstract concept, idea, or theory?\"\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello per la classificazione zero-shot.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
        "            import torch\n",
        "\n",
        "            print(f\"DEBUG: Tentativo di caricare il classificatore zero-shot: {self.model_name}\")\n",
        "\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "            # Prova a caricare il modello principale\n",
        "            try:\n",
        "                self.classifier = pipeline(\n",
        "                    \"zero-shot-classification\",\n",
        "                    model=self.model_name,\n",
        "                    device=device\n",
        "                )\n",
        "                self.is_available = True\n",
        "                print(f\"DEBUG: Classificatore zero-shot caricato con successo: {self.model_name}\")\n",
        "\n",
        "                # Test rapido\n",
        "                test_result = self.classifier(\n",
        "                    \"Roma è la capitale d'Italia\",\n",
        "                    candidate_labels=[\"città\", \"paese\", \"monumenti\"],\n",
        "                )\n",
        "                print(f\"DEBUG: Test zero-shot completato con successo\")\n",
        "                return\n",
        "\n",
        "            except Exception as model_error:\n",
        "                print(f\"DEBUG: Errore nel caricamento del modello principale: {str(model_error)}\")\n",
        "\n",
        "                # Prova modelli alternativi\n",
        "                for alt_model in self.alternative_models:\n",
        "                    if alt_model != self.model_name:  # Evita di provare lo stesso modello\n",
        "                        try:\n",
        "                            print(f\"DEBUG: Tentativo con modello alternativo: {alt_model}\")\n",
        "                            self.classifier = pipeline(\n",
        "                                \"zero-shot-classification\",\n",
        "                                model=alt_model,\n",
        "                                device=device\n",
        "                            )\n",
        "                            self.model_name = alt_model\n",
        "                            self.is_available = True\n",
        "                            print(f\"DEBUG: Modello alternativo caricato con successo: {alt_model}\")\n",
        "                            return\n",
        "                        except Exception as alt_error:\n",
        "                            print(f\"DEBUG: Errore con modello alternativo {alt_model}: {str(alt_error)}\")\n",
        "\n",
        "                raise model_error\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: Errore di importazione per zero-shot-classification: {str(e)}\")\n",
        "            print(\"Installare transformers e torch: pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore generale nel caricamento del classificatore zero-shot: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.is_available = False\n",
        "\n",
        "    def classify_entity(self, entity_text: str, context: str = None, custom_labels: List[str] = None):\n",
        "        \"\"\"\n",
        "        Classifica un'entità con il modello zero-shot.\n",
        "\n",
        "        Args:\n",
        "            entity_text: Il testo dell'entità da classificare\n",
        "            context: Contesto opzionale in cui appare l'entità\n",
        "            custom_labels: Etichette personalizzate da utilizzare (se None, usa le predefinite)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con etichette e punteggi di confidenza\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.classifier:\n",
        "            print(\"DEBUG: Classificatore zero-shot non disponibile\")\n",
        "            return {'label': 'unknown', 'scores': {}}\n",
        "\n",
        "        try:\n",
        "            # Usa le etichette predefinite se non sono specificate etichette personalizzate\n",
        "            if custom_labels is None:\n",
        "                # Scegli le etichette basate sulla lingua\n",
        "                lang_key = 'en' if self.language not in self.default_labels else self.language\n",
        "                labels = list(self.default_labels[lang_key].values())\n",
        "            else:\n",
        "                labels = custom_labels\n",
        "\n",
        "            # Prepara il testo per la classificazione\n",
        "            if context:\n",
        "                # Se c'è un contesto, aggiungiamo una frase che colloca l'entità nel contesto\n",
        "                text_to_classify = f\"Nel contesto di: '{context}', l'entità '{entity_text}' è di che tipo?\"\n",
        "            else:\n",
        "                # Altrimenti classifichiamo direttamente l'entità\n",
        "                text_to_classify = entity_text\n",
        "\n",
        "            # Esegui la classificazione\n",
        "            result = self.classifier(\n",
        "                text_to_classify,\n",
        "                candidate_labels=labels,\n",
        "                hypothesis_template=\"{}\",  # Template vuoto (già incluso nelle etichette)\n",
        "                multi_label=False\n",
        "            )\n",
        "\n",
        "            # Estrai l'etichetta migliore e il punteggio\n",
        "            best_label = result['labels'][0]\n",
        "            best_score = result['scores'][0]\n",
        "\n",
        "            # Se stiamo usando le etichette predefinite, mappa l'ipotesi all'etichetta reale\n",
        "            if custom_labels is None:\n",
        "                lang_key = 'en' if self.language not in self.default_labels else self.language\n",
        "                # Inverte il dizionario per ottenere l'etichetta dalla domanda\n",
        "                label_map = {v: k for k, v in self.default_labels[lang_key].items()}\n",
        "                best_label = label_map.get(best_label, best_label)\n",
        "\n",
        "            # Crea un dizionario di punteggi per ogni etichetta\n",
        "            scores = {}\n",
        "            for idx, label in enumerate(result['labels']):\n",
        "                if custom_labels is None:\n",
        "                    lang_key = 'en' if self.language not in self.default_labels else self.language\n",
        "                    label_map = {v: k for k, v in self.default_labels[lang_key].items()}\n",
        "                    mapped_label = label_map.get(label, label)\n",
        "                    scores[mapped_label] = result['scores'][idx]\n",
        "                else:\n",
        "                    scores[label] = result['scores'][idx]\n",
        "\n",
        "            return {\n",
        "                'label': best_label,\n",
        "                'score': best_score,\n",
        "                'scores': scores\n",
        "            }\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nella classificazione zero-shot: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return {'label': 'error', 'score': 0.0, 'scores': {}}\n",
        "\n",
        "    def classify_entities(self, entities, original_text=None):\n",
        "        \"\"\"\n",
        "        Classifica un elenco di entità.\n",
        "\n",
        "        Args:\n",
        "            entities: Lista di entità da classificare\n",
        "            original_text: Testo originale per contesto\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità arricchite con classificazione\n",
        "        \"\"\"\n",
        "        if not self.is_available:\n",
        "            return entities\n",
        "\n",
        "        print(f\"DEBUG: Classificazione zero-shot di {len(entities)} entità\")\n",
        "\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Salta entità che hanno già tipi definiti\n",
        "            if entity.get('types') and len(entity.get('types', [])) > 0:\n",
        "                enriched_entities.append(entity)\n",
        "                continue\n",
        "\n",
        "            # Estrai un contesto intorno all'entità se disponibile\n",
        "            context = None\n",
        "            if original_text and 'text' in entity:\n",
        "                entity_text = entity['text']\n",
        "                if entity_text in original_text:\n",
        "                    # Trova la posizione dell'entità nel testo\n",
        "                    start_pos = original_text.find(entity_text)\n",
        "                    # Estrai fino a 100 caratteri prima e dopo come contesto\n",
        "                    context_start = max(0, start_pos - 100)\n",
        "                    context_end = min(len(original_text), start_pos + len(entity_text) + 100)\n",
        "                    context = original_text[context_start:context_end]\n",
        "\n",
        "            # Classifica l'entità\n",
        "            classification = self.classify_entity(entity['text'], context)\n",
        "\n",
        "            # Aggiungi i risultati all'entità\n",
        "            if 'types' not in entity or not entity['types']:\n",
        "                entity['types'] = [classification['label']]\n",
        "            if 'score' not in entity:\n",
        "                entity['score'] = classification['score']\n",
        "\n",
        "            # Aggiungi attributo di classificazione zero-shot\n",
        "            entity['zero_shot_classification'] = {\n",
        "                'label': classification['label'],\n",
        "                'score': classification['score'],\n",
        "                'scores': classification['scores']\n",
        "            }\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        print(f\"DEBUG: Classificazione zero-shot completata\")\n",
        "        return enriched_entities\n",
        "\n",
        "\n",
        "class WordNetKnowledgeBase:\n",
        "    \"\"\"\n",
        "    Classe per l'arricchimento semantico di entità usando WordNet.\n",
        "    Fornisce definizioni, sinonimi, iponimi e iperonimi.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"ita\"):\n",
        "        \"\"\"\n",
        "        Inizializza la knowledge base con WordNet.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua per WordNet (ita per italiano, eng per inglese)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa lingue a codici WordNet\n",
        "        self.language_map = {\n",
        "            \"it\": \"ita\",\n",
        "            \"en\": \"eng\",\n",
        "            \"fr\": \"fra\",\n",
        "            \"de\": \"deu\",\n",
        "            \"es\": \"spa\"\n",
        "        }\n",
        "\n",
        "        # Inizializza WordNet\n",
        "        self._initialize_wordnet()\n",
        "\n",
        "    def _initialize_wordnet(self):\n",
        "        \"\"\"\n",
        "        Inizializza la risorsa WordNet e verifica disponibilità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from nltk.corpus import wordnet as wn\n",
        "            import nltk\n",
        "\n",
        "            # Assicurati che le risorse NLTK necessarie siano scaricate\n",
        "            try:\n",
        "                nltk.data.find('corpora/wordnet')\n",
        "            except LookupError:\n",
        "                print(\"DEBUG: Scaricamento di WordNet...\")\n",
        "                nltk.download('wordnet')\n",
        "\n",
        "            try:\n",
        "                nltk.data.find('corpora/omw-1.4')\n",
        "            except LookupError:\n",
        "                print(\"DEBUG: Scaricamento di Open Multilingual WordNet...\")\n",
        "                nltk.download('omw-1.4')\n",
        "\n",
        "            # Verifica che la lingua sia supportata\n",
        "            mapped_lang = self.language_map.get(self.language, self.language)\n",
        "\n",
        "            # Test rapido per verificare il funzionamento di WordNet\n",
        "            test_word = \"casa\" if mapped_lang == \"ita\" else \"house\"\n",
        "            synsets = wn.synsets(test_word, lang=mapped_lang)\n",
        "\n",
        "            if synsets:\n",
        "                self.is_available = True\n",
        "                print(f\"DEBUG: WordNet inizializzato con successo. Lingua: {mapped_lang}\")\n",
        "                print(f\"DEBUG: Esempio per '{test_word}': {len(synsets)} synsets trovati\")\n",
        "            else:\n",
        "                print(f\"DEBUG: WordNet non ha trovato synsets per '{test_word}' in {mapped_lang}\")\n",
        "                print(\"DEBUG: Verifica che la lingua sia supportata e installata correttamente\")\n",
        "                # Prova con l'inglese come fallback\n",
        "                synsets = wn.synsets('house', lang='eng')\n",
        "                if synsets:\n",
        "                    self.language = \"en\"\n",
        "                    print(\"DEBUG: Utilizzo dell'inglese come lingua di fallback\")\n",
        "                    self.is_available = True\n",
        "                else:\n",
        "                    raise Exception(\"WordNet non funziona neanche con la lingua inglese\")\n",
        "\n",
        "        except ImportError as e:\n",
        "            print(f\"DEBUG: Errore di importazione NLTK/WordNet: {str(e)}\")\n",
        "            print(\"Installare NLTK: pip install nltk\")\n",
        "            self.is_available = False\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nell'inizializzazione di WordNet: {str(e)}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            self.is_available = False\n",
        "\n",
        "    def get_synsets(self, word: str):\n",
        "        \"\"\"\n",
        "        Ottiene i synset di WordNet per una parola.\n",
        "\n",
        "        Args:\n",
        "            word: Parola da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di synset trovati\n",
        "        \"\"\"\n",
        "        if not self.is_available:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            from nltk.corpus import wordnet as wn\n",
        "\n",
        "            # Usa il codice lingua mappato\n",
        "            mapped_lang = self.language_map.get(self.language, self.language)\n",
        "\n",
        "            # Cerca i synset nella lingua specificata\n",
        "            synsets = wn.synsets(word, lang=mapped_lang)\n",
        "\n",
        "            # Se non trova nulla e la lingua non è inglese, prova anche in inglese\n",
        "            if not synsets and mapped_lang != 'eng':\n",
        "                eng_synsets = wn.synsets(word, lang='eng')\n",
        "                if eng_synsets:\n",
        "                    print(f\"DEBUG: Usando synset inglesi per '{word}' (non trovati in {mapped_lang})\")\n",
        "                    return eng_synsets\n",
        "\n",
        "            return synsets\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nel recupero dei synset per '{word}': {str(e)}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_semantic_info(self, entity_text: str):\n",
        "        \"\"\"\n",
        "        Estrae informazioni semantiche su un'entità da WordNet.\n",
        "\n",
        "        Args:\n",
        "            entity_text: Testo dell'entità\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con informazioni semantiche (definizioni, sinonimi, iperonimi, iponimi)\n",
        "        \"\"\"\n",
        "        if not self.is_available:\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            # Normalizza il testo dell'entità (minuscolo per la ricerca in WordNet)\n",
        "            normalized_text = entity_text.lower()\n",
        "\n",
        "            # Per entità multi-parola, prova diverse strategie\n",
        "            words = normalized_text.split()\n",
        "\n",
        "            # Lista di synset da considerare\n",
        "            all_synsets = []\n",
        "\n",
        "            # 1. Prova l'entità completa\n",
        "            synsets = self.get_synsets(normalized_text)\n",
        "            all_synsets.extend(synsets)\n",
        "\n",
        "            # 2. Per entità multi-parola, prova anche le singole parole\n",
        "            if len(words) > 1 and not synsets:\n",
        "                # Prova l'ultima parola (spesso la più significativa in molte lingue)\n",
        "                last_word_synsets = self.get_synsets(words[-1])\n",
        "                if last_word_synsets:\n",
        "                    all_synsets.extend(last_word_synsets)\n",
        "\n",
        "                # Prova anche la prima parola se non è un articolo/preposizione\n",
        "                articles_preps = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\",\n",
        "                                  \"the\", \"a\", \"an\", \"le\", \"la\", \"les\", \"der\", \"die\", \"das\"}\n",
        "                if words[0].lower() not in articles_preps:\n",
        "                    first_word_synsets = self.get_synsets(words[0])\n",
        "                    if first_word_synsets:\n",
        "                        all_synsets.extend(first_word_synsets)\n",
        "\n",
        "            # Se non abbiamo trovato synset, restituisci un dizionario vuoto\n",
        "            if not all_synsets:\n",
        "                return {}\n",
        "\n",
        "            # Prendi solo i primi N synset per evitare info irrilevanti\n",
        "            top_synsets = all_synsets[:3]\n",
        "\n",
        "            # Estrai informazioni semantiche dai synset\n",
        "            definitions = []\n",
        "            synonyms = set()\n",
        "            hypernyms = []\n",
        "            hyponyms = []\n",
        "\n",
        "            for synset in top_synsets:\n",
        "                # Definizione\n",
        "                definitions.append(synset.definition())\n",
        "\n",
        "                # Sinonimi (lemmi)\n",
        "                for lemma in synset.lemmas():\n",
        "                    synonyms.add(lemma.name().replace('_', ' '))\n",
        "\n",
        "                # Iperonimi (concetti più generali)\n",
        "                for hypernym in synset.hypernyms():\n",
        "                    hypernyms.append({\n",
        "                        'name': hypernym.name().split('.')[0].replace('_', ' '),\n",
        "                        'definition': hypernym.definition()\n",
        "                    })\n",
        "\n",
        "                # Iponimi (concetti più specifici)\n",
        "                for hyponym in synset.hyponyms():\n",
        "                    hyponyms.append({\n",
        "                        'name': hyponym.name().split('.')[0].replace('_', ' '),\n",
        "                        'definition': hyponym.definition()\n",
        "                    })\n",
        "\n",
        "            # Crea il dizionario di informazioni semantiche\n",
        "            semantic_info = {\n",
        "                'definitions': definitions,\n",
        "                'synonyms': list(synonyms),\n",
        "                'hypernyms': hypernyms[:5],  # Limita a 5 per evitare troppe info\n",
        "                'hyponyms': hyponyms[:5]     # Limita a 5 per evitare troppe info\n",
        "            }\n",
        "\n",
        "            return semantic_info\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"DEBUG: Errore nell'estrazione di informazioni semantiche per '{entity_text}': {str(e)}\")\n",
        "            return {}\n",
        "\n",
        "    def enrich_entity(self, entity):\n",
        "        \"\"\"\n",
        "        Arricchisce un'entità con informazioni semantiche da WordNet.\n",
        "\n",
        "        Args:\n",
        "            entity: Dizionario dell'entità da arricchire\n",
        "\n",
        "        Returns:\n",
        "            Entità arricchita con informazioni semantiche\n",
        "        \"\"\"\n",
        "        if not self.is_available or 'text' not in entity:\n",
        "            return entity\n",
        "\n",
        "        entity_text = entity['text']\n",
        "        semantic_info = self.get_entity_semantic_info(entity_text)\n",
        "\n",
        "        if semantic_info:\n",
        "            entity['semantic_info'] = semantic_info\n",
        "\n",
        "            # Se l'entità non ha tipi, prova a inferirli dagli iperonimi\n",
        "            if ('types' not in entity or not entity['types']) and semantic_info.get('hypernyms'):\n",
        "                inferred_types = []\n",
        "                for hypernym in semantic_info['hypernyms']:\n",
        "                    hypernym_name = hypernym['name']\n",
        "                    # Mappa alcuni iperonimi comuni a categorie di entità\n",
        "                    if hypernym_name in ['person', 'persona', 'human', 'umano']:\n",
        "                        inferred_types.append('Person')\n",
        "                    elif hypernym_name in ['location', 'place', 'luogo', 'località']:\n",
        "                        inferred_types.append('Location')\n",
        "                    elif hypernym_name in ['organization', 'organizzazione', 'institution', 'istituzione']:\n",
        "                        inferred_types.append('Organization')\n",
        "                    elif hypernym_name in ['event', 'evento']:\n",
        "                        inferred_types.append('Event')\n",
        "                    elif hypernym_name in ['artifact', 'artefatto', 'product', 'prodotto']:\n",
        "                        inferred_types.append('Product')\n",
        "\n",
        "                if inferred_types:\n",
        "                    entity['types'] = inferred_types\n",
        "                    entity['types_source'] = 'wordnet_hypernyms'\n",
        "\n",
        "        return entity\n",
        "\n",
        "    def enrich_entities(self, entities):\n",
        "        \"\"\"\n",
        "        Arricchisce una lista di entità con informazioni semantiche.\n",
        "\n",
        "        Args:\n",
        "            entities: Lista di entità da arricchire\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità arricchite\n",
        "        \"\"\"\n",
        "        if not self.is_available:\n",
        "            return entities\n",
        "\n",
        "        print(f\"DEBUG: Arricchimento semantico di {len(entities)} entità con WordNet\")\n",
        "\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            enriched_entity = self.enrich_entity(entity)\n",
        "            enriched_entities.append(enriched_entity)\n",
        "\n",
        "        print(f\"DEBUG: Arricchimento semantico completato\")\n",
        "        return enriched_entities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "# Aggiorna il codice principale per utilizzare le nuove funzionalità\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Importazioni necessarie\n",
        "    import os\n",
        "    import sys\n",
        "    import traceback\n",
        "\n",
        "    # Imposta il livello di log per transformers\n",
        "    os.environ[\"TRANSFORMERS_VERBOSITY\"] = \"info\"\n",
        "\n",
        "    # Verifica le dipendenze\n",
        "    try:\n",
        "        import torch\n",
        "        import transformers\n",
        "        print(f\"PyTorch versione: {torch.__version__}\")\n",
        "        print(f\"Transformers versione: {transformers.__version__}\")\n",
        "        print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
        "    except ImportError as e:\n",
        "        print(f\"ATTENZIONE: Libreria mancante - {e}\")\n",
        "\n",
        "    try:\n",
        "        # Crea un'istanza del linker di entità migliorato con debug aggiuntivo\n",
        "        print(\"Inizializzazione EntityLinkerItalian avanzato...\")\n",
        "        linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "        # Verifica disponibilità dei riconoscitori\n",
        "        print(\"\\nDisponibilità riconoscitori:\")\n",
        "        print(f\"- SpaCy: {linker.wikidata.spacy_recognizer.is_available}\")\n",
        "        print(f\"- Italian NLP: {linker.italian_recognizer.is_available}\")\n",
        "        if hasattr(linker, 'transformers_recognizer') and linker.transformers_recognizer:\n",
        "            print(f\"- Transformer NER: {linker.transformers_recognizer.is_available}\")\n",
        "            if linker.transformers_recognizer.is_available:\n",
        "                print(f\"  Modello: {linker.transformers_recognizer.model_name}\")\n",
        "        else:\n",
        "            print(\"- Transformer NER: non disponibile\")\n",
        "\n",
        "        if hasattr(linker, 'zero_shot_classifier') and linker.zero_shot_classifier:\n",
        "            print(f\"- Zero-Shot Classifier: {linker.zero_shot_classifier.is_available}\")\n",
        "            if linker.zero_shot_classifier.is_available:\n",
        "                print(f\"  Modello: {linker.zero_shot_classifier.model_name}\")\n",
        "        else:\n",
        "            print(\"- Zero-Shot Classifier: non disponibile\")\n",
        "\n",
        "        if hasattr(linker, 'wordnet_kb') and linker.wordnet_kb:\n",
        "            print(f\"- WordNet KB: {linker.wordnet_kb.is_available}\")\n",
        "            if linker.wordnet_kb.is_available:\n",
        "                print(f\"  Lingua: {linker.wordnet_kb.language}\")\n",
        "        else:\n",
        "            print(\"- WordNet KB: non disponibile\")\n",
        "\n",
        "        # Testo di esempio\n",
        "        text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "               \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "               \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "               \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "               \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "               \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "               \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "               \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "               \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "               \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "        print(f\"\\nAnalisi del testo di esempio...\")\n",
        "\n",
        "        # Trova entità\n",
        "        print(\"\\nRicerca entità nel testo principale...\")\n",
        "        entities, stats = linker.find_entities(text)\n",
        "\n",
        "        # Stampa statistiche\n",
        "        print(f\"\\nStatistiche riconoscimento entità:\")\n",
        "        for stat_name, stat_value in stats.items():\n",
        "            print(f\"- {stat_name}: {stat_value}\")\n",
        "\n",
        "        # Stampa esempi di entità per tipo di riconoscitore\n",
        "        print(\"\\nEsempi di entità per tipo di riconoscitore:\")\n",
        "\n",
        "        # Mostra entità Wikidata\n",
        "        wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "        if wikidata_entities:\n",
        "            print(f\"\\nEntità Wikidata ({len(wikidata_entities)}):\")\n",
        "            for entity in wikidata_entities[:3]:  # Mostra solo le prime 3\n",
        "                print(f\"- '{entity['text']}' → ID: {entity.get('wikidata_id')}\")\n",
        "                if 'types' in entity and entity['types']:\n",
        "                    print(f\"  Tipi: {', '.join(entity['types'][:3])}\")\n",
        "\n",
        "        # Mostra entità Transformer NER\n",
        "        transformer_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "        if transformer_entities:\n",
        "            print(f\"\\nEntità Transformer NER ({len(transformer_entities)}):\")\n",
        "            for entity in transformer_entities[:3]:  # Mostra solo le prime 3\n",
        "                print(f\"- '{entity['text']}' → Etichetta: {entity.get('label')}, Score: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        # Mostra entità con classificazione zero-shot\n",
        "        zero_shot_entities = [e for e in entities if 'zero_shot_classification' in e]\n",
        "        if zero_shot_entities:\n",
        "            print(f\"\\nEntità con classificazione Zero-Shot ({len(zero_shot_entities)}):\")\n",
        "            for entity in zero_shot_entities[:3]:  # Mostra solo le prime 3\n",
        "                classification = entity.get('zero_shot_classification', {})\n",
        "                print(f\"- '{entity['text']}' → Classe: {classification.get('label')}, Confidenza: {classification.get('score', 0):.4f}\")\n",
        "\n",
        "        # Mostra entità con info semantiche WordNet\n",
        "        wordnet_entities = [e for e in entities if 'semantic_info' in e]\n",
        "        if wordnet_entities:\n",
        "            print(f\"\\nEntità con informazioni semantiche WordNet ({len(wordnet_entities)}):\")\n",
        "            for entity in wordnet_entities[:3]:  # Mostra solo le prime 3\n",
        "                semantic_info = entity.get('semantic_info', {})\n",
        "                definitions = semantic_info.get('definitions', [])\n",
        "                print(f\"- '{entity['text']}'\")\n",
        "                if definitions:\n",
        "                    print(f\"  Definizione: {definitions[0][:100]}...\")\n",
        "                synonyms = semantic_info.get('synonyms', [])\n",
        "                if synonyms:\n",
        "                    print(f\"  Sinonimi: {', '.join(synonyms[:3])}\")\n",
        "\n",
        "        # Esporta le entità in un file\n",
        "        print(\"\\nEsportazione entità in file...\")\n",
        "        try:\n",
        "            output_file_path = export_entities_to_txt(\n",
        "                entities,\n",
        "                stats,\n",
        "                \"entita_complete.txt\",\n",
        "                spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "                italian_recognizer=linker.italian_recognizer,\n",
        "                transformers_recognizer=linker.transformers_recognizer if hasattr(linker, 'transformers_recognizer') else None,\n",
        "                zero_shot_classifier=linker.zero_shot_classifier if hasattr(linker, 'zero_shot_classifier') else None,\n",
        "                wordnet_kb=linker.wordnet_kb if hasattr(linker, 'wordnet_kb') else None,\n",
        "                original_text=text\n",
        "            )\n",
        "            print(f\"File salvato in: {output_file_path}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'esportazione del file: {e}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "        # Verifica il file di output\n",
        "        import os\n",
        "        if os.path.exists(output_file_path):\n",
        "            file_size = os.path.getsize(output_file_path)\n",
        "            print(f\"Verifica file: '{output_file_path}' esiste e ha dimensione {file_size} bytes\")\n",
        "            print(\"Analisi delle entità completata con successo!\")\n",
        "        else:\n",
        "            print(f\"ATTENZIONE: Il file '{output_file_path}' non è stato creato!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Errore generale: {e}\")\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "9486ac661de14f0a81c1806d360226b8",
            "22ee249c4985493584b7a85c6de51a48",
            "c4fce953344e4a3e9f3ad640776e6829",
            "2ab2fe63e9cc43e8a013455da89b7aa0",
            "7296b29bab7d42a29f31e7029bb22497",
            "589128f8e0094201839631c0d63f0110",
            "f3be6cf3449c4d9ea4f82db6b3ee812c",
            "1902ab79c4344391be2ff26b424f8e39",
            "d7dd442cb5cb4404b07cb6aa7c670159",
            "501b95298d7842ac81a3123dd11dcf60",
            "8ad945fb2fe34ddeb349fbf84ea8516f",
            "dcd1f0ae6fca4dc19f2a183bf603c5f0",
            "f1c661c310a84f098fcacc2916afa98f",
            "db80e0e26af445bbb7f78acfcd84bf2a",
            "7112bf463e7a4c64b4f813aac61832cd",
            "2d82a99774004158b96fa38c58b8d545",
            "e4abf65244d245c29d327f83c62d80e9",
            "2e0f80f299ba4eb5b605a749256a8a25",
            "04798674a0084ad18f899c74973131e7",
            "52564ad8451d453a883ad4d6ae4c799f",
            "dc484ea7329148068314321d3e6e470e",
            "45abbe53c4a4441dba3c1937c020dc5b"
          ]
        },
        "id": "0LhBWtznCz1y",
        "outputId": "6c132106-dacc-4827-a857-a4967b5223cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch versione: 2.6.0+cu124\n",
            "Transformers versione: 4.49.0\n",
            "CUDA disponibile: False\n",
            "Inizializzazione EntityLinkerItalian avanzato...\n",
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9486ac661de14f0a81c1806d360226b8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcd1f0ae6fca4dc19f2a183bf603c5f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "DEBUG: Moduli transformers e torch disponibili. Inizializzazione riconoscitore Transformer NER...\n",
            "DEBUG: Selezionando modello per lingua: it\n",
            "DEBUG: Proverò questi modelli nell'ordine: ['dbmdz/bert-base-italian-xxl-cased', 'Babelscape/wikineural-multilingual-ner', 'Davlan/xlm-roberta-base-ner-hrl', 'MilaNLProc/bert-italian-cased-ner', '5had3/bert-base-italian-cased-ner', 'dslim/bert-base-NER']\n",
            "DEBUG: Verifica disponibilità modello: dbmdz/bert-base-italian-xxl-cased\n",
            "DEBUG: Modello dbmdz/bert-base-italian-xxl-cased disponibile e selezionato\n",
            "DEBUG: Tentativo di caricare il modello: dbmdz/bert-base-italian-xxl-cased\n",
            "DEBUG: Usando device CPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at dbmdz/bert-base-italian-xxl-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Modello dbmdz/bert-base-italian-xxl-cased caricato con successo!\n",
            "DEBUG: Test modello superato - entità trovate: 6\n",
            "DEBUG: Riconoscitore Transformer NER inizializzato con successo\n",
            "DEBUG: Inizializzazione WordNet knowledge base...\n",
            "DEBUG: Inizializzazione Zero-Shot Classifier...\n",
            "DEBUG: Tentativo di caricare il classificatore zero-shot: facebook/bart-large-mnli\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DEBUG: Classificatore zero-shot caricato con successo: facebook/bart-large-mnli\n",
            "DEBUG: Test zero-shot completato con successo\n",
            "DEBUG: Zero-Shot Classifier inizializzato con successo\n",
            "DEBUG: Inizializzazione WordNet knowledge base...\n",
            "DEBUG: NLTK e WordNet importati con successo\n",
            "DEBUG: WordNet corpus non trovato, tentativo di download...\n",
            "DEBUG: Open Multilingual WordNet non trovato, tentativo di download...\n",
            "DEBUG: Test WordNet in italiano per 'casa': 8 synsets trovati\n",
            "DEBUG: Scaricamento di WordNet...\n",
            "DEBUG: Scaricamento di Open Multilingual WordNet...\n",
            "DEBUG: WordNet inizializzato con successo. Lingua: ita\n",
            "DEBUG: Esempio per 'casa': 8 synsets trovati\n",
            "DEBUG: WordNet KB disponibilità: True\n",
            "\n",
            "Disponibilità riconoscitori:\n",
            "- SpaCy: True\n",
            "- Italian NLP: True\n",
            "- Transformer NER: True\n",
            "  Modello: dbmdz/bert-base-italian-xxl-cased\n",
            "- Zero-Shot Classifier: True\n",
            "  Modello: facebook/bart-large-mnli\n",
            "- WordNet KB: True\n",
            "  Lingua: it\n",
            "\n",
            "Analisi del testo di esempio...\n",
            "\n",
            "Ricerca entità nel testo principale...\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Modello Transformer NER in uso: dbmdz/bert-base-italian-xxl-cased\n",
            "DEBUG: Analisi testo con modello dbmdz/bert-base-italian-xxl-cased\n",
            "DEBUG: Testo diviso in 10 frasi\n",
            "DEBUG: Analisi frase 1/10\n",
            "DEBUG: Mismatch - tokenizer ha '##ti' ma il testo ha 'ti'\n",
            "DEBUG: Mismatch - tokenizer ha '##nar' ma il testo ha 'nar'\n",
            "DEBUG: Mismatch - tokenizer ha '##ia' ma il testo ha 'ia'\n",
            "DEBUG: Analisi frase 2/10\n",
            "DEBUG: Mismatch - tokenizer ha '##sse' ma il testo ha 'sse'\n",
            "DEBUG: Mismatch - tokenizer ha '##o' ma il testo ha 'o'\n",
            "DEBUG: Analisi frase 3/10\n",
            "DEBUG: Mismatch - tokenizer ha '##u' ma il testo ha 'u'\n",
            "DEBUG: Mismatch - tokenizer ha '##vio' ma il testo ha 'vio'\n",
            "DEBUG: Analisi frase 4/10\n",
            "DEBUG: Mismatch - tokenizer ha '##riera' ma il testo ha 'riera'\n",
            "DEBUG: Mismatch - tokenizer ha '##port' ma il testo ha 'port'\n",
            "DEBUG: Analisi frase 5/10\n",
            "DEBUG: Mismatch - tokenizer ha '##enza' ma il testo ha 'enza'\n",
            "DEBUG: Mismatch - tokenizer ha '##tecnico' ma il testo ha 'tecnico'\n",
            "DEBUG: Analisi frase 6/10\n",
            "DEBUG: Mismatch - tokenizer ha '##gastron' ma il testo ha 'gastron'\n",
            "DEBUG: Mismatch - tokenizer ha '##omica' ma il testo ha 'omica'\n",
            "DEBUG: Mismatch - tokenizer ha '##ti' ma il testo ha 'ti'\n",
            "DEBUG: Mismatch - tokenizer ha '##olo' ma il testo ha 'olo'\n",
            "DEBUG: Analisi frase 7/10\n",
            "DEBUG: Mismatch - tokenizer ha '##org' ma il testo ha 'org'\n",
            "DEBUG: Mismatch - tokenizer ha '##hin' ma il testo ha 'hin'\n",
            "DEBUG: Mismatch - tokenizer ha '##i' ma il testo ha 'i'\n",
            "DEBUG: Mismatch - tokenizer ha '##ti' ma il testo ha 'ti'\n",
            "DEBUG: Analisi frase 8/10\n",
            "DEBUG: Analisi frase 9/10\n",
            "DEBUG: Mismatch - tokenizer ha '##ghie' ma il testo ha 'ghie'\n",
            "DEBUG: Mismatch - tokenizer ha '##ri' ma il testo ha 'ri'\n",
            "DEBUG: Mismatch - tokenizer ha '##lei' ma il testo ha 'lei'\n",
            "DEBUG: Analisi frase 10/10\n",
            "DEBUG: Mismatch - tokenizer ha '##zzazione' ma il testo ha 'zzazione'\n",
            "DEBUG: Totale token di entità trovati: 217\n",
            "DEBUG: Raggruppate in 50 entità\n",
            "DEBUG: Numero di entità Transformer NER trovate: 50\n",
            "DEBUG: Numero di entità Transformer NER mappate: 45\n",
            "DEBUG: Entità Transformer NER trovate: 45\n",
            "DEBUG: Applicazione classificazione Zero-Shot alle entità\n",
            "DEBUG: Classificazione zero-shot di 83 entità\n",
            "DEBUG: Classificazione zero-shot completata\n",
            "DEBUG: Classificazione Zero-Shot completata\n",
            "DEBUG: Arricchimento entità con WordNet\n",
            "DEBUG: Arricchimento semantico di 83 entità con WordNet\n",
            "DEBUG: Usando synset inglesi per 'chianti' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'dante' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'galileo' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'galileo' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'chianti' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'dante' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'al' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'con' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'un' (non trovati in ita)\n",
            "DEBUG: Usando synset inglesi per 'dell' (non trovati in ita)\n",
            "DEBUG: Arricchimento semantico completato\n",
            "DEBUG: Arricchimento WordNet completato\n",
            "\n",
            "Statistiche riconoscimento entità:\n",
            "- total_entities: 83\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "- transformers_only_entities: 45\n",
            "\n",
            "Esempi di entità per tipo di riconoscitore:\n",
            "\n",
            "Entità Wikidata (29):\n",
            "- 'Italia' → ID: Q38\n",
            "  Tipi: paese, stato sovrano, stato sociale\n",
            "- 'Roma' → ID: Q220\n",
            "  Tipi: città di confine, comune italiano soppresso, destinazione turistica\n",
            "- 'Colosseo' → ID: Q10285\n",
            "  Tipi: anfiteatro romano, sito archeologico, attrazione turistica\n",
            "\n",
            "Entità Transformer NER (45):\n",
            "- 'L'Italia è uno dei paesi più' → Etichetta: LABEL_1, Score: 0.6695488095283508\n",
            "- 'visita' → Etichetta: LABEL_0, Score: 0.5076773166656494\n",
            "- 'al mondo' → Etichetta: LABEL_0, Score: 0.6508998274803162\n",
            "\n",
            "Entità con classificazione Zero-Shot (1):\n",
            "- 'Politecnico' → Classe: concetto, Confidenza: 0.3112\n",
            "\n",
            "Entità con informazioni semantiche WordNet (56):\n",
            "- 'Italia'\n",
            "  Definizione: a republic in southern Europe on the Italian Peninsula; was the core of the Roman Republic and the R...\n",
            "  Sinonimi: Italy, Italia, Italian Republic\n",
            "- 'Roma'\n",
            "  Definizione: capital and largest city of Italy; on the Tiber; seat of the Roman Catholic Church; formerly the cap...\n",
            "  Sinonimi: Roma, Italian capital, capital of Italy\n",
            "- 'Firenze'\n",
            "  Definizione: a city in central Italy on the Arno; provincial capital of Tuscany; center of the Italian Renaissanc...\n",
            "  Sinonimi: Firenze, Florence\n",
            "\n",
            "Esportazione entità in file...\n",
            "DEBUG: Tentativo di ottenere entità dal riconoscitore Transformer\n",
            "DEBUG: Analisi testo con modello dbmdz/bert-base-italian-xxl-cased\n",
            "DEBUG: Testo diviso in 10 frasi\n",
            "DEBUG: Analisi frase 1/10\n",
            "DEBUG: Mismatch - tokenizer ha '##ti' ma il testo ha 'ti'\n",
            "DEBUG: Mismatch - tokenizer ha '##nar' ma il testo ha 'nar'\n",
            "DEBUG: Mismatch - tokenizer ha '##ia' ma il testo ha 'ia'\n",
            "DEBUG: Analisi frase 2/10\n",
            "DEBUG: Mismatch - tokenizer ha '##sse' ma il testo ha 'sse'\n",
            "DEBUG: Mismatch - tokenizer ha '##o' ma il testo ha 'o'\n",
            "DEBUG: Analisi frase 3/10\n",
            "DEBUG: Mismatch - tokenizer ha '##u' ma il testo ha 'u'\n",
            "DEBUG: Mismatch - tokenizer ha '##vio' ma il testo ha 'vio'\n",
            "DEBUG: Analisi frase 4/10\n",
            "DEBUG: Mismatch - tokenizer ha '##riera' ma il testo ha 'riera'\n",
            "DEBUG: Mismatch - tokenizer ha '##port' ma il testo ha 'port'\n",
            "DEBUG: Analisi frase 5/10\n",
            "DEBUG: Mismatch - tokenizer ha '##enza' ma il testo ha 'enza'\n",
            "DEBUG: Mismatch - tokenizer ha '##tecnico' ma il testo ha 'tecnico'\n",
            "DEBUG: Analisi frase 6/10\n",
            "DEBUG: Mismatch - tokenizer ha '##gastron' ma il testo ha 'gastron'\n",
            "DEBUG: Mismatch - tokenizer ha '##omica' ma il testo ha 'omica'\n",
            "DEBUG: Mismatch - tokenizer ha '##ti' ma il testo ha 'ti'\n",
            "DEBUG: Mismatch - tokenizer ha '##olo' ma il testo ha 'olo'\n",
            "DEBUG: Analisi frase 7/10\n",
            "DEBUG: Mismatch - tokenizer ha '##org' ma il testo ha 'org'\n",
            "DEBUG: Mismatch - tokenizer ha '##hin' ma il testo ha 'hin'\n",
            "DEBUG: Mismatch - tokenizer ha '##i' ma il testo ha 'i'\n",
            "DEBUG: Mismatch - tokenizer ha '##ti' ma il testo ha 'ti'\n",
            "DEBUG: Analisi frase 8/10\n",
            "DEBUG: Analisi frase 9/10\n",
            "DEBUG: Mismatch - tokenizer ha '##ghie' ma il testo ha 'ghie'\n",
            "DEBUG: Mismatch - tokenizer ha '##ri' ma il testo ha 'ri'\n",
            "DEBUG: Mismatch - tokenizer ha '##lei' ma il testo ha 'lei'\n",
            "DEBUG: Analisi frase 10/10\n",
            "DEBUG: Mismatch - tokenizer ha '##zzazione' ma il testo ha 'zzazione'\n",
            "DEBUG: Totale token di entità trovati: 217\n",
            "DEBUG: Raggruppate in 50 entità\n",
            "DEBUG: Trovate 50 entità da Transformer\n",
            "File salvato in: /content/entita_complete.txt\n",
            "File salvato in: /content/entita_complete.txt\n",
            "Verifica file: '/content/entita_complete.txt' esiste e ha dimensione 28043 bytes\n",
            "Analisi delle entità completata con successo!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99b2c7037a41432b9c4ca5082c3d7a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3086d26b5924bbdae57674d26283897",
              "IPY_MODEL_4c900e21b11b411ab7c455f04e1b85dc",
              "IPY_MODEL_7a5cdc20c07e4816a3a0fd19a61e804b"
            ],
            "layout": "IPY_MODEL_56047f68127946569aada5452404e4e1"
          }
        },
        "d3086d26b5924bbdae57674d26283897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2d6713ad9e34197b91a68f69132c349",
            "placeholder": "​",
            "style": "IPY_MODEL_79392c1fd4734882828b6c5da77dd33f",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "4c900e21b11b411ab7c455f04e1b85dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15364e7e9ef242e9b8951c76da22f619",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3571070609e241efb626541028b1e09c",
            "value": 52557
          }
        },
        "7a5cdc20c07e4816a3a0fd19a61e804b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef78ffb6569f44be8b40aed193d3ac2b",
            "placeholder": "​",
            "style": "IPY_MODEL_46d4463a40ab48749959d0a2f8f0c044",
            "value": " 424k/? [00:00&lt;00:00, 26.6MB/s]"
          }
        },
        "56047f68127946569aada5452404e4e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2d6713ad9e34197b91a68f69132c349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79392c1fd4734882828b6c5da77dd33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15364e7e9ef242e9b8951c76da22f619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3571070609e241efb626541028b1e09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef78ffb6569f44be8b40aed193d3ac2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46d4463a40ab48749959d0a2f8f0c044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "befca2b67b274a72a2f5213ae964fb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb8d2a11d3a84dd79a4f3f308aa465d4",
              "IPY_MODEL_857f3bcab111422f9e30442ba135460d",
              "IPY_MODEL_acdc3319110d496ab68ac4b0b9e0d638"
            ],
            "layout": "IPY_MODEL_727d64886002405a923d4407af0c4209"
          }
        },
        "fb8d2a11d3a84dd79a4f3f308aa465d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43fef5bc8a9b4d1abda35a54026e44bb",
            "placeholder": "​",
            "style": "IPY_MODEL_06315793acb0496b9fe3946783d10b04",
            "value": "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/default.zip: 100%"
          }
        },
        "857f3bcab111422f9e30442ba135460d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6f61c84b8604421a831513077391927",
            "max": 455516818,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32b257cc71514839b256ca7e3830f83f",
            "value": 455516818
          }
        },
        "acdc3319110d496ab68ac4b0b9e0d638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3607370f349249d6b8acfd001ff8b198",
            "placeholder": "​",
            "style": "IPY_MODEL_909766e612dd453cbfa79cdf15fbb2be",
            "value": " 456M/456M [00:02&lt;00:00, 183MB/s]"
          }
        },
        "727d64886002405a923d4407af0c4209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43fef5bc8a9b4d1abda35a54026e44bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06315793acb0496b9fe3946783d10b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6f61c84b8604421a831513077391927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b257cc71514839b256ca7e3830f83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3607370f349249d6b8acfd001ff8b198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "909766e612dd453cbfa79cdf15fbb2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc2fc4c8b2ab410eb95dfa58bd086335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37eb79cbd8504854b848074be4a59921",
              "IPY_MODEL_f84fab5f290045bbbada5b979cdc3cd5",
              "IPY_MODEL_2016dd4b64694c78a4eb7bc816aed6ad"
            ],
            "layout": "IPY_MODEL_6a293547cc7b47fcb6c0dab36c12c7c7"
          }
        },
        "37eb79cbd8504854b848074be4a59921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8fe12d941444d04b202ff08e772526f",
            "placeholder": "​",
            "style": "IPY_MODEL_ef3b30f1c661461d9c63dc9561bcb9ef",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "f84fab5f290045bbbada5b979cdc3cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb82a3d5031942ce84a5c714270036b5",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3793086f93704ad292403ead367c6073",
            "value": 52557
          }
        },
        "2016dd4b64694c78a4eb7bc816aed6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80ef0d369f04de199b2352c26b61111",
            "placeholder": "​",
            "style": "IPY_MODEL_987be07332014d6a8c1a27c07d5e7876",
            "value": " 424k/? [00:00&lt;00:00, 19.7MB/s]"
          }
        },
        "6a293547cc7b47fcb6c0dab36c12c7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fe12d941444d04b202ff08e772526f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef3b30f1c661461d9c63dc9561bcb9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb82a3d5031942ce84a5c714270036b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3793086f93704ad292403ead367c6073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e80ef0d369f04de199b2352c26b61111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987be07332014d6a8c1a27c07d5e7876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56fa5b786c9547beae467638a6887b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f253de36031a487688b8c3394b8fecb3",
              "IPY_MODEL_950a15c315e54f4a8e7d9f26ee9c72f7",
              "IPY_MODEL_914580895a3a4d3c8bdc2216ed9737a3"
            ],
            "layout": "IPY_MODEL_34aaf234ebba4e4e84d3e49c0f14789d"
          }
        },
        "f253de36031a487688b8c3394b8fecb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb7edb43a3d4c22ae587cfc90eab650",
            "placeholder": "​",
            "style": "IPY_MODEL_9e36db42b58b4b8fa1faf1a8310b83a1",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "950a15c315e54f4a8e7d9f26ee9c72f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea16871ee4a94f68aeab9203eea49451",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6875c961e6b142f7842ba4c5869e2d25",
            "value": 52557
          }
        },
        "914580895a3a4d3c8bdc2216ed9737a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecee372f9da74648b0d002e985f28a72",
            "placeholder": "​",
            "style": "IPY_MODEL_289ef29f74814dc1aac2908cd6759870",
            "value": " 424k/? [00:00&lt;00:00, 24.7MB/s]"
          }
        },
        "34aaf234ebba4e4e84d3e49c0f14789d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb7edb43a3d4c22ae587cfc90eab650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e36db42b58b4b8fa1faf1a8310b83a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea16871ee4a94f68aeab9203eea49451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6875c961e6b142f7842ba4c5869e2d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecee372f9da74648b0d002e985f28a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "289ef29f74814dc1aac2908cd6759870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e25e37f7743148909e366968eed138f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_421fd2ada2214240810db5d0e431e451",
              "IPY_MODEL_6233535aaeab483b8f7023b51457d184",
              "IPY_MODEL_4b5b13c7cbab40429f32ddaed90de202"
            ],
            "layout": "IPY_MODEL_54b605a2890f4be1b855797f7efdb598"
          }
        },
        "421fd2ada2214240810db5d0e431e451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9dddea903d430184b57088cae426b2",
            "placeholder": "​",
            "style": "IPY_MODEL_998c3e99bb254ff4bb5087c66aee0b95",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "6233535aaeab483b8f7023b51457d184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68042e42b1e44e65a8627da0aec3f560",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9a99e66755c4c11a74e253389dd7c9d",
            "value": 52557
          }
        },
        "4b5b13c7cbab40429f32ddaed90de202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4824d4d85ebb4b13a244df954788e965",
            "placeholder": "​",
            "style": "IPY_MODEL_9560df99c2094f7cb63dd5975c69124f",
            "value": " 424k/? [00:00&lt;00:00, 27.2MB/s]"
          }
        },
        "54b605a2890f4be1b855797f7efdb598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9dddea903d430184b57088cae426b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "998c3e99bb254ff4bb5087c66aee0b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68042e42b1e44e65a8627da0aec3f560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9a99e66755c4c11a74e253389dd7c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4824d4d85ebb4b13a244df954788e965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9560df99c2094f7cb63dd5975c69124f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "125a82f05f1d446a9acbc905b5920f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_376f7c94992a4c0d8c30132ff6b60582",
              "IPY_MODEL_bfdd0cd7d31a493bbba82ee1c915c322",
              "IPY_MODEL_cad65ef1c7e04c878a61fdd3738f72f9"
            ],
            "layout": "IPY_MODEL_03858db06c134d53bb64d57ebf41061f"
          }
        },
        "376f7c94992a4c0d8c30132ff6b60582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a7275240914264a137e6fd494494af",
            "placeholder": "​",
            "style": "IPY_MODEL_6f5f418350f24b1c93a8d40940e9631a",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "bfdd0cd7d31a493bbba82ee1c915c322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45dd27ca7da64abba701bac781ecb2b2",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7292ec3cc67e478c87a78d50cd20210d",
            "value": 52557
          }
        },
        "cad65ef1c7e04c878a61fdd3738f72f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e792397b884b94a60ae279896e4744",
            "placeholder": "​",
            "style": "IPY_MODEL_b4665fa2a28041558db4578cd0ccacc6",
            "value": " 424k/? [00:00&lt;00:00, 18.4MB/s]"
          }
        },
        "03858db06c134d53bb64d57ebf41061f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a7275240914264a137e6fd494494af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5f418350f24b1c93a8d40940e9631a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45dd27ca7da64abba701bac781ecb2b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7292ec3cc67e478c87a78d50cd20210d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0e792397b884b94a60ae279896e4744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4665fa2a28041558db4578cd0ccacc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7628c1a518e448cb3b6831288bc118a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8350b5345a924735ba99c6ad08573790",
              "IPY_MODEL_f4aee7509d4843438d5b1dc129c87ba1",
              "IPY_MODEL_80cae6ab93784136bf089d82493e5dc4"
            ],
            "layout": "IPY_MODEL_4eaeabcf6c174d728bb9ecb7cebfdc7e"
          }
        },
        "8350b5345a924735ba99c6ad08573790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c713c7a97b4f49cf9f1ff5fd57a38c1a",
            "placeholder": "​",
            "style": "IPY_MODEL_7bf31623893a4b5ba6858456abc1b43c",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "f4aee7509d4843438d5b1dc129c87ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895f8bbee858458b8911b61ae1a44f86",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38ab564917a24a22967b1ff42e6858a6",
            "value": 52557
          }
        },
        "80cae6ab93784136bf089d82493e5dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8968f10d49b0492da351dfecff981720",
            "placeholder": "​",
            "style": "IPY_MODEL_f90521b4fed84656becb8e3770be405c",
            "value": " 424k/? [00:00&lt;00:00, 25.3MB/s]"
          }
        },
        "4eaeabcf6c174d728bb9ecb7cebfdc7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c713c7a97b4f49cf9f1ff5fd57a38c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf31623893a4b5ba6858456abc1b43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "895f8bbee858458b8911b61ae1a44f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ab564917a24a22967b1ff42e6858a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8968f10d49b0492da351dfecff981720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f90521b4fed84656becb8e3770be405c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c27647f7e74193a37d241be940571b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_462f06ca806e4365a30e2fcb079cf026",
              "IPY_MODEL_ff5adce707d34e948e8aec223b80c23b",
              "IPY_MODEL_7cb541b4f6024f2e966a8e37a5cd093c"
            ],
            "layout": "IPY_MODEL_538ccc648a2f40228e1e588a6249fd87"
          }
        },
        "462f06ca806e4365a30e2fcb079cf026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6e6b85d44d940848c9d95cc24f814dd",
            "placeholder": "​",
            "style": "IPY_MODEL_660b6030ddfa4acd806517b4a8ba2da9",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "ff5adce707d34e948e8aec223b80c23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3c37a4607044b4998041e6a55cbba1c",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac5636907393468d90ae5ca40c9b67cb",
            "value": 52557
          }
        },
        "7cb541b4f6024f2e966a8e37a5cd093c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f925dc41b3641a5ad94244732dad5cc",
            "placeholder": "​",
            "style": "IPY_MODEL_43a347010fc745c7a13638d4a53fbf95",
            "value": " 424k/? [00:00&lt;00:00, 20.6MB/s]"
          }
        },
        "538ccc648a2f40228e1e588a6249fd87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e6b85d44d940848c9d95cc24f814dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660b6030ddfa4acd806517b4a8ba2da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3c37a4607044b4998041e6a55cbba1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5636907393468d90ae5ca40c9b67cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f925dc41b3641a5ad94244732dad5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a347010fc745c7a13638d4a53fbf95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dd45e1efb824a9fa64eed0a2cb28683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_734d841bcfe944bfa28d479df00b624b",
              "IPY_MODEL_3269d9ef5be0400da3d8f232c799c7b6",
              "IPY_MODEL_3c5ba092d043479d843c97314af67e4c"
            ],
            "layout": "IPY_MODEL_effb0092e34941188768b5a8765ec472"
          }
        },
        "734d841bcfe944bfa28d479df00b624b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584be2e248a94313b814df876b1e32e1",
            "placeholder": "​",
            "style": "IPY_MODEL_c3a15d887b38402c8a84769b0d9663cc",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "3269d9ef5be0400da3d8f232c799c7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3508ede777ae4e11a83ff6e80415a3fd",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ad5e73b7cef402e96a15f5b36bb4c51",
            "value": 52557
          }
        },
        "3c5ba092d043479d843c97314af67e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bb827146b8649719ca3a0961e997ced",
            "placeholder": "​",
            "style": "IPY_MODEL_d026a7d56aac4d549e8c900b8e789975",
            "value": " 424k/? [00:00&lt;00:00, 17.1MB/s]"
          }
        },
        "effb0092e34941188768b5a8765ec472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "584be2e248a94313b814df876b1e32e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a15d887b38402c8a84769b0d9663cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3508ede777ae4e11a83ff6e80415a3fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad5e73b7cef402e96a15f5b36bb4c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bb827146b8649719ca3a0961e997ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d026a7d56aac4d549e8c900b8e789975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c90dde9098534fea853d095ccb84eb19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c1dc55bdcdf4e41b5d013902b8e9d39",
              "IPY_MODEL_403c1e63a4ea437f99d9ec89f11cc0a3",
              "IPY_MODEL_d96bf999477e4ef9b8bc234b98145c8a"
            ],
            "layout": "IPY_MODEL_a5433829f47442fd94c5684af30193aa"
          }
        },
        "0c1dc55bdcdf4e41b5d013902b8e9d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eae40eb33c5a470c88b258d55b1c813f",
            "placeholder": "​",
            "style": "IPY_MODEL_5a332f3c39294dd1a6e630e98801cc05",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "403c1e63a4ea437f99d9ec89f11cc0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ab92969e164615b1ee83b07e712ab6",
            "max": 59,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_698382eaa87749a6bdebfdc5b1b4750b",
            "value": 59
          }
        },
        "d96bf999477e4ef9b8bc234b98145c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcbbba22dd4f488fa17c3833095487ef",
            "placeholder": "​",
            "style": "IPY_MODEL_183fd4438f53425fb48e09b774b659f6",
            "value": " 59.0/59.0 [00:00&lt;00:00, 3.90kB/s]"
          }
        },
        "a5433829f47442fd94c5684af30193aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eae40eb33c5a470c88b258d55b1c813f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a332f3c39294dd1a6e630e98801cc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6ab92969e164615b1ee83b07e712ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698382eaa87749a6bdebfdc5b1b4750b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcbbba22dd4f488fa17c3833095487ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "183fd4438f53425fb48e09b774b659f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f3a66cfa9284ebaaa9b725a3c441481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76f6bff5db694fc199a9cac79e9ee059",
              "IPY_MODEL_0adcacf5046248e2a2755ccfae8a81dd",
              "IPY_MODEL_329e85b08c974eab8489919c4f19f779"
            ],
            "layout": "IPY_MODEL_a3e64a9a84374c6aa882c3d392787de4"
          }
        },
        "76f6bff5db694fc199a9cac79e9ee059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece0841d4413440882c3f9ade6c96aa5",
            "placeholder": "​",
            "style": "IPY_MODEL_a411a57a8c664ab292ba61db4b1c8b97",
            "value": "config.json: 100%"
          }
        },
        "0adcacf5046248e2a2755ccfae8a81dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f1f94c3f06249a5a5ddba8612a4540f",
            "max": 829,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e62bc58dc5314d8f8ce3d281414f5ef6",
            "value": 829
          }
        },
        "329e85b08c974eab8489919c4f19f779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29961dda9b2340c2910dae9a7362ef95",
            "placeholder": "​",
            "style": "IPY_MODEL_45f7c233516c44ed92fdf1b827824b5f",
            "value": " 829/829 [00:00&lt;00:00, 78.8kB/s]"
          }
        },
        "a3e64a9a84374c6aa882c3d392787de4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece0841d4413440882c3f9ade6c96aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a411a57a8c664ab292ba61db4b1c8b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f1f94c3f06249a5a5ddba8612a4540f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e62bc58dc5314d8f8ce3d281414f5ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29961dda9b2340c2910dae9a7362ef95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f7c233516c44ed92fdf1b827824b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52edbf406d994662a0f9b2d502c50e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25fb7569935d45c492b1e40f42beb924",
              "IPY_MODEL_4499685d5da4456dba01c7ba715fb2fd",
              "IPY_MODEL_9a2ae8819e1847ad86ca1e2b6854fa63"
            ],
            "layout": "IPY_MODEL_ac7c72e2e80547dc983b8b525959001f"
          }
        },
        "25fb7569935d45c492b1e40f42beb924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff9cdc3ea5324b0fb6ea24ac464918af",
            "placeholder": "​",
            "style": "IPY_MODEL_357d96cf6a8b49d6835b3c3561405289",
            "value": "vocab.txt: 100%"
          }
        },
        "4499685d5da4456dba01c7ba715fb2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a48d2f8041b4548bd144c53d5388615",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e50f2ccb731f49079f1e5686f044b514",
            "value": 213450
          }
        },
        "9a2ae8819e1847ad86ca1e2b6854fa63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2355602a97024827b88855f72f8b0999",
            "placeholder": "​",
            "style": "IPY_MODEL_f9073c358088495c8421f0052e52445f",
            "value": " 213k/213k [00:00&lt;00:00, 1.56MB/s]"
          }
        },
        "ac7c72e2e80547dc983b8b525959001f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9cdc3ea5324b0fb6ea24ac464918af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "357d96cf6a8b49d6835b3c3561405289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a48d2f8041b4548bd144c53d5388615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50f2ccb731f49079f1e5686f044b514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2355602a97024827b88855f72f8b0999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9073c358088495c8421f0052e52445f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba1b64a2f4214ebdb9fd8e6656f20c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d7b2bc39f014c9c871bddbd5270a21a",
              "IPY_MODEL_223bf4622ffa492c985d665cf36cc0dd",
              "IPY_MODEL_0260d2c7f71149bd88c35a938eda887d"
            ],
            "layout": "IPY_MODEL_85ff0058b3f842608d587c28b68662c8"
          }
        },
        "5d7b2bc39f014c9c871bddbd5270a21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bff531258344601927d91bca35dd246",
            "placeholder": "​",
            "style": "IPY_MODEL_351680af59cd420da6f79816f68b80e4",
            "value": "added_tokens.json: 100%"
          }
        },
        "223bf4622ffa492c985d665cf36cc0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99173a3ae2024f5982ac860d30c9402d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a312de900c68435d9a6e2e6ffc7c4936",
            "value": 2
          }
        },
        "0260d2c7f71149bd88c35a938eda887d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbf602a21744e188a063083a75ba833",
            "placeholder": "​",
            "style": "IPY_MODEL_130db07b81e64547b13fb78641f1fb31",
            "value": " 2.00/2.00 [00:00&lt;00:00, 60.5B/s]"
          }
        },
        "85ff0058b3f842608d587c28b68662c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bff531258344601927d91bca35dd246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351680af59cd420da6f79816f68b80e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99173a3ae2024f5982ac860d30c9402d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a312de900c68435d9a6e2e6ffc7c4936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fbf602a21744e188a063083a75ba833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130db07b81e64547b13fb78641f1fb31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a198d9f0cac418389ef3405d35a3fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92d5f9952d9b4688ab646d0f630c67f6",
              "IPY_MODEL_7ec6c714d5834b5f990bbd8d561e1943",
              "IPY_MODEL_a8cb44ae119c44be807fc3acac3921dd"
            ],
            "layout": "IPY_MODEL_5adae7dcf5d54bdb97edd4e549deb2f9"
          }
        },
        "92d5f9952d9b4688ab646d0f630c67f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9484de7528449058d309e1e0def2999",
            "placeholder": "​",
            "style": "IPY_MODEL_157039af7a8d44299f97f27dd8ccd7f1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "7ec6c714d5834b5f990bbd8d561e1943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d92e96f491b442194aa70e8a6b4ccee",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_664d04ccfef8460984418f04e71a9007",
            "value": 112
          }
        },
        "a8cb44ae119c44be807fc3acac3921dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c17b7744158a448896aad8812b327bda",
            "placeholder": "​",
            "style": "IPY_MODEL_52931570e243479896f8f791693f9bf8",
            "value": " 112/112 [00:00&lt;00:00, 4.16kB/s]"
          }
        },
        "5adae7dcf5d54bdb97edd4e549deb2f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9484de7528449058d309e1e0def2999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157039af7a8d44299f97f27dd8ccd7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d92e96f491b442194aa70e8a6b4ccee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664d04ccfef8460984418f04e71a9007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c17b7744158a448896aad8812b327bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52931570e243479896f8f791693f9bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dda7a5a2a3c04dc4aed55cc9bc7bd5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19f96645145c46de863d24f62802ee89",
              "IPY_MODEL_45c4627054544a7abe82a232dbbdb270",
              "IPY_MODEL_cb5596c133094bbb9518ba4754fd5844"
            ],
            "layout": "IPY_MODEL_293b590c9d10454f89c619c9ea7a032d"
          }
        },
        "19f96645145c46de863d24f62802ee89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef39645116f2433dafbdc067f58cb9e2",
            "placeholder": "​",
            "style": "IPY_MODEL_2a4498b52be64694b221a361a7a88b8d",
            "value": "model.safetensors: 100%"
          }
        },
        "45c4627054544a7abe82a232dbbdb270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bff3ae090b442f68932a09076df7a66",
            "max": 433292294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_533fce5ea7324727ade621adb06f4855",
            "value": 433292294
          }
        },
        "cb5596c133094bbb9518ba4754fd5844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e590f0043064a06b7e1bda995b6336d",
            "placeholder": "​",
            "style": "IPY_MODEL_6111b1be06644e06b1e798e23e804a9d",
            "value": " 433M/433M [00:09&lt;00:00, 32.6MB/s]"
          }
        },
        "293b590c9d10454f89c619c9ea7a032d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef39645116f2433dafbdc067f58cb9e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4498b52be64694b221a361a7a88b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bff3ae090b442f68932a09076df7a66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533fce5ea7324727ade621adb06f4855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e590f0043064a06b7e1bda995b6336d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6111b1be06644e06b1e798e23e804a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f2b0ed79d38450d90c0f9aaf26b703d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4c9e4b8d082240af91f07e5801d660c6",
              "IPY_MODEL_31f77130c9fc4123924a09cbae7b55cf",
              "IPY_MODEL_b2834fc068b34027b9749dd2e147f110"
            ],
            "layout": "IPY_MODEL_22b331ae9213459793c664fa8bae9f1b"
          }
        },
        "4c9e4b8d082240af91f07e5801d660c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dfd16ae1d0c94c5a8f1d61dff63c9472",
            "placeholder": "​",
            "style": "IPY_MODEL_fb53263e2a0046ec84b6af30412e15a6",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "31f77130c9fc4123924a09cbae7b55cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b35f95cd0be44ccbe9b86f517b70514",
            "max": 52554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e17b1aece8d74f478c55df692f85064b",
            "value": 52554
          }
        },
        "b2834fc068b34027b9749dd2e147f110": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c2ac16576c948cea2357e2e6dc1e1f5",
            "placeholder": "​",
            "style": "IPY_MODEL_165c03cdec8740298c5fbfbd10e1e7a2",
            "value": " 424k/? [00:00&lt;00:00, 20.5MB/s]"
          }
        },
        "22b331ae9213459793c664fa8bae9f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfd16ae1d0c94c5a8f1d61dff63c9472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb53263e2a0046ec84b6af30412e15a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b35f95cd0be44ccbe9b86f517b70514": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e17b1aece8d74f478c55df692f85064b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c2ac16576c948cea2357e2e6dc1e1f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "165c03cdec8740298c5fbfbd10e1e7a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2358c7e72a614e82983c4fa1c1786235": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_df5948e06714428fa678a301dcfbaec5",
              "IPY_MODEL_f84a770d415b4d0999bc9688ce8c0521",
              "IPY_MODEL_98e1deb8366049bf82dff5fb8aefcf95"
            ],
            "layout": "IPY_MODEL_58f47b40c87c45eaa1d1b9d4a7ceeec4"
          }
        },
        "df5948e06714428fa678a301dcfbaec5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_869b15e439c54574a6c792d55e44f327",
            "placeholder": "​",
            "style": "IPY_MODEL_f61f2229c317449783cb8ca91ed90e48",
            "value": "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/default.zip: 100%"
          }
        },
        "f84a770d415b4d0999bc9688ce8c0521": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7b16ac57b8545978dd8410265efd669",
            "max": 455516818,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7a8a68428524ff296babb0dec308625",
            "value": 455516818
          }
        },
        "98e1deb8366049bf82dff5fb8aefcf95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1b370a2c76864358a6799e4dd69a535e",
            "placeholder": "​",
            "style": "IPY_MODEL_f2b9aac9d7bf4324ba1734bed66c94f5",
            "value": " 456M/456M [00:05&lt;00:00, 161MB/s]"
          }
        },
        "58f47b40c87c45eaa1d1b9d4a7ceeec4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "869b15e439c54574a6c792d55e44f327": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f61f2229c317449783cb8ca91ed90e48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7b16ac57b8545978dd8410265efd669": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7a8a68428524ff296babb0dec308625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1b370a2c76864358a6799e4dd69a535e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2b9aac9d7bf4324ba1734bed66c94f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f59f8769d1d463b8dcc2ad600f31acd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_546f7060068d45aaa7ddb7482d788c53",
              "IPY_MODEL_359dfed2e58e4c728d2719a78dc0ca6e",
              "IPY_MODEL_375b13829cd4422f802c426cf72d6bd9"
            ],
            "layout": "IPY_MODEL_a651b3dba8714c40ac320519aca87f3e"
          }
        },
        "546f7060068d45aaa7ddb7482d788c53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f26d0a03a31e414d827d4b2a120355e8",
            "placeholder": "​",
            "style": "IPY_MODEL_2aa366237ee9438cad91323109003d84",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "359dfed2e58e4c728d2719a78dc0ca6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2ddb8ba063441c4bdf6548fd78ebc31",
            "max": 52554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3913e2a3cf3c4e76b0a116275a44b8c8",
            "value": 52554
          }
        },
        "375b13829cd4422f802c426cf72d6bd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c23b007e16fd4af8bb78d5c56726fbf7",
            "placeholder": "​",
            "style": "IPY_MODEL_95b5a74373e640e1a3dddaf0f79545fa",
            "value": " 424k/? [00:00&lt;00:00, 24.2MB/s]"
          }
        },
        "a651b3dba8714c40ac320519aca87f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f26d0a03a31e414d827d4b2a120355e8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aa366237ee9438cad91323109003d84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2ddb8ba063441c4bdf6548fd78ebc31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3913e2a3cf3c4e76b0a116275a44b8c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c23b007e16fd4af8bb78d5c56726fbf7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95b5a74373e640e1a3dddaf0f79545fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6dec49fa3e74e0497b56dec5f5bfe39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6efd2dac3abb4bb28a58a7e786b0dbc7",
              "IPY_MODEL_e22ba9648e8b4ef49196fab4286e23fb",
              "IPY_MODEL_c48776fd91484b0f9a3152d5c95ace2f"
            ],
            "layout": "IPY_MODEL_a5b1eab5922449afba948ded1cf8dfa1"
          }
        },
        "6efd2dac3abb4bb28a58a7e786b0dbc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70ad6580689b4760b3c66ff3bd1adce5",
            "placeholder": "​",
            "style": "IPY_MODEL_501795b33408438fac1f3aaaa0c58d0d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "e22ba9648e8b4ef49196fab4286e23fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d5a24a0a192845a79b5a393ff8ff066a",
            "max": 264,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_35d3b2b222c24210986f1a20394f8619",
            "value": 264
          }
        },
        "c48776fd91484b0f9a3152d5c95ace2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0bf15c9b0dc446e59665349f35607105",
            "placeholder": "​",
            "style": "IPY_MODEL_c689d16707c2499ead910ede868bb5f6",
            "value": " 264/264 [00:00&lt;00:00, 8.30kB/s]"
          }
        },
        "a5b1eab5922449afba948ded1cf8dfa1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70ad6580689b4760b3c66ff3bd1adce5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "501795b33408438fac1f3aaaa0c58d0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5a24a0a192845a79b5a393ff8ff066a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35d3b2b222c24210986f1a20394f8619": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0bf15c9b0dc446e59665349f35607105": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c689d16707c2499ead910ede868bb5f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c2b38b3f77f54006bc037e348fca994d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_58053e8c67a34ee9ab89520f44934b8d",
              "IPY_MODEL_e30e5d2b680a41cd8a1ca08968094c98",
              "IPY_MODEL_66b0e5c8da4d4beb8e396c33d594aff4"
            ],
            "layout": "IPY_MODEL_08bb325b93ec4dbf864689b75fdc63e3"
          }
        },
        "58053e8c67a34ee9ab89520f44934b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6105e1a68b614eaebaca27cf77f56b51",
            "placeholder": "​",
            "style": "IPY_MODEL_de6b44938d104b59bd9be3a796a95d93",
            "value": "config.json: 100%"
          }
        },
        "e30e5d2b680a41cd8a1ca08968094c98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b7cd39f8b2b4bec859daded93bda415",
            "max": 1105,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8d23af6bffe6422faf28114b62901406",
            "value": 1105
          }
        },
        "66b0e5c8da4d4beb8e396c33d594aff4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e6729756ad4f99a1ad7d32a67990c7",
            "placeholder": "​",
            "style": "IPY_MODEL_8238bd5ce4624ad89cbdfff5ef84365c",
            "value": " 1.10k/1.10k [00:00&lt;00:00, 68.0kB/s]"
          }
        },
        "08bb325b93ec4dbf864689b75fdc63e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6105e1a68b614eaebaca27cf77f56b51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6b44938d104b59bd9be3a796a95d93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6b7cd39f8b2b4bec859daded93bda415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d23af6bffe6422faf28114b62901406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6e6729756ad4f99a1ad7d32a67990c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8238bd5ce4624ad89cbdfff5ef84365c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a5c0abaf49c4a0e893601b3a64ee12c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_18dec80a771a4909b9819c1adf1a8a12",
              "IPY_MODEL_2b7dedf4572946938aaddb039a383da1",
              "IPY_MODEL_82d5ca52b4f94fa8919cdb08acd80046"
            ],
            "layout": "IPY_MODEL_00d66d2f3392429a87f882fc6caab54b"
          }
        },
        "18dec80a771a4909b9819c1adf1a8a12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae688afe18c145d7829bc362bb2cf06b",
            "placeholder": "​",
            "style": "IPY_MODEL_e9b5e018eda14deb8463a1455160cc5b",
            "value": "vocab.txt: 100%"
          }
        },
        "2b7dedf4572946938aaddb039a383da1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1cad711a4cff47a988ca297b60236ac0",
            "max": 995526,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b46a91fc9a3473fb459341e2e99b034",
            "value": 995526
          }
        },
        "82d5ca52b4f94fa8919cdb08acd80046": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f7fcc7fb65348a49e35d1b9f651ee32",
            "placeholder": "​",
            "style": "IPY_MODEL_9d411f11d4ee4ba6abb4f0f5180d240d",
            "value": " 996k/996k [00:00&lt;00:00, 9.88MB/s]"
          }
        },
        "00d66d2f3392429a87f882fc6caab54b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ae688afe18c145d7829bc362bb2cf06b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9b5e018eda14deb8463a1455160cc5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cad711a4cff47a988ca297b60236ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b46a91fc9a3473fb459341e2e99b034": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3f7fcc7fb65348a49e35d1b9f651ee32": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9d411f11d4ee4ba6abb4f0f5180d240d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb2e9b2ac76d4207ab295806486bf080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e23e4ed8366748939d6a865fc363124e",
              "IPY_MODEL_7a012e7dd292449c9b109d30212da5c9",
              "IPY_MODEL_1e01bd647553482eb3851758584d4e70"
            ],
            "layout": "IPY_MODEL_17e3070ad9b64992bbaf6d9601a07274"
          }
        },
        "e23e4ed8366748939d6a865fc363124e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e6e7dd6104db44729caeb98a486a424d",
            "placeholder": "​",
            "style": "IPY_MODEL_53e2a50e482b42a5ac5310bffd01eedc",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "7a012e7dd292449c9b109d30212da5c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c7ba345679144329b58e68017d8db7b",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e78cc9c96e57420f8024f6d344dd070d",
            "value": 112
          }
        },
        "1e01bd647553482eb3851758584d4e70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c023e240a4b415881adc31955425e49",
            "placeholder": "​",
            "style": "IPY_MODEL_82ff4d1886af40fa9cee92ee66bfdcfc",
            "value": " 112/112 [00:00&lt;00:00, 5.51kB/s]"
          }
        },
        "17e3070ad9b64992bbaf6d9601a07274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e6e7dd6104db44729caeb98a486a424d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53e2a50e482b42a5ac5310bffd01eedc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c7ba345679144329b58e68017d8db7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e78cc9c96e57420f8024f6d344dd070d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9c023e240a4b415881adc31955425e49": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82ff4d1886af40fa9cee92ee66bfdcfc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7329c230cce8492a8e0a1eceffec1682": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_63fa36456e0941cd8e88bd6501225552",
              "IPY_MODEL_6d21f049e1c04df99a24041b02618871",
              "IPY_MODEL_85afdc7d15bf4b47965ea941d04b0c90"
            ],
            "layout": "IPY_MODEL_2145217afc964b8fb5aebcbdbd89004e"
          }
        },
        "63fa36456e0941cd8e88bd6501225552": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_904f34f10356404099636325e07cb5e6",
            "placeholder": "​",
            "style": "IPY_MODEL_6e095b12de39400f902044fba18809e1",
            "value": "model.safetensors: 100%"
          }
        },
        "6d21f049e1c04df99a24041b02618871": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ea8a45add2d4b90b6ea06d0ad224107",
            "max": 709106620,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01a5dc851c234e19b4e6884e79cba1d5",
            "value": 709106620
          }
        },
        "85afdc7d15bf4b47965ea941d04b0c90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1202b3dfd9749e8bfbc7384bf0f00dc",
            "placeholder": "​",
            "style": "IPY_MODEL_0a83484412274610a5d2716c688a54a2",
            "value": " 709M/709M [00:08&lt;00:00, 99.9MB/s]"
          }
        },
        "2145217afc964b8fb5aebcbdbd89004e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "904f34f10356404099636325e07cb5e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e095b12de39400f902044fba18809e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ea8a45add2d4b90b6ea06d0ad224107": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01a5dc851c234e19b4e6884e79cba1d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c1202b3dfd9749e8bfbc7384bf0f00dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a83484412274610a5d2716c688a54a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15ce6034796a49a3b6e7c9e7a2f03706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f3d65901d194eafa68c57ac5f70e884",
              "IPY_MODEL_62bb192f2bbc48d4b97ee4e4c0734d5f",
              "IPY_MODEL_6ce8d1d9ce1f47cd9eb579659fbadfc9"
            ],
            "layout": "IPY_MODEL_a1fc843819284c98899aeec4fb0aa9a1"
          }
        },
        "5f3d65901d194eafa68c57ac5f70e884": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f057cc731996435d9f58f6e1c0f1d531",
            "placeholder": "​",
            "style": "IPY_MODEL_ca471935e409406e82cba346e94868a3",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "62bb192f2bbc48d4b97ee4e4c0734d5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c95842da4d71426293cc3e2301bd5f28",
            "max": 52554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6658a810cf0b44c89c339a1a9ca2d749",
            "value": 52554
          }
        },
        "6ce8d1d9ce1f47cd9eb579659fbadfc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8a4387ada3b2444cbd91074a6bf1adf4",
            "placeholder": "​",
            "style": "IPY_MODEL_94e1cdc39f374f57a4a471fb284fcd45",
            "value": " 424k/? [00:00&lt;00:00, 9.35MB/s]"
          }
        },
        "a1fc843819284c98899aeec4fb0aa9a1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f057cc731996435d9f58f6e1c0f1d531": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca471935e409406e82cba346e94868a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c95842da4d71426293cc3e2301bd5f28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6658a810cf0b44c89c339a1a9ca2d749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8a4387ada3b2444cbd91074a6bf1adf4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94e1cdc39f374f57a4a471fb284fcd45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bf24cbaaa71b49d79216a262d86ed746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_98459161f55d4b26bc3e109f28ea2da8",
              "IPY_MODEL_3f62967e70344ec892105a60e2679864",
              "IPY_MODEL_c1e85f7a125d4013a307e13b1eb136cc"
            ],
            "layout": "IPY_MODEL_462518975075445c9def79715409bc3d"
          }
        },
        "98459161f55d4b26bc3e109f28ea2da8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00ba219608104d638f56e79da271165b",
            "placeholder": "​",
            "style": "IPY_MODEL_79c5b09923d945929bcf1539d79794b4",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "3f62967e70344ec892105a60e2679864": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b85a277075a447388cb8aef59159ab6d",
            "max": 52554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9c66679093de48f6accb49e52afa80bd",
            "value": 52554
          }
        },
        "c1e85f7a125d4013a307e13b1eb136cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d36c6ace5914809b8cccd911b29e8f2",
            "placeholder": "​",
            "style": "IPY_MODEL_8d71414f037549e0b129cc7c6139d786",
            "value": " 424k/? [00:00&lt;00:00, 21.8MB/s]"
          }
        },
        "462518975075445c9def79715409bc3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "00ba219608104d638f56e79da271165b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79c5b09923d945929bcf1539d79794b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b85a277075a447388cb8aef59159ab6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c66679093de48f6accb49e52afa80bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3d36c6ace5914809b8cccd911b29e8f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d71414f037549e0b129cc7c6139d786": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9486ac661de14f0a81c1806d360226b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_22ee249c4985493584b7a85c6de51a48",
              "IPY_MODEL_c4fce953344e4a3e9f3ad640776e6829",
              "IPY_MODEL_2ab2fe63e9cc43e8a013455da89b7aa0"
            ],
            "layout": "IPY_MODEL_7296b29bab7d42a29f31e7029bb22497"
          }
        },
        "22ee249c4985493584b7a85c6de51a48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_589128f8e0094201839631c0d63f0110",
            "placeholder": "​",
            "style": "IPY_MODEL_f3be6cf3449c4d9ea4f82db6b3ee812c",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "c4fce953344e4a3e9f3ad640776e6829": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1902ab79c4344391be2ff26b424f8e39",
            "max": 52554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d7dd442cb5cb4404b07cb6aa7c670159",
            "value": 52554
          }
        },
        "2ab2fe63e9cc43e8a013455da89b7aa0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_501b95298d7842ac81a3123dd11dcf60",
            "placeholder": "​",
            "style": "IPY_MODEL_8ad945fb2fe34ddeb349fbf84ea8516f",
            "value": " 424k/? [00:00&lt;00:00, 7.35MB/s]"
          }
        },
        "7296b29bab7d42a29f31e7029bb22497": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "589128f8e0094201839631c0d63f0110": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3be6cf3449c4d9ea4f82db6b3ee812c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1902ab79c4344391be2ff26b424f8e39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7dd442cb5cb4404b07cb6aa7c670159": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "501b95298d7842ac81a3123dd11dcf60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8ad945fb2fe34ddeb349fbf84ea8516f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dcd1f0ae6fca4dc19f2a183bf603c5f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f1c661c310a84f098fcacc2916afa98f",
              "IPY_MODEL_db80e0e26af445bbb7f78acfcd84bf2a",
              "IPY_MODEL_7112bf463e7a4c64b4f813aac61832cd"
            ],
            "layout": "IPY_MODEL_2d82a99774004158b96fa38c58b8d545"
          }
        },
        "f1c661c310a84f098fcacc2916afa98f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e4abf65244d245c29d327f83c62d80e9",
            "placeholder": "​",
            "style": "IPY_MODEL_2e0f80f299ba4eb5b605a749256a8a25",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "db80e0e26af445bbb7f78acfcd84bf2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04798674a0084ad18f899c74973131e7",
            "max": 52554,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_52564ad8451d453a883ad4d6ae4c799f",
            "value": 52554
          }
        },
        "7112bf463e7a4c64b4f813aac61832cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc484ea7329148068314321d3e6e470e",
            "placeholder": "​",
            "style": "IPY_MODEL_45abbe53c4a4441dba3c1937c020dc5b",
            "value": " 424k/? [00:00&lt;00:00, 7.41MB/s]"
          }
        },
        "2d82a99774004158b96fa38c58b8d545": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e4abf65244d245c29d327f83c62d80e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e0f80f299ba4eb5b605a749256a8a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "04798674a0084ad18f899c74973131e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52564ad8451d453a883ad4d6ae4c799f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc484ea7329148068314321d3e6e470e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45abbe53c4a4441dba3c1937c020dc5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}