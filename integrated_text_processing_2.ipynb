{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciccioshake/colab/blob/main/integrated_text_processing_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chardet\n",
        "import chardet\n",
        "import os\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "from io import StringIO\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download the necessary Punkt Tokenizer Models\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Scaricare il tokenizer di NLTK se necessario\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Impostazioni per la segmentazione\n",
        "MAX_LEN = 384  # Lunghezza massima di un segmento\n",
        "OVERLAP = 100  # Sovrapposizione tra segmenti\n",
        "PUNCTUATION = [\".\", \"!\", \"?\", \";\", \":\", \",\"]  # Punti di spezzatura preferiti\n",
        "CHUNK_SIZE = 1024 * 1024  # Dimensione del blocco di lettura (1 MB)\n",
        "\n",
        "# Percorsi dei file di output\n",
        "output_txt_path = \"/content/unified_bandi.txt\"\n",
        "output_json_path = \"/content/unified_bandi.json\"\n",
        "output_file_path = \"/content/testo_segmentato_nltk.txt\"\n",
        "\n",
        "# Lista di file caricati\n",
        "document_files = [\"/content/plain_9\", \"/content/plain_10\", \"/content/plain_11\", \"/content/plain_12\"]\n",
        "input_file_paths = [\"/content/plain_9\", \"/content/plain_10\", \"/content/plain_11\", \"/content/plain_12\"]\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Effettua la pulizia del testo rimuovendo caratteri speciali e spazi extra.\"\"\"\n",
        "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Rimuove caratteri non alfanumerici tranne punteggiatura\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Rimuove spazi extra\n",
        "    return text\n",
        "\n",
        "# Funzione per tokenizzare il testo usando NLTK\n",
        "def nltk_tokenize(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "def find_best_split(words, max_len):\n",
        "    \"\"\"Trova il miglior punto di spezzatura in base alla punteggiatura.\"\"\"\n",
        "    if len(words) <= max_len:\n",
        "        return len(words)  # Nessuna spezzatura necessaria\n",
        "    best_split = max_len  # Punto di spezzatura predefinito\n",
        "    for i in range(max_len - 10, max_len - 50, -1):  # Cerca un punto vicino alla fine\n",
        "        if words[i] in PUNCTUATION:\n",
        "            best_split = i + 1  # Include la punteggiatura nel segmento\n",
        "            break\n",
        "    return best_split\n",
        "\n",
        "def segment_text(text):\n",
        "    \"\"\"Segmenta il testo in blocchi di massimo MAX_LEN parole.\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    segments = []\n",
        "    buffer = []\n",
        "    while words:\n",
        "        buffer.extend(words[:MAX_LEN])\n",
        "        words = words[MAX_LEN:]\n",
        "        if len(buffer) >= MAX_LEN:\n",
        "            end = find_best_split(buffer, MAX_LEN)\n",
        "            segments.append(' '.join(buffer[:end]))\n",
        "            buffer = buffer[end - OVERLAP:]  # Sovrapposizione\n",
        "    if buffer:\n",
        "        segments.append(' '.join(buffer))\n",
        "    return '\\n\\n'.join(segments)\n",
        "\n",
        "# Elaborazione dei file in blocchi, iterando su ogni file path\n",
        "# Open the output files before the loop\n",
        "with open(output_file_path, \"a\", encoding=\"utf-8\") as outfile, \\\n",
        "     open(output_file_path.replace(\".txt\", \".json\"), \"w\", encoding=\"utf-8\") as jsonfile:\n",
        "\n",
        "    all_segments = []  # List to store all segments for JSON\n",
        "\n",
        "    for input_file_path in input_file_paths:\n",
        "        with open(input_file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "            buffer = []\n",
        "            segment_count = 0\n",
        "\n",
        "            while True:\n",
        "                chunk = f.read(CHUNK_SIZE)\n",
        "                if not chunk:\n",
        "                    break\n",
        "\n",
        "                words = nltk_tokenize(chunk)\n",
        "                buffer.extend(words)\n",
        "\n",
        "                while len(buffer) >= MAX_LEN:\n",
        "                    end = find_best_split(buffer, MAX_LEN)\n",
        "                    segment = buffer[:end]\n",
        "                    segment_count += 1\n",
        "\n",
        "                    # Write to text file\n",
        "                    outfile.write(\n",
        "                        f\"Segment {segment_count} (Token Count: {len(segment)}):\\n{' '.join(segment)}\\n\\n\"\n",
        "                    )\n",
        "\n",
        "                    # Append to list for JSON\n",
        "                    all_segments.append({\n",
        "                        \"segment_number\": segment_count,\n",
        "                        \"token_count\": len(segment),\n",
        "                        \"text\": ' '.join(segment)\n",
        "                    })\n",
        "\n",
        "                    buffer = buffer[end - OVERLAP:]\n",
        "\n",
        "            if buffer:\n",
        "                segment_count += 1\n",
        "                outfile.write(\n",
        "                    f\"Segment {segment_count} (Token Count: {len(buffer)}):\\n{' '.join(buffer)}\\n\\n\"\n",
        "                )\n",
        "                all_segments.append({\n",
        "                    \"segment_number\": segment_count,\n",
        "                    \"token_count\": len(buffer),\n",
        "                    \"text\": ' '.join(buffer)\n",
        "                })\n",
        "\n",
        "    # Write all segments to JSON file\n",
        "    json.dump(all_segments, jsonfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Creazione della lista JSON\n",
        "documents = []\n",
        "\n",
        "# Unificazione dei file\n",
        "with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "    for file_path in document_files:\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                doc_id = os.path.basename(file_path).replace(\"plain_\", \"\").strip()\n",
        "\n",
        "                # Pulizia e segmentazione\n",
        "                cleaned_content = clean_text(content)\n",
        "                segmented_content = segment_text(cleaned_content)\n",
        "\n",
        "                # Scrivere su file di testo\n",
        "                txt_file.write(f\"\\n--- Document {doc_id} ---\\n\")\n",
        "                txt_file.write(segmented_content + \"\\n\")\n",
        "\n",
        "                # Aggiungere al JSON\n",
        "                documents.append({\"id\": doc_id, \"content\": segmented_content})\n",
        "\n",
        "# Salvare il JSON\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(documents, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Processo completato! File salvati: {output_txt_path}, {output_json_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ffssSKhKGpgc",
        "outputId": "00ccfbd9-4b1a-4756-c0cc-9331e0e02692"
      },
      "id": "ffssSKhKGpgc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (5.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processo completato! File salvati: /content/unified_bandi.txt, /content/unified_bandi.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "2E4mCg9ZLfqD"
      },
      "id": "2E4mCg9ZLfqD"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install chardet\n",
        "import chardet\n",
        "import os\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import xml.etree.ElementTree as ET\n",
        "import glob\n",
        "from io import StringIO\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download the necessary Punkt Tokenizer Models\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Scaricare il tokenizer di NLTK se necessario\n",
        "nltk.download(\"punkt\")\n",
        "\n",
        "# Impostazioni per la segmentazione\n",
        "MAX_LEN = 384  # Lunghezza massima di un segmento\n",
        "OVERLAP = 100  # Sovrapposizione tra segmenti\n",
        "PUNCTUATION = [\".\", \"!\", \"?\", \";\", \":\", \",\"]  # Punti di spezzatura preferiti\n",
        "CHUNK_SIZE = 1024 * 1024  # Dimensione del blocco di lettura (1 MB)\n",
        "\n",
        "# Percorsi dei file di output\n",
        "output_txt_path = \"/content/unified_bandi.txt\"\n",
        "output_json_path = \"/content/unified_bandi.json\"\n",
        "output_file_path = \"/content/testo_segmentato_nltk.txt\"\n",
        "\"\"\"\n",
        "# Lista di file caricati\n",
        "document_files = [\"/content/plain_9\", \"/content/plain_10\", \"/content/plain_11\", \"/content/plain_12\"]\n",
        "input_file_paths = [\"/content/plain_9\", \"/content/plain_10\", \"/content/plain_11\", \"/content/plain_12\"]\n",
        "\"\"\"\n",
        "# Lista di file caricati (modificata)\n",
        "document_files = glob.glob(\"/content/plain/*\")\n",
        "input_file_paths = glob.glob(\"/content/plain/*\")\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Effettua la pulizia del testo rimuovendo caratteri speciali e spazi extra.\"\"\"\n",
        "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Rimuove caratteri non alfanumerici tranne punteggiatura\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Rimuove spazi extra\n",
        "    return text\n",
        "\n",
        "# Funzione per tokenizzare il testo usando NLTK\n",
        "def nltk_tokenize(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "def find_best_split(words, max_len):\n",
        "    \"\"\"Trova il miglior punto di spezzatura in base alla punteggiatura.\"\"\"\n",
        "    if len(words) <= max_len:\n",
        "        return len(words)  # Nessuna spezzatura necessaria\n",
        "    best_split = max_len  # Punto di spezzatura predefinito\n",
        "    for i in range(max_len - 10, max_len - 50, -1):  # Cerca un punto vicino alla fine\n",
        "        if words[i] in PUNCTUATION:\n",
        "            best_split = i + 1  # Include la punteggiatura nel segmento\n",
        "            break\n",
        "    return best_split\n",
        "\n",
        "def segment_text(text):\n",
        "    \"\"\"Segmenta il testo in blocchi di massimo MAX_LEN parole.\"\"\"\n",
        "    words = word_tokenize(text)\n",
        "    segments = []\n",
        "    buffer = []\n",
        "    while words:\n",
        "        buffer.extend(words[:MAX_LEN])\n",
        "        words = words[MAX_LEN:]\n",
        "        if len(buffer) >= MAX_LEN:\n",
        "            end = find_best_split(buffer, MAX_LEN)\n",
        "            segments.append(' '.join(buffer[:end]))\n",
        "            buffer = buffer[end - OVERLAP:]  # Sovrapposizione\n",
        "    if buffer:\n",
        "        segments.append(' '.join(buffer))\n",
        "    return '\\n\\n'.join(segments)\n",
        "\n",
        "# Elaborazione dei file in blocchi, iterando su ogni file path\n",
        "# Open the output files before the loop\n",
        "with open(output_file_path, \"a\", encoding=\"utf-8\") as outfile, \\\n",
        "     open(output_file_path.replace(\".txt\", \".json\"), \"w\", encoding=\"utf-8\") as jsonfile:\n",
        "\n",
        "    all_segments = []  # List to store all segments for JSON\n",
        "\n",
        "    for input_file_path in input_file_paths:\n",
        "        with open(input_file_path, \"r\", encoding=\"ISO-8859-1\") as f:\n",
        "            buffer = []\n",
        "            segment_count = 0\n",
        "\n",
        "            while True:\n",
        "                chunk = f.read(CHUNK_SIZE)\n",
        "                if not chunk:\n",
        "                    break\n",
        "\n",
        "                words = nltk_tokenize(chunk)\n",
        "                buffer.extend(words)\n",
        "\n",
        "                while len(buffer) >= MAX_LEN:\n",
        "                    end = find_best_split(buffer, MAX_LEN)\n",
        "                    segment = buffer[:end]\n",
        "                    segment_count += 1\n",
        "\n",
        "                    # Write to text file\n",
        "                    outfile.write(\n",
        "                        f\"Segment {segment_count} (Token Count: {len(segment)}):\\n{' '.join(segment)}\\n\\n\"\n",
        "                    )\n",
        "\n",
        "                    # Append to list for JSON\n",
        "                    all_segments.append({\n",
        "                        \"segment_number\": segment_count,\n",
        "                        \"token_count\": len(segment),\n",
        "                        \"text\": ' '.join(segment)\n",
        "                    })\n",
        "\n",
        "                    buffer = buffer[end - OVERLAP:]\n",
        "\n",
        "            if buffer:\n",
        "                segment_count += 1\n",
        "                outfile.write(\n",
        "                    f\"Segment {segment_count} (Token Count: {len(buffer)}):\\n{' '.join(buffer)}\\n\\n\"\n",
        "                )\n",
        "                all_segments.append({\n",
        "                    \"segment_number\": segment_count,\n",
        "                    \"token_count\": len(buffer),\n",
        "                    \"text\": ' '.join(buffer)\n",
        "                })\n",
        "\n",
        "    # Write all segments to JSON file\n",
        "    json.dump(all_segments, jsonfile, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Creazione della lista JSON\n",
        "documents = []\n",
        "\n",
        "# Unificazione dei file\n",
        "with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "    for file_path in document_files:\n",
        "        if os.path.exists(file_path):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                content = f.read()\n",
        "                doc_id = os.path.basename(file_path).replace(\"plain_\", \"\").strip()\n",
        "\n",
        "                # Pulizia e segmentazione\n",
        "                cleaned_content = clean_text(content)\n",
        "                segmented_content = segment_text(cleaned_content)\n",
        "\n",
        "                # Scrivere su file di testo\n",
        "                txt_file.write(f\"\\n--- Document {doc_id} ---\\n\")\n",
        "                txt_file.write(segmented_content + \"\\n\")\n",
        "\n",
        "                # Aggiungere al JSON\n",
        "                documents.append({\"id\": doc_id, \"content\": segmented_content})\n",
        "\n",
        "# Salvare il JSON\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(documents, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Processo completato! File salvati: {output_txt_path}, {output_json_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPAx_glbLhwM",
        "outputId": "45183525-910b-41a2-aa2e-d36659ca2d9e"
      },
      "id": "gPAx_glbLhwM",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (5.2.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processo completato! File salvati: /content/unified_bandi.txt, /content/unified_bandi.json\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "integrated_text_processing_2.ipynb",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}