{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-LauCB1c4v-",
        "outputId": "f4e93164-8b83-491d-f3ae-7cb62af2784e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "import os\n",
        "import json\n",
        "import nltk\n",
        "import re\n",
        "import glob\n",
        "from io import StringIO\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "# Download the necessary Punkt Tokenizer Models\n",
        "nltk.download('punkt', quiet=True)\n",
        "try:\n",
        "    nltk.download('punkt_tab', quiet=True)  # Tentiamo di scaricare anche punkt_tab\n",
        "except:\n",
        "    print(\"Impossibile scaricare punkt_tab, utilizzo punkt standard\")\n",
        "\n",
        "# Assicuriamoci che il tokenizer sia disponibile\n",
        "if not nltk.data.find('tokenizers/punkt'):\n",
        "    raise RuntimeError(\"Impossibile trovare il tokenizer punkt di NLTK. Verificare l'installazione di NLTK.\")\n",
        "\n",
        "# Impostazioni per la segmentazione\n",
        "MAX_LEN = 384  # Lunghezza massima di un segmento\n",
        "OVERLAP = 100  # Sovrapposizione tra segmenti\n",
        "PUNCTUATION = [\".\", \"!\", \"?\", \";\", \":\", \",\"]  # Punti di spezzatura preferiti\n",
        "CHUNK_SIZE = 1024 * 1024  # Dimensione del blocco di lettura (1 MB)\n",
        "\n",
        "# Percorsi dei file di output\n",
        "output_txt_path = \"/content/unified_bandi.txt\"\n",
        "output_json_path = \"/content/unified_bandi.json\"\n",
        "output_segments_path = \"/content/testo_segmentato_nltk.txt\"\n",
        "\n",
        "# Lista di file caricati - gestione delle sottocartelle\n",
        "document_files = []\n",
        "# Ottieni tutte le sottocartelle all'interno di /content/plain/\n",
        "subdirs = glob.glob(\"/content/plain/*/\")\n",
        "for subdir in subdirs:\n",
        "    # Per ogni sottocartella, ottieni tutti i file plain_*\n",
        "    files_in_subdir = glob.glob(os.path.join(subdir, \"plain_*\"))\n",
        "    document_files.extend(files_in_subdir)\n",
        "\n",
        "# Se non ci sono sottocartelle o per sicurezza, verifica anche i file direttamente in /content/plain/\n",
        "direct_files = glob.glob(\"/content/plain/plain_*\")\n",
        "document_files.extend(direct_files)\n",
        "\n",
        "# Rimuovi eventuali duplicati\n",
        "document_files = list(set(document_files))\n",
        "\n",
        "def detect_encoding(file_path):\n",
        "    \"\"\"Rileva l'encoding corretto del file.\"\"\"\n",
        "    with open(file_path, 'rb') as f:\n",
        "        raw_data = f.read(CHUNK_SIZE)\n",
        "        result = chardet.detect(raw_data)\n",
        "        return result['encoding']\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Effettua la pulizia del testo rimuovendo caratteri speciali e spazi extra.\"\"\"\n",
        "    text = re.sub(r'[^\\w\\s.,!?]', '', text)  # Rimuove caratteri non alfanumerici tranne punteggiatura\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Rimuove spazi extra\n",
        "    return text\n",
        "\n",
        "def find_best_split(words, max_len):\n",
        "    \"\"\"Trova il miglior punto di spezzatura in base alla punteggiatura.\"\"\"\n",
        "    if len(words) <= max_len:\n",
        "        return len(words)  # Nessuna spezzatura necessaria\n",
        "    best_split = max_len  # Punto di spezzatura predefinito\n",
        "    for i in range(max_len - 10, max_len - 50, -1):  # Cerca un punto vicino alla fine\n",
        "        if i < 0 or i >= len(words):  # Controllo aggiuntivo per sicurezza\n",
        "            continue\n",
        "        if words[i] in PUNCTUATION:\n",
        "            best_split = i + 1  # Include la punteggiatura nel segmento\n",
        "            break\n",
        "    return best_split\n",
        "\n",
        "def segment_text(words):\n",
        "    \"\"\"Segmenta il testo in blocchi di massimo MAX_LEN parole.\"\"\"\n",
        "    segments = []\n",
        "    buffer = []\n",
        "\n",
        "    while words:\n",
        "        buffer.extend(words[:MAX_LEN])\n",
        "        words = words[MAX_LEN:]\n",
        "\n",
        "        if len(buffer) >= MAX_LEN:\n",
        "            end = find_best_split(buffer, MAX_LEN)\n",
        "            segments.append({\n",
        "                \"text\": ' '.join(buffer[:end]),\n",
        "                \"token_count\": len(buffer[:end])\n",
        "            })\n",
        "            buffer = buffer[end - OVERLAP:]  # Sovrapposizione\n",
        "\n",
        "    if buffer:\n",
        "        segments.append({\n",
        "            \"text\": ' '.join(buffer),\n",
        "            \"token_count\": len(buffer)\n",
        "        })\n",
        "\n",
        "    return segments\n",
        "\n",
        "# Elaborazione principale - flusso unificato\n",
        "documents = []\n",
        "all_segments = []\n",
        "segment_count = 0\n",
        "\n",
        "with open(output_segments_path, \"w\", encoding=\"utf-8\") as segments_file:\n",
        "    for file_path in document_files:\n",
        "        # Rilevare l'encoding corretto\n",
        "        encoding = detect_encoding(file_path)\n",
        "        if not encoding:\n",
        "            encoding = \"utf-8\"  # Fallback su utf-8 se non rilevato\n",
        "\n",
        "        # Estrai l'ID del documento (plain_XX) e includi anche l'ID della cartella\n",
        "        file_name = os.path.basename(file_path)\n",
        "        folder_name = os.path.basename(os.path.dirname(file_path))\n",
        "        doc_id = f\"{folder_name}_{file_name.replace('plain_', '').strip()}\"\n",
        "\n",
        "        buffer = []\n",
        "        doc_segments = []\n",
        "\n",
        "        try:\n",
        "            with open(file_path, \"r\", encoding=encoding) as f:\n",
        "                while True:\n",
        "                    chunk = f.read(CHUNK_SIZE)\n",
        "                    if not chunk:\n",
        "                        break\n",
        "\n",
        "                    # Pulizia e tokenizzazione\n",
        "                    cleaned_chunk = clean_text(chunk)\n",
        "                    words = word_tokenize(cleaned_chunk)\n",
        "                    buffer.extend(words)\n",
        "\n",
        "                    while len(buffer) >= MAX_LEN:\n",
        "                        end = find_best_split(buffer, MAX_LEN)\n",
        "                        segment = buffer[:end]\n",
        "                        segment_count += 1\n",
        "\n",
        "                        segment_text = ' '.join(segment)\n",
        "                        segment_data = {\n",
        "                            \"folder\": folder_name,\n",
        "                            \"document\": file_name,\n",
        "                            \"segment_number\": segment_count,\n",
        "                            \"token_count\": len(segment),\n",
        "                            \"text\": segment_text,\n",
        "                            \"document_id\": doc_id\n",
        "                        }\n",
        "\n",
        "                        # Scrivere nel file dei segmenti con formato migliorato\n",
        "                        segments_file.write(\n",
        "                            f\"Folder: {folder_name} | Document: {file_name} | Segment: {segment_count} | Token Count: {len(segment)}\\n{segment_text}\\n\\n\"\n",
        "                        )\n",
        "\n",
        "                        # Aggiungere alla lista generale e a quella del documento\n",
        "                        all_segments.append(segment_data)\n",
        "                        doc_segments.append(segment_data)\n",
        "\n",
        "                        buffer = buffer[end - OVERLAP:]\n",
        "\n",
        "                # Processare l'eventuale buffer rimanente\n",
        "                if buffer:\n",
        "                    segment_count += 1\n",
        "                    segment_text = ' '.join(buffer)\n",
        "                    segment_data = {\n",
        "                        \"folder\": folder_name,\n",
        "                        \"document\": file_name,\n",
        "                        \"segment_number\": segment_count,\n",
        "                        \"token_count\": len(buffer),\n",
        "                        \"text\": segment_text,\n",
        "                        \"document_id\": doc_id\n",
        "                    }\n",
        "\n",
        "                    segments_file.write(\n",
        "                        f\"Folder: {folder_name} | Document: {file_name} | Segment: {segment_count} | Token Count: {len(buffer)}\\n{segment_text}\\n\\n\"\n",
        "                    )\n",
        "\n",
        "                    all_segments.append(segment_data)\n",
        "                    doc_segments.append(segment_data)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore durante la lettura del file {file_path}: {e}\")\n",
        "            continue\n",
        "\n",
        "        # Preparare i dati del documento\n",
        "        doc_content = \"\\n\\n\".join([segment[\"text\"] for segment in doc_segments])\n",
        "        documents.append({\n",
        "            \"id\": doc_id,\n",
        "            \"content\": doc_content,\n",
        "            \"segments\": doc_segments\n",
        "        })\n",
        "\n",
        "# Salvare tutti i segmenti in un file JSON\n",
        "with open(output_segments_path.replace(\".txt\", \".json\"), \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(all_segments, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "# Creazione del file di testo unificato\n",
        "with open(output_txt_path, \"w\", encoding=\"utf-8\") as txt_file:\n",
        "    for doc in documents:\n",
        "        txt_file.write(f\"\\n--- Document {doc['id']} ---\\n\")\n",
        "        txt_file.write(doc['content'] + \"\\n\")\n",
        "\n",
        "# Salvare la collezione di documenti in JSON\n",
        "with open(output_json_path, \"w\", encoding=\"utf-8\") as json_file:\n",
        "    json.dump(documents, json_file, ensure_ascii=False, indent=4)\n",
        "\n",
        "print(f\"Processo completato! File salvati: {output_txt_path}, {output_json_path}, {output_segments_path}\")"
      ],
      "metadata": {
        "id": "TNnIx_q4i1Fi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77402d3-9907-4684-d61e-051f5148425c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processo completato! File salvati: /content/unified_bandi.txt, /content/unified_bandi.json, /content/testo_segmentato_nltk.txt\n"
          ]
        }
      ]
    }
  ]
}