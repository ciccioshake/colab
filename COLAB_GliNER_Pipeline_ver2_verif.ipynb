{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ciccioshake/colab/blob/main/COLAB_GliNER_Pipeline_ver2_verif.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "k1lte16eeJzR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dea9e04e-5527-4689-dd43-d33274adba98"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests SPARQLWrapper Flask\n",
        "!pip install certifi --upgrade\n",
        "!pip install requests SPARQLWrapper Flask urllib3\n",
        "!pip install requests SPARQLWrapper Flask urllib3 pandas\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r requirements.txt\n",
        "!pip install -r requirements.txt\n",
        "!pip install spacy\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJoVE4minOer",
        "outputId": "7c31ba30-00c3-4f2a-9522-4fe615f2f2f0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Collecting SPARQLWrapper\n",
            "  Downloading SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Collecting rdflib>=6.1.1 (from SPARQLWrapper)\n",
            "  Downloading rdflib-7.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n",
            "Downloading SPARQLWrapper-2.0.0-py3-none-any.whl (28 kB)\n",
            "Downloading rdflib-7.1.3-py3-none-any.whl (564 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m564.9/564.9 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rdflib, SPARQLWrapper\n",
            "Successfully installed SPARQLWrapper-2.0.0 rdflib-7.1.3\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.1.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: stanza in /usr/local/lib/python3.11/dist-packages (1.10.1)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.11/dist-packages (from stanza) (2.14.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (4.25.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from stanza) (2.32.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from stanza) (3.4.2)\n",
            "Requirement already satisfied: torch>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from stanza) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (4.12.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.3.0->stanza) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.3.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->stanza) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.3.0->stanza) (3.0.2)\n",
            "Collecting simplemma\n",
            "  Using cached simplemma-1.1.2-py3-none-any.whl.metadata (23 kB)\n",
            "Using cached simplemma-1.1.2-py3-none-any.whl (67.2 MB)\n",
            "Installing collected packages: simplemma\n",
            "Successfully installed simplemma-1.1.2\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m83.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting it-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting it-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.8.0/it_core_news_lg-3.8.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-lg\n",
            "Successfully installed it-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Cloning into 'dbpedia-spotlight-wrapper'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "Cloning into 'dbpedia-spotlight-wrapper'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile dbpedia_spotlight_wrapper.py\n",
        "# Incolla qui il codice del wrapper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GiKy4Mq4nksg",
        "outputId": "3a4fb712-8321-4d46-868b-c1ae0fa20799"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing dbpedia_spotlight_wrapper.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile examples.py\n",
        "# Incolla qui il codice degli esempi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRKmkRFRnnoh",
        "outputId": "b2906281-55af-495b-bafe-4c70f043a9eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing examples.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install certifi --upgrade\n",
        "!pip install requests SPARQLWrapper Flask urllib3\n",
        "# Installazione delle dipendenze\n",
        "!pip install requests SPARQLWrapper Flask urllib3 pandas\n",
        "!python -m spacy download it_core_news_sm\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "!pip install spacy\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hZuYTS97vQ_N",
        "outputId": "82428924-9f82-4cae-d48b-555d2dd237e2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests SPARQLWrapper Flask urllib3\n",
        "# Installazione delle dipendenze\n",
        "!pip install requests SPARQLWrapper Flask urllib3 pandas\n",
        "!python -m spacy download it_core_news_sm\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "!pip install spacy\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElkJjeUJ0959",
        "outputId": "5565f169-365d-4cc0-bf40-bbed74a86a9b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizzazione dei risultati con pandas\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "def display_results_as_table(result):\n",
        "    resources = result.get('Resources', [])\n",
        "    if not resources:\n",
        "        print(\"Nessuna entità trovata.\")\n",
        "        return\n",
        "\n",
        "    # Crea un DataFrame\n",
        "    data = []\n",
        "    for r in resources:\n",
        "        data.append({\n",
        "            'Entità': r.get('surfaceForm'),\n",
        "            'URI Originale': r.get('originalURI', 'N/A'),\n",
        "            'URI Aggiornato': r.get('@URI'),\n",
        "            'Stato URI': r.get('uriStatus', 'N/A'),\n",
        "            'Confidenza': r.get('@confidence', 'N/A')\n",
        "        })\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "    display(df)\n",
        "\n",
        "    return df\n",
        "\n",
        "# Testa con un esempio\n",
        "text = \"Roma è la capitale d'Italia. Milano è una città importante. La Lombardia è una regione del nord Italia.\"\n",
        "result = wrapper.annotate(text)\n",
        "display_results_as_table(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "Kt9yvzBA2bJT",
        "outputId": "24be3201-069d-4619-8419-8c53ef2acde2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wrapper' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-18285319cb68>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# Testa con un esempio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Roma è la capitale d'Italia. Milano è una città importante. La Lombardia è una regione del nord Italia.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mannotate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0mdisplay_results_as_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wrapper' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica se un URI esiste\n",
        "uri = \"http://dbpedia.org/resource/Rome\"\n",
        "exists = wrapper._check_uri_exists(uri)\n",
        "print(f\"L'URI {uri} {'esiste' if exists else 'non esiste'}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "bwVUyrQw3cnp",
        "outputId": "28774323-980e-485c-fae4-a2f4b0c67994"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wrapper' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-3976ac4fe3a9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Verifica se un URI esiste\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0muri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://dbpedia.org/resource/Rome\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mexists\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_uri_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"L'URI {uri} {'esiste' if exists else 'non esiste'}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wrapper' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ottieni l'URI aggiornato per una label\n",
        "label = \"Roma\"\n",
        "old_uri = \"http://it.dbpedia.org/resource/Roma\"\n",
        "updated_uri = wrapper._get_updated_uri(label, old_uri)\n",
        "print(f\"URI aggiornato: {updated_uri}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "id": "kpCHc7WM3hct",
        "outputId": "1766d34a-1acf-4b09-dff5-35d04ec9da3f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'wrapper' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-6d5b8752a812>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Roma\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mold_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"http://it.dbpedia.org/resource/Roma\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mupdated_uri\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_updated_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"URI aggiornato: {updated_uri}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'wrapper' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RwsfDsu_4pVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UWvpEQKM4pSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Installazione delle dipendenze\n",
        "!pip install requests SPARQLWrapper Flask urllib3 pandas\n",
        "!python -m spacy download it_core_news_sm\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "!pip install spacy\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flvz648E4pO7",
        "outputId": "e8b35e77-4181-4afc-9314-da8f458a9033"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper in /usr/local/lib/python3.11/dist-packages (2.0.0)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.11/dist-packages (2.3.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask) (1.9.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper) (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download it_core_news_sm\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "!pip install spacy\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk\n",
        "!python -m spacy download it_core_news_sm\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NL4_EO36CrP",
        "outputId": "7ae61d31-e711-4469-f5cc-93b442ba90fa"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-sm\n",
            "Successfully installed it-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "i49huXXy6uEe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python test_wrapper.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9uT1fOXS6uBl",
        "outputId": "de455127-e9b1-4fc0-c937-7298938b7509"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/test_wrapper.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Clona il repository\n",
        "!git clone https://github.com/tuo-username/dbpedia-spotlight-wrapper.git\n",
        "!cd dbpedia-spotlight-wrapper\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "# Installa le dipendenze\n",
        "!pip install -r requirements.txt\n",
        "!python -m spacy download it_core_news_sm\n",
        "!pip install spacy\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xnszNM7W-4HX",
        "outputId": "c327b7ca-6f2a-4990-8c65-3c4938a1cf0b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'dbpedia-spotlight-wrapper'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "/bin/bash: line 1: cd: dbpedia-spotlight-wrapper: No such file or directory\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper>=1.8.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: Flask>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.1.0)\n",
            "Collecting gunicorn>=20.1.0 (from -r requirements.txt (line 4))\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper>=1.8.5->-r requirements.txt (line 2)) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=2.0.0->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper>=1.8.5->-r requirements.txt (line 2)) (3.2.1)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gunicorn\n",
            "Successfully installed gunicorn-23.0.0\n",
            "Cloning into 'dbpedia-spotlight-wrapper'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "/bin/bash: line 1: cd: dbpedia-spotlight-wrapper: No such file or directory\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper>=1.8.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: Flask>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (23.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper>=1.8.5->-r requirements.txt (line 2)) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=2.0.0->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper>=1.8.5->-r requirements.txt (line 2)) (3.2.1)\n",
            "Requirement already satisfied: requests>=2.25.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.32.3)\n",
            "Requirement already satisfied: SPARQLWrapper>=1.8.5 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (2.0.0)\n",
            "Requirement already satisfied: Flask>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: gunicorn>=20.1.0 in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 4)) (23.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.25.0->-r requirements.txt (line 1)) (2025.1.31)\n",
            "Requirement already satisfied: rdflib>=6.1.1 in /usr/local/lib/python3.11/dist-packages (from SPARQLWrapper>=1.8.5->-r requirements.txt (line 2)) (7.1.3)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (3.1.6)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=2.0.0->-r requirements.txt (line 3)) (1.9.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gunicorn>=20.1.0->-r requirements.txt (line 4)) (24.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=2.0.0->-r requirements.txt (line 3)) (3.0.2)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from rdflib>=6.1.1->SPARQLWrapper>=1.8.5->-r requirements.txt (line 2)) (3.2.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download it_core_news_sm\n",
        "!pip install spacy\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iGxRy4JLBLPj",
        "outputId": "0d3b2e62-9a55-49b3-ac89-6420051310f0"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting it-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_sm-3.8.0/it_core_news_sm-3.8.0-py3-none-any.whl (13.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.0/13.0 MB\u001b[0m \u001b[31m74.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per mostrare tutte le entità spaCy\n",
        "def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, original_text=None):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Mostra tutte le entità riconosciute da spaCy, non solo quelle uniche.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy da utilizzare (opzionale)\n",
        "        original_text: Testo originale da analizzare con spaCy (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback per compatibilità con la versione precedente\n",
        "        spacy_only_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "\n",
        "        if not spacy_only_entities:\n",
        "            # Se non ci sono entità spaCy uniche, segnala che è necessario passare il riconoscitore\n",
        "            if spacy_recognizer is None or original_text is None:\n",
        "                lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "                lines.append(\"Modificare la chiamata a export_entities_to_txt includendo i parametri spacy_recognizer e original_text.\")\n",
        "            else:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "        else:\n",
        "            # Mostra le entità spaCy uniche\n",
        "            for entity in spacy_only_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Correggi il costruttore della classe EntityLinkerWithSpacy:\n",
        "\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Assicurati che questa riga venga eseguita per generare il file\n",
        "    # Nel codice principale, modifica la chiamata così:\n",
        "    output_file_path = export_entities_to_txt(\n",
        "        entities,\n",
        "        stats,\n",
        "        \"entita_italia.txt\",\n",
        "        spacy_recognizer=linker.wikidata.spacy_recognizer,  # Passa il riconoscitore spaCy\n",
        "        original_text=text  # Passa il testo originale\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Kg9sGRyacrFK",
        "outputId": "bf9f0e13-b04f-45b7-e58c-20f6639f8c8b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'WikidataConnector' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-f406539c19c0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;31m# Estendi la classe WikidataConnector esistente per integrare spaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mWikidataConnectorWithSpacy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mWikidataConnector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m     \"\"\"\n\u001b[1;32m    198\u001b[0m     \u001b[0mVersione\u001b[0m \u001b[0mestesa\u001b[0m \u001b[0mdi\u001b[0m \u001b[0mWikidataConnector\u001b[0m \u001b[0mche\u001b[0m \u001b[0mintegra\u001b[0m \u001b[0mspaCy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'WikidataConnector' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "\n",
        "import os\n",
        "\n",
        "def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\"):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') == 'wikidata']\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except requests.RequestException as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "# Esempio di utilizzo\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità\n",
        "    linker = EntityLinker(language=\"it\")\n",
        "\n",
        "    # Testo di esempio esteso\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'wikidata')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in uri_results['stats'].items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    export_entities_to_txt(entities, stats, \"entita_italia.txt\") # Change"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odlCPo5NEWQF",
        "outputId": "a8b6bed7-6c95-4f5b-d53a-65547a418656"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "\n",
            "Trovate 36 entità:\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milan\n",
            "  Tipo: City\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Rome\n",
            "  Tipo: City\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: local\n",
            "\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milan esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Rome esiste\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 36\n",
            "- wikidata_entities: 36\n",
            "- local_entities: 10\n",
            "- entities_with_dbpedia: 36\n",
            "- entities_with_types: 35\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Milano' → http://dbpedia.org/resource/Milan\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "- 'Roma' → http://dbpedia.org/resource/Rome\n",
            "File salvato in: /content/entita_italia.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import logging\n",
        "from typing import List, Dict, Any, Optional, Set, Tuple\n",
        "from abc import ABC, abstractmethod\n",
        "\n",
        "# Configurazione logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[logging.FileHandler(\"entity_recognition.log\"), logging.StreamHandler()]\n",
        ")\n",
        "logger = logging.getLogger(\"entity_recognizer\")\n",
        "\n",
        "###########################################\n",
        "# Classi base astratte per i riconoscitori\n",
        "###########################################\n",
        "\n",
        "class EntityRecognizer(ABC):\n",
        "    \"\"\"Classe base per tutti i riconoscitori di entità.\"\"\"\n",
        "\n",
        "    def __init__(self, name: str, language: str = \"it\"):\n",
        "        self.name = name\n",
        "        self.language = language\n",
        "        self.cache = {}\n",
        "        self.enabled = True\n",
        "\n",
        "    @abstractmethod\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Metodo principale per riconoscere entità in un testo.\"\"\"\n",
        "        pass\n",
        "\n",
        "    def is_available(self) -> bool:\n",
        "        \"\"\"Verifica se il riconoscitore è disponibile.\"\"\"\n",
        "        return self.enabled\n",
        "\n",
        "    def disable(self):\n",
        "        \"\"\"Disabilita il riconoscitore.\"\"\"\n",
        "        self.enabled = False\n",
        "        logger.info(f\"Riconoscitore {self.name} disabilitato.\")\n",
        "\n",
        "    def enable(self):\n",
        "        \"\"\"Abilita il riconoscitore.\"\"\"\n",
        "        self.enabled = True\n",
        "        logger.info(f\"Riconoscitore {self.name} abilitato.\")\n",
        "\n",
        "\n",
        "class KnowledgeBase(ABC):\n",
        "    \"\"\"Classe base per le knowledge base (fonti di conoscenza).\"\"\"\n",
        "\n",
        "    def __init__(self, name: str):\n",
        "        self.name = name\n",
        "        self.cache = {}\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_entity_info(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"Recupera informazioni dettagliate su un'entità.\"\"\"\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def search_entity(self, query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Cerca entità in base a un termine.\"\"\"\n",
        "        pass\n",
        "\n",
        "\n",
        "###########################################\n",
        "# Implementazioni concrete di riconoscitori\n",
        "###########################################\n",
        "\n",
        "class SpacyRecognizer(EntityRecognizer):\n",
        "    \"\"\"Riconoscitore di entità basato su spaCy.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        super().__init__(\"SpaCy\", language)\n",
        "        self.model = None\n",
        "        try:\n",
        "            import spacy\n",
        "            model_name = f\"{language}_core_news_sm\"\n",
        "            self.model = spacy.load(model_name)\n",
        "            logger.info(f\"SpaCy inizializzato con modello {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore inizializzazione SpaCy: {e}\")\n",
        "            self.disable()\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        if not self.is_available() or not self.model:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            doc = self.model(text)\n",
        "            entities = []\n",
        "\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start': ent.start_char,\n",
        "                    'end': ent.end_char,\n",
        "                    'source': self.name\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "            return entities\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore in SpacyRecognizer: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "class NLTKRecognizer(EntityRecognizer):\n",
        "    \"\"\"Riconoscitore di entità basato su NLTK.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        super().__init__(\"NLTK\", language)\n",
        "        self.initialized = False\n",
        "\n",
        "        try:\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica le risorse necessarie\n",
        "            nltk.download('punkt', quiet=True)\n",
        "            nltk.download('averaged_perceptron_tagger', quiet=True)\n",
        "            nltk.download('maxent_ne_chunker', quiet=True)\n",
        "            nltk.download('words', quiet=True)\n",
        "\n",
        "            self.initialized = True\n",
        "            logger.info(\"NLTK inizializzato correttamente\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore inizializzazione NLTK: {e}\")\n",
        "            self.disable()\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        if not self.is_available() or not self.initialized:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            import nltk\n",
        "            from nltk import ne_chunk, pos_tag, word_tokenize\n",
        "            from nltk.tree import Tree\n",
        "\n",
        "            # Nota: NLTK funziona meglio con testi in inglese\n",
        "            if self.language != \"en\":\n",
        "                logger.warning(\"NLTK funziona meglio con testi in inglese.\")\n",
        "\n",
        "            tokens = word_tokenize(text)\n",
        "            pos_tags = pos_tag(tokens)\n",
        "            chunks = ne_chunk(pos_tags)\n",
        "\n",
        "            entities = []\n",
        "\n",
        "            for chunk in chunks:\n",
        "                if isinstance(chunk, Tree):\n",
        "                    text = \" \".join([token for token, pos in chunk.leaves()])\n",
        "                    label = chunk.label()\n",
        "\n",
        "                    entity = {\n",
        "                        'text': text,\n",
        "                        'label': label,\n",
        "                        'source': self.name\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "            return entities\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore in NLTKRecognizer: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer(EntityRecognizer):\n",
        "    \"\"\"Riconoscitore di entità basato su modelli Transformers per NER.\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"dbmdz/bert-large-cased-finetuned-conll03-english\", language: str = \"en\"):\n",
        "        super().__init__(\"TransformersNER\", language)\n",
        "        self.model_name = model_name\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        try:\n",
        "            from transformers import pipeline\n",
        "            self.ner_pipeline = pipeline(\"ner\", model=model_name)\n",
        "            logger.info(f\"Transformer NER inizializzato con modello {model_name}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore inizializzazione Transformer NER: {e}\")\n",
        "            self.disable()\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        if not self.is_available() or not self.ner_pipeline:\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            # Ottieni le predizioni dal modello\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa i token in entità complete\n",
        "            current_entity = None\n",
        "            entities = []\n",
        "\n",
        "            for item in ner_results:\n",
        "                # Se questo token inizia una nuova entità o ne continua una esistente\n",
        "                if current_entity is None or item['entity'].startswith('B-'):\n",
        "                    # Se abbiamo un'entità in corso, aggiungiamola ai risultati\n",
        "                    if current_entity is not None:\n",
        "                        entities.append(current_entity)\n",
        "\n",
        "                    # Inizia una nuova entità\n",
        "                    current_entity = {\n",
        "                        'text': item['word'],\n",
        "                        'label': item['entity'].split('-')[1] if '-' in item['entity'] else item['entity'],\n",
        "                        'score': item['score'],\n",
        "                        'start': item['start'],\n",
        "                        'end': item['end'],\n",
        "                        'source': self.name\n",
        "                    }\n",
        "                else:\n",
        "                    # Continua l'entità corrente\n",
        "                    current_entity['text'] += item['word'].replace('##', '')\n",
        "                    current_entity['end'] = item['end']\n",
        "                    current_entity['score'] = (current_entity['score'] + item['score']) / 2\n",
        "\n",
        "            # Aggiungi l'ultima entità se esiste\n",
        "            if current_entity is not None:\n",
        "                entities.append(current_entity)\n",
        "\n",
        "            return entities\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore in TransformersNERRecognizer: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "class ZeroShotClassifier(EntityRecognizer):\n",
        "    \"\"\"Classificatore zero-shot per entità già identificate.\"\"\"\n",
        "\n",
        "    def __init__(self, candidate_labels: List[str], language: str = \"it\"):\n",
        "        super().__init__(\"ZeroShotClassifier\", language)\n",
        "        self.candidate_labels = candidate_labels\n",
        "        self.classifier = None\n",
        "\n",
        "        try:\n",
        "            from transformers import pipeline\n",
        "            self.classifier = pipeline(\"zero-shot-classification\")\n",
        "            logger.info(f\"Zero-shot classifier inizializzato con {len(candidate_labels)} etichette\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore inizializzazione Zero-shot classifier: {e}\")\n",
        "            self.disable()\n",
        "\n",
        "    def classify_entity(self, entity_text: str) -> Dict[str, Any]:\n",
        "        \"\"\"Classifica un'entità usando il classificatore zero-shot.\"\"\"\n",
        "        if not self.is_available() or not self.classifier:\n",
        "            return {}\n",
        "\n",
        "        try:\n",
        "            result = self.classifier(entity_text, self.candidate_labels)\n",
        "\n",
        "            return {\n",
        "                'labels': result['labels'],\n",
        "                'scores': result['scores'],\n",
        "                'top_label': result['labels'][0],\n",
        "                'top_score': result['scores'][0]\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nella classificazione zero-shot: {e}\")\n",
        "            return {}\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Nota: questo metodo non è destinato a essere usato direttamente.\n",
        "        Il classificatore zero-shot dovrebbe essere usato per arricchire entità\n",
        "        già riconosciute da altri recognizer.\n",
        "        \"\"\"\n",
        "        logger.warning(\"ZeroShotClassifier non è progettato per riconoscere entità direttamente.\")\n",
        "        return []\n",
        "\n",
        "\n",
        "class BabelfyRecognizer(EntityRecognizer):\n",
        "    \"\"\"Riconoscitore di entità basato su Babelfy.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, language: str = \"IT\"):\n",
        "        super().__init__(\"Babelfy\", language)\n",
        "        self.api_key = api_key\n",
        "        self.base_url = \"https://babelfy.io/v1/disambiguate\"\n",
        "\n",
        "        if not api_key:\n",
        "            logger.error(\"API key per Babelfy non fornita\")\n",
        "            self.disable()\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        if not self.is_available():\n",
        "            return []\n",
        "\n",
        "        try:\n",
        "            import requests\n",
        "\n",
        "            params = {\n",
        "                \"text\": text,\n",
        "                \"lang\": self.language.upper(),\n",
        "                \"key\": self.api_key,\n",
        "                \"extAIDAentities\": True\n",
        "            }\n",
        "\n",
        "            response = requests.get(self.base_url, params=params)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                logger.error(f\"Errore API Babelfy: {response.status_code} - {response.text}\")\n",
        "                return []\n",
        "\n",
        "            data = response.json()\n",
        "            entities = []\n",
        "\n",
        "            for item in data:\n",
        "                if 'charFragment' in item and 'tokenFragment' in item:\n",
        "                    char_fragment = item['charFragment']\n",
        "                    start = char_fragment['start']\n",
        "                    end = char_fragment['end'] + 1\n",
        "\n",
        "                    entity_text = text[start:end]\n",
        "\n",
        "                    entity = {\n",
        "                        'text': entity_text,\n",
        "                        'babelSynsetID': item.get('babelSynsetID', ''),\n",
        "                        'DBpediaURL': item.get('DBpediaURL', ''),\n",
        "                        'BabelNetURL': item.get('BabelNetURL', ''),\n",
        "                        'score': item.get('score', 0),\n",
        "                        'coherenceScore': item.get('coherenceScore', 0),\n",
        "                        'globalScore': item.get('globalScore', 0),\n",
        "                        'source': self.name\n",
        "                    }\n",
        "\n",
        "                    entities.append(entity)\n",
        "\n",
        "            return entities\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore in BabelfyRecognizer: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "###########################################\n",
        "# Implementazioni di knowledge base\n",
        "###########################################\n",
        "\n",
        "class WikidataKnowledgeBase(KnowledgeBase):\n",
        "    \"\"\"Knowledge base basata su Wikidata.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        super().__init__(\"Wikidata\")\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "\n",
        "    def search_entity(self, query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            query: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{query}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            import requests\n",
        "\n",
        "            # Parametri per la ricerca su Wikidata\n",
        "            params = {\n",
        "                'action': 'wbsearchentities',\n",
        "                'search': query,\n",
        "                'language': self.language,\n",
        "                'format': 'json',\n",
        "                'limit': 5\n",
        "            }\n",
        "\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_info(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            import requests\n",
        "\n",
        "            # Parametri per l'API Wikidata\n",
        "            params = {\n",
        "                'action': 'wbgetentities',\n",
        "                'ids': entity_id,\n",
        "                'languages': self.language,\n",
        "                'format': 'json'\n",
        "            }\n",
        "\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def query_sparql(self, query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Esegue una query SPARQL su Wikidata.\n",
        "\n",
        "        Args:\n",
        "            query: La query SPARQL da eseguire\n",
        "\n",
        "        Returns:\n",
        "            Lista di risultati\n",
        "        \"\"\"\n",
        "        try:\n",
        "            from SPARQLWrapper import SPARQLWrapper, JSON\n",
        "\n",
        "            sparql = SPARQLWrapper(\"https://query.wikidata.org/sparql\")\n",
        "            sparql.setQuery(query)\n",
        "            sparql.setReturnFormat(JSON)\n",
        "\n",
        "            results = sparql.query().convert()\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results.get('results', {}).get('bindings', [])\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nell'esecuzione della query SPARQL: {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "class GoogleKnowledgeGraphAPI(KnowledgeBase):\n",
        "    \"\"\"Knowledge base basata su Google Knowledge Graph API.\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: str, language: str = \"it\"):\n",
        "        super().__init__(\"GoogleKnowledgeGraph\")\n",
        "        self.api_key = api_key\n",
        "        self.language = language\n",
        "\n",
        "        if not api_key:\n",
        "            logger.error(\"API key per Google Knowledge Graph non fornita\")\n",
        "\n",
        "    def search_entity(self, query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Google Knowledge Graph in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            query: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        if not self.api_key:\n",
        "            return []\n",
        "\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{query}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            import requests\n",
        "\n",
        "            service_url = 'https://kgsearch.googleapis.com/v1/entities:search'\n",
        "            params = {\n",
        "                'query': query,\n",
        "                'limit': 10,\n",
        "                'indent': True,\n",
        "                'key': self.api_key,\n",
        "                'languages': self.language\n",
        "            }\n",
        "\n",
        "            response = requests.get(service_url, params=params)\n",
        "            response.raise_for_status()\n",
        "\n",
        "            data = response.json()\n",
        "            results = data.get('itemListElement', [])\n",
        "\n",
        "            # Trasforma i risultati in un formato più usabile\n",
        "            entities = []\n",
        "            for item in results:\n",
        "                result = item.get('result', {})\n",
        "                entity = {\n",
        "                    'name': result.get('name', ''),\n",
        "                    'description': result.get('description', ''),\n",
        "                    'type': [t.get('name', '') for t in result.get('@type', [])],\n",
        "                    'id': result.get('@id', ''),\n",
        "                    'score': item.get('resultScore', 0)\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "            self.cache[cache_key] = entities\n",
        "            return entities\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nella ricerca Google Knowledge Graph: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_info(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Google Knowledge Graph API non supporta direttamente il recupero per ID,\n",
        "        quindi questo metodo non è implementato.\n",
        "        \"\"\"\n",
        "        logger.warning(\"get_entity_info non supportato per Google Knowledge Graph API\")\n",
        "        return None\n",
        "\n",
        "\n",
        "class WordNetKnowledgeBase(KnowledgeBase):\n",
        "    \"\"\"Knowledge base basata su WordNet.\"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"eng\"):\n",
        "        super().__init__(\"WordNet\")\n",
        "        self.language = language\n",
        "\n",
        "        try:\n",
        "            import nltk\n",
        "            from nltk.corpus import wordnet\n",
        "\n",
        "            # Scarica le risorse necessarie\n",
        "            nltk.download('wordnet', quiet=True)\n",
        "            nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "            self.wn = wordnet\n",
        "            logger.info(\"WordNet inizializzato correttamente\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore inizializzazione WordNet: {e}\")\n",
        "            self.wn = None\n",
        "\n",
        "    def search_entity(self, query: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca termini in WordNet.\n",
        "\n",
        "        Args:\n",
        "            query: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di synset trovati\n",
        "        \"\"\"\n",
        "        if self.wn is None:\n",
        "            return []\n",
        "\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{query}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            synsets = self.wn.synsets(query, lang=self.language)\n",
        "\n",
        "            # Trasforma i risultati in un formato più usabile\n",
        "            results = []\n",
        "            for synset in synsets:\n",
        "                result = {\n",
        "                    'name': synset.name(),\n",
        "                    'definition': synset.definition(),\n",
        "                    'examples': synset.examples(),\n",
        "                    'lemmas': [lemma.name() for lemma in synset.lemmas()],\n",
        "                    'pos': synset.pos()\n",
        "                }\n",
        "                results.append(result)\n",
        "\n",
        "            self.cache[cache_key] = results\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nella ricerca WordNet: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_info(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene informazioni su un synset specifico in WordNet.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID del synset (es. 'car.n.01')\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le informazioni sul synset o None in caso di errore\n",
        "        \"\"\"\n",
        "        if self.wn is None:\n",
        "            return None\n",
        "\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        try:\n",
        "            synset = self.wn.synset(entity_id)\n",
        "\n",
        "            # Hypernyms (categorie più generali)\n",
        "            hypernyms = [h.name() for h in synset.hypernyms()]\n",
        "\n",
        "            # Hyponyms (categorie più specifiche)\n",
        "            hyponyms = [h.name() for h in synset.hyponyms()]\n",
        "\n",
        "            # Holonyms (relazione \"parte di\")\n",
        "            holonyms = [h.name() for h in synset.member_holonyms() +\n",
        "                       synset.substance_holonyms() +\n",
        "                       synset.part_holonyms()]\n",
        "\n",
        "            # Meronyms (relazione \"ha come parte\")\n",
        "            meronyms = [m.name() for m in synset.member_meronyms() +\n",
        "                       synset.substance_meronyms() +\n",
        "                       synset.part_meronyms()]\n",
        "\n",
        "            result = {\n",
        "                'name': synset.name(),\n",
        "                'definition': synset.definition(),\n",
        "                'examples': synset.examples(),\n",
        "                'lemmas': [lemma.name() for lemma in synset.lemmas()],\n",
        "                'pos': synset.pos(),\n",
        "                'hypernyms': hypernyms,\n",
        "                'hyponyms': hyponyms,\n",
        "                'holonyms': holonyms,\n",
        "                'meronyms': meronyms\n",
        "            }\n",
        "\n",
        "            self.cache[cache_key] = result\n",
        "            return result\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nel recupero del synset {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "\n",
        "###########################################\n",
        "# Sistema di apprendimento continuo\n",
        "###########################################\n",
        "\n",
        "class ContinuousLearningSystem:\n",
        "    \"\"\"\n",
        "    Sistema che apprende continuamente nuove entità e le aggiunge al dizionario.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dictionary_path: str, confidence_threshold: float = 0.7):\n",
        "        self.dictionary_path = dictionary_path\n",
        "        self.confidence_threshold = confidence_threshold\n",
        "        self.dictionary = self._load_dictionary()\n",
        "\n",
        "    def _load_dictionary(self) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Carica il dizionario dal file.\"\"\"\n",
        "        if os.path.exists(self.dictionary_path):\n",
        "            try:\n",
        "                with open(self.dictionary_path, 'r', encoding='utf-8') as f:\n",
        "                    return json.load(f)\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Errore nel caricamento del dizionario: {e}\")\n",
        "                return {}\n",
        "        else:\n",
        "            logger.info(f\"Creazione di un nuovo dizionario in {self.dictionary_path}\")\n",
        "            return {}\n",
        "\n",
        "    def _save_dictionary(self):\n",
        "        \"\"\"Salva il dizionario nel file.\"\"\"\n",
        "        try:\n",
        "            with open(self.dictionary_path, 'w', encoding='utf-8') as f:\n",
        "                json.dump(self.dictionary, f, indent=2, ensure_ascii=False)\n",
        "            logger.info(f\"Dizionario salvato in {self.dictionary_path}\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nel salvataggio del dizionario: {e}\")\n",
        "\n",
        "    def add_entity(self, entity: Dict[str, Any], source: str) -> bool:\n",
        "        \"\"\"\n",
        "        Aggiunge un'entità al dizionario se supera la soglia di confidenza.\n",
        "\n",
        "        Args:\n",
        "            entity: L'entità da aggiungere\n",
        "            source: La fonte dell'entità\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è stata aggiunta, False altrimenti\n",
        "        \"\"\"\n",
        "        if 'text' not in entity:\n",
        "            logger.warning(\"Tentativo di aggiungere un'entità senza testo\")\n",
        "            return False\n",
        "\n",
        "        text = entity['text'].lower()\n",
        "\n",
        "        # Verifica se l'entità è già nel dizionario\n",
        "        if text in self.dictionary:\n",
        "            # Aggiorna solo se la nuova entità ha più informazioni\n",
        "            existing = self.dictionary[text]\n",
        "\n",
        "            # Aggiungi la nuova fonte se non è già presente\n",
        "            sources = existing.get('sources', [])\n",
        "            if source not in sources:\n",
        "                sources.append(source)\n",
        "                existing['sources'] = sources\n",
        "\n",
        "            # Aggiorna i tipi se non sono già presenti\n",
        "            if 'types' in entity and entity['types']:\n",
        "                existing_types = set(existing.get('types', []))\n",
        "                new_types = set(entity.get('types', []))\n",
        "                existing['types'] = list(existing_types.union(new_types))\n",
        "\n",
        "            # Aggiorna gli URI se non sono già presenti\n",
        "            if 'dbpedia_uri' in entity and entity['dbpedia_uri']:\n",
        "                existing['dbpedia_uri'] = entity['dbpedia_uri']\n",
        "\n",
        "            if 'wikidata_id' in entity and entity['wikidata_id']:\n",
        "                existing['wikidata_id'] = entity['wikidata_id']\n",
        "\n",
        "            self.dictionary[text] = existing\n",
        "            self._save_dictionary()\n",
        "            return True\n",
        "\n",
        "        # Altrimenti, aggiungi la nuova entità se supera la soglia di confidenza\n",
        "        confidence = entity.get('score', 0)\n",
        "        if confidence >= self.confidence_threshold:\n",
        "            new_entry = {\n",
        "                'text': entity['text'],\n",
        "                'types': entity.get('types', []),\n",
        "                'dbpedia_uri': entity.get('dbpedia_uri'),\n",
        "                'wikidata_id': entity.get('wikidata_id'),\n",
        "                'score': confidence,\n",
        "                'sources': [source],\n",
        "                'added_date': time.strftime('%Y-%m-%d %H:%M:%S')\n",
        "            }\n",
        "\n",
        "            self.dictionary[text] = new_entry\n",
        "            self._save_dictionary()\n",
        "            return True\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_entity(self, text: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Recupera un'entità dal dizionario.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo dell'entità\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le informazioni sull'entità o None se non trovata\n",
        "        \"\"\"\n",
        "        return self.dictionary.get(text.lower())\n",
        "\n",
        "    def get_all_entities(self) -> Dict[str, Dict[str, Any]]:\n",
        "        \"\"\"Restituisce tutte le entità nel dizionario.\"\"\"\n",
        "        return self.dictionary\n",
        "\n",
        "\n",
        "###########################################\n",
        "# Sistema principale di integrazione\n",
        "###########################################\n",
        "\n",
        "class EntityRecognitionSystem:\n",
        "    \"\"\"\n",
        "    Sistema principale che integra tutti i riconoscitori e le knowledge base.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        self.language = language\n",
        "        self.recognizers = []\n",
        "        self.knowledge_bases = []\n",
        "        self.continuous_learning = None\n",
        "        self.local_dictionary = {}\n",
        "\n",
        "    def add_recognizer(self, recognizer: EntityRecognizer):\n",
        "        \"\"\"Aggiunge un riconoscitore al sistema.\"\"\"\n",
        "        self.recognizers.append(recognizer)\n",
        "        logger.info(f\"Aggiunto riconoscitore: {recognizer.name}\")\n",
        "\n",
        "    def add_knowledge_base(self, kb: KnowledgeBase):\n",
        "        \"\"\"Aggiunge una knowledge base al sistema.\"\"\"\n",
        "        self.knowledge_bases.append(kb)\n",
        "        logger.info(f\"Aggiunta knowledge base: {kb.name}\")\n",
        "\n",
        "    def setup_continuous_learning(self, dictionary_path: str, confidence_threshold: float = 0.7):\n",
        "        \"\"\"Configura il sistema di apprendimento continuo.\"\"\"\n",
        "        self.continuous_learning = ContinuousLearningSystem(dictionary_path, confidence_threshold)\n",
        "        logger.info(f\"Sistema di apprendimento continuo configurato con dizionario in {dictionary_path}\")\n",
        "\n",
        "    def load_local_dictionary(self, dictionary_path: str):\n",
        "        \"\"\"\n",
        "        Carica un dizionario locale di entità conosciute.\n",
        "\n",
        "        Args:\n",
        "            dictionary_path: Percorso al file JSON del dizionario\n",
        "        \"\"\"\n",
        "        if os.path.exists(dictionary_path):\n",
        "            try:\n",
        "                with open(dictionary_path, 'r', encoding='utf-8') as f:\n",
        "                    self.local_dictionary = json.load(f)\n",
        "                logger.info(f\"Dizionario locale caricato da {dictionary_path}: {len(self.local_dictionary)} entità\")\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Errore nel caricamento del dizionario locale: {e}\")\n",
        "        else:\n",
        "            logger.warning(f\"Il file del dizionario locale {dictionary_path} non esiste\")\n",
        "\n",
        "    def recognize_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando tutti i riconoscitori disponibili.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tupla con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        all_entities = []\n",
        "        recognizer_results = {}\n",
        "\n",
        "        # Utilizza tutti i riconoscitori disponibili\n",
        "        for recognizer in self.recognizers:\n",
        "            if recognizer.is_available():\n",
        "                logger.info(f\"Utilizzo riconoscitore: {recognizer.name}\")\n",
        "                entities = recognizer.recognize_entities(text)\n",
        "                recognizer_results[recognizer.name] = entities\n",
        "                all_entities.extend(entities)\n",
        "\n",
        "        # Rimuovi duplicati basati sul testo\n",
        "        # Strategia: mantieni l'entità con il punteggio più alto o più informazioni\n",
        "        unique_entities = {}\n",
        "        for entity in all_entities:\n",
        "            entity_text = entity.get('text', '').lower()\n",
        "            if not entity_text:\n",
        "                continue\n",
        "\n",
        "            if entity_text not in unique_entities:\n",
        "                unique_entities[entity_text] = entity\n",
        "            else:\n",
        "                # Decidi quale entità mantenere\n",
        "                existing = unique_entities[entity_text]\n",
        "\n",
        "                # Se la nuova entità ha un punteggio più alto, sostituisci\n",
        "                new_score = entity.get('score', 0)\n",
        "                existing_score = existing.get('score', 0)\n",
        "\n",
        "                if new_score > existing_score:\n",
        "                    unique_entities[entity_text] = entity\n",
        "                elif 'dbpedia_uri' in entity and 'dbpedia_uri' not in existing:\n",
        "                    # La nuova entità ha un URI DBpedia che manca all'esistente\n",
        "                    unique_entities[entity_text] = entity\n",
        "                elif 'wikidata_id' in entity and 'wikidata_id' not in existing:\n",
        "                    # La nuova entità ha un ID Wikidata che manca all'esistente\n",
        "                    unique_entities[entity_text] = entity\n",
        "                elif 'types' in entity and entity['types'] and ('types' not in existing or not existing['types']):\n",
        "                    # La nuova entità ha tipi che mancano all'esistente\n",
        "                    unique_entities[entity_text] = entity\n",
        "\n",
        "        # Converti in lista\n",
        "        unique_entity_list = list(unique_entities.values())\n",
        "\n",
        "        # Cerca entità locali nel dizionario\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina con entità locali, evitando duplicati\n",
        "        existing_texts = {e.get('text', '').lower() for e in unique_entity_list}\n",
        "        for local_entity in local_entities:\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "            if text_lower and text_lower not in existing_texts:\n",
        "                unique_entity_list.append(local_entity)\n",
        "                existing_texts.add(text_lower)\n",
        "\n",
        "        # Arricchisci le entità con le knowledge base\n",
        "        enriched_entities = self._enrich_entities(unique_entity_list)\n",
        "\n",
        "        # Se il sistema di apprendimento continuo è abilitato,\n",
        "        # aggiungi le nuove entità al dizionario\n",
        "        if self.continuous_learning:\n",
        "            for entity in enriched_entities:\n",
        "                source = entity.get('source', 'unknown')\n",
        "                self.continuous_learning.add_entity(entity, source)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'recognizers_used': len([r for r in self.recognizers if r.is_available()]),\n",
        "            'entities_per_recognizer': {name: len(entities) for name, entities in recognizer_results.items()},\n",
        "            'local_entities': len(local_entities),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_wikidata': sum(1 for e in enriched_entities if 'wikidata_id' in e and e['wikidata_id']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_dictionary.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_dictionary[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info.get('uri'),\n",
        "                    'types': info.get('types', []),\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local_dictionary'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _enrich_entities(self, entities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Arricchisce le entità con informazioni dalle knowledge base.\n",
        "\n",
        "        Args:\n",
        "            entities: Lista di entità da arricchire\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità arricchite\n",
        "        \"\"\"\n",
        "        if not self.knowledge_bases:\n",
        "            return entities\n",
        "\n",
        "        enriched_entities = []\n",
        "\n",
        "        for entity in entities:\n",
        "            entity_text = entity.get('text', '')\n",
        "            if not entity_text:\n",
        "                continue\n",
        "\n",
        "            # Copia l'entità originale\n",
        "            enriched = entity.copy()\n",
        "\n",
        "            # Cerca entità nelle knowledge base\n",
        "            for kb in self.knowledge_bases:\n",
        "                # Se l'entità ha già un ID Wikidata e questa è una KB Wikidata,\n",
        "                # usa l'ID per ottenere informazioni più dettagliate\n",
        "                if 'wikidata_id' in entity and isinstance(kb, WikidataKnowledgeBase):\n",
        "                    entity_info = kb.get_entity_info(entity['wikidata_id'])\n",
        "                    if entity_info:\n",
        "                        # Estrai le informazioni rilevanti\n",
        "                        if 'labels' in entity_info and self.language in entity_info['labels']:\n",
        "                            enriched['label'] = entity_info['labels'][self.language]['value']\n",
        "\n",
        "                        if 'descriptions' in entity_info and self.language in entity_info['descriptions']:\n",
        "                            enriched['description'] = entity_info['descriptions'][self.language]['value']\n",
        "\n",
        "                        if 'aliases' in entity_info and self.language in entity_info['aliases']:\n",
        "                            enriched['aliases'] = [a['value'] for a in entity_info['aliases'][self.language]]\n",
        "\n",
        "                        # Se non abbiamo un URI DBpedia, prova a ottenerlo\n",
        "                        if 'dbpedia_uri' not in enriched or not enriched['dbpedia_uri']:\n",
        "                            dbpedia_uri = self._get_dbpedia_uri_from_wikidata(kb, entity['wikidata_id'])\n",
        "                            if dbpedia_uri:\n",
        "                                enriched['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "                # Altrimenti, cerca l'entità per nome\n",
        "                elif not ('wikidata_id' in enriched or 'dbpedia_uri' in enriched):\n",
        "                    search_results = kb.search_entity(entity_text)\n",
        "                    if search_results:\n",
        "                        top_result = search_results[0]\n",
        "\n",
        "                        # Per Wikidata\n",
        "                        if kb.name == 'Wikidata' and 'id' in top_result:\n",
        "                            enriched['wikidata_id'] = top_result['id']\n",
        "                            enriched['wikidata_url'] = f\"https://www.wikidata.org/wiki/{top_result['id']}\"\n",
        "\n",
        "                            # Ottieni URI DBpedia\n",
        "                            dbpedia_uri = self._get_dbpedia_uri_from_wikidata(kb, top_result['id'])\n",
        "                            if dbpedia_uri:\n",
        "                                enriched['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "                        # Per Google Knowledge Graph\n",
        "                        elif kb.name == 'GoogleKnowledgeGraph':\n",
        "                            if 'id' in top_result:\n",
        "                                enriched['google_kg_id'] = top_result['id']\n",
        "                            if 'type' in top_result:\n",
        "                                enriched['types'] = enriched.get('types', []) + top_result['type']\n",
        "\n",
        "                        # Per WordNet\n",
        "                        elif kb.name == 'WordNet':\n",
        "                            if 'name' in top_result:\n",
        "                                enriched['wordnet_synset'] = top_result['name']\n",
        "                            if 'pos' in top_result:\n",
        "                                enriched['pos'] = top_result['pos']\n",
        "\n",
        "            # Assicurati che i tipi siano unici\n",
        "            if 'types' in enriched and enriched['types']:\n",
        "                enriched['types'] = list(set(enriched['types']))\n",
        "\n",
        "            enriched_entities.append(enriched)\n",
        "\n",
        "        return enriched_entities\n",
        "\n",
        "    def _get_dbpedia_uri_from_wikidata(self, kb: WikidataKnowledgeBase, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Ottiene l'URI DBpedia corrispondente a un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            kb: L'istanza di WikidataKnowledgeBase\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia o None se non trovato\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return kb.get_equivalent_dbpedia_uri(entity_id)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Errore nel recupero dell'URI DBpedia per {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def export_entities_to_txt(self, entities: List[Dict[str, Any]], stats: Dict[str, Any], output_file: str = \"entita_riconosciute.txt\") -> str:\n",
        "        \"\"\"\n",
        "        Esporta le entità trovate in un file di testo.\n",
        "\n",
        "        Args:\n",
        "            entities: Lista di entità trovate\n",
        "            stats: Statistiche sulle entità\n",
        "            output_file: Nome del file di output\n",
        "\n",
        "        Returns:\n",
        "            Percorso completo del file creato\n",
        "        \"\"\"\n",
        "        # Filtra le entità per categoria\n",
        "        wikidata_entities = [e for e in entities if 'wikidata_id' in e]\n",
        "        entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "        entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "\n",
        "        # Prepara i contenuti\n",
        "        lines = []\n",
        "\n",
        "        # Intestazione\n",
        "        lines.append(\"ELENCO ENTITÀ RICONOSCIUTE\")\n",
        "        lines.append(\"========================\\n\")\n",
        "\n",
        "        # Statistiche\n",
        "        lines.append(\"STATISTICHE:\")\n",
        "        lines.append(\"-----------\")\n",
        "        lines.append(f\"Entità totali: {stats['total_entities']}\")\n",
        "        lines.append(f\"Entità con URI DBpedia: {stats['entities_with_dbpedia']}\")\n",
        "        lines.append(f\"Entità con ID Wikidata: {stats['entities_with_wikidata']}\")\n",
        "        lines.append(f\"Entità con tipi definiti: {stats['entities_with_types']}\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "        # Entità Wikidata\n",
        "        lines.append(\"WIKIDATA ENTITIES:\")\n",
        "        lines.append(\"------------------\")\n",
        "        for entity in wikidata_entities:\n",
        "            lines.append(f\"- '{entity['text']}' → ID: {entity['wikidata_id']}\")\n",
        "            if 'dbpedia_uri' in entity and entity['dbpedia_uri']:\n",
        "                lines.append(f\"  DBpedia URI: {entity['dbpedia_uri']}\")\n",
        "            if 'types' in entity and entity['types']:\n",
        "                lines.append(f\"  Tipi: {', '.join(entity['types'])}\")\n",
        "            if 'description' in entity:\n",
        "                lines.append(f\"  Descrizione: {entity['description']}\")\n",
        "            lines.append(\"\")\n",
        "\n",
        "        # Entità con URI DBpedia\n",
        "        lines.append(\"\\nENTITIES WITH DBPEDIA:\")\n",
        "        lines.append(\"----------------------\")\n",
        "        for entity in entities_with_dbpedia:\n",
        "            lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "        lines.append(\"\")\n",
        "\n",
        "        # Entità con tipi definiti\n",
        "        lines.append(\"\\nENTITIES WITH TYPES:\")\n",
        "        lines.append(\"-------------------\")\n",
        "        # Raggruppa per tipo\n",
        "        entities_by_type = {}\n",
        "        for entity in entities_with_types:\n",
        "            for entity_type in entity.get('types', []):\n",
        "                if entity_type not in entities_by_type:\n",
        "                    entities_by_type[entity_type] = []\n",
        "                entities_by_type[entity_type].append(entity)\n",
        "\n",
        "        # Elenca per tipo\n",
        "        for entity_type, type_entities in sorted(entities_by_type.items()):\n",
        "            lines.append(f\"\\nTipo: {entity_type}\")\n",
        "            lines.append(\"-\" * (len(entity_type) + 6))\n",
        "            for entity in type_entities:\n",
        "                lines.append(f\"- '{entity['text']}'\")\n",
        "                if 'dbpedia_uri' in entity and entity['dbpedia_uri']:\n",
        "                    lines.append(f\"  DBpedia: {entity['dbpedia_uri']}\")\n",
        "                if 'wikidata_id' in entity:\n",
        "                    lines.append(f\"  Wikidata: {entity['wikidata_id']}\")\n",
        "\n",
        "        # Scrivi il file\n",
        "        with open(output_file, 'w', encoding='utf-8') as f:\n",
        "            f.write('\\n'.join(lines))\n",
        "\n",
        "        # Ottieni il percorso completo\n",
        "        output_path = os.path.abspath(output_file)\n",
        "        logger.info(f\"File salvato in: {output_path}\")\n",
        "\n",
        "        return output_path\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.recognize_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }"
      ],
      "metadata": {
        "id": "pbzOsUXEY6QW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4IKkl226cgUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download it_core_news_lg  # Per italiano\n",
        "!python -m spacy download en_core_web_lg   # Per inglese\n",
        "!pip install stanza\n",
        "!pip install simplemma\n",
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhq34lPbcgKF",
        "outputId": "24632e2f-5189-44f1-ad33-b5485db921b6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.11/dist-packages (3.8.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (8.3.4)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (0.15.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.11/dist-packages (from spacy) (2.10.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy) (75.1.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (24.2)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.1.31)\n",
            "Requirement already satisfied: blis<1.3.0,>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.21.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->spacy) (3.0.2)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting it-core-news-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/it_core_news_lg-3.8.0/it_core_news_lg-3.8.0-py3-none-any.whl (567.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m567.9/567.9 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: it-core-news-lg\n",
            "Successfully installed it-core-news-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('it_core_news_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-lg==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.8.0/en_core_web_lg-3.8.0-py3-none-any.whl (400.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.7/400.7 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: en-core-web-lg\n",
            "Successfully installed en-core-web-lg-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_lg')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per mostrare tutte le entità spaCy\n",
        "def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, original_text=None):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Mostra tutte le entità riconosciute da spaCy, non solo quelle uniche.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy da utilizzare (opzionale)\n",
        "        original_text: Testo originale da analizzare con spaCy (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback per compatibilità con la versione precedente\n",
        "        spacy_only_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "\n",
        "        if not spacy_only_entities:\n",
        "            # Se non ci sono entità spaCy uniche, segnala che è necessario passare il riconoscitore\n",
        "            if spacy_recognizer is None or original_text is None:\n",
        "                lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "                lines.append(\"Modificare la chiamata a export_entities_to_txt includendo i parametri spacy_recognizer e original_text.\")\n",
        "            else:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "        else:\n",
        "            # Mostra le entità spaCy uniche\n",
        "            for entity in spacy_only_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Assicurati che questa riga venga eseguita per generare il file\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,  # Passa il riconoscitore spaCy\n",
        "    original_text=text  # Passa il testo originale\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita_italia.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita_italia.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xQE5NAFrwAcf",
        "outputId": "54887850-7653-4197-f597-ea1ecb40906d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "\n",
            "Trovate 37 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 37\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "\n",
            "Verifica file: 'entita_italia.txt' esiste e ha dimensione 7612 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy e il riconoscitore italiano.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati del riconoscitore italiano.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi la sezione del riconoscitore italiano ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "        entities,\n",
        "        stats,\n",
        "        \"entita.txt\",\n",
        "        spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "        italian_recognizer=linker.italian_recognizer,\n",
        "        original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "99b2c7037a41432b9c4ca5082c3d7a04",
            "d3086d26b5924bbdae57674d26283897",
            "4c900e21b11b411ab7c455f04e1b85dc",
            "7a5cdc20c07e4816a3a0fd19a61e804b",
            "56047f68127946569aada5452404e4e1",
            "d2d6713ad9e34197b91a68f69132c349",
            "79392c1fd4734882828b6c5da77dd33f",
            "15364e7e9ef242e9b8951c76da22f619",
            "3571070609e241efb626541028b1e09c",
            "ef78ffb6569f44be8b40aed193d3ac2b",
            "46d4463a40ab48749959d0a2f8f0c044",
            "befca2b67b274a72a2f5213ae964fb98",
            "fb8d2a11d3a84dd79a4f3f308aa465d4",
            "857f3bcab111422f9e30442ba135460d",
            "acdc3319110d496ab68ac4b0b9e0d638",
            "727d64886002405a923d4407af0c4209",
            "43fef5bc8a9b4d1abda35a54026e44bb",
            "06315793acb0496b9fe3946783d10b04",
            "d6f61c84b8604421a831513077391927",
            "32b257cc71514839b256ca7e3830f83f",
            "3607370f349249d6b8acfd001ff8b198",
            "909766e612dd453cbfa79cdf15fbb2be",
            "dc2fc4c8b2ab410eb95dfa58bd086335",
            "37eb79cbd8504854b848074be4a59921",
            "f84fab5f290045bbbada5b979cdc3cd5",
            "2016dd4b64694c78a4eb7bc816aed6ad",
            "6a293547cc7b47fcb6c0dab36c12c7c7",
            "a8fe12d941444d04b202ff08e772526f",
            "ef3b30f1c661461d9c63dc9561bcb9ef",
            "bb82a3d5031942ce84a5c714270036b5",
            "3793086f93704ad292403ead367c6073",
            "e80ef0d369f04de199b2352c26b61111",
            "987be07332014d6a8c1a27c07d5e7876"
          ]
        },
        "id": "lBJ_xLa86p0T",
        "outputId": "ad071bd6-4e48-4f49-9f7e-5cec5c5cd4f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "99b2c7037a41432b9c4ca5082c3d7a04"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/default.zip:   0%|          | …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "befca2b67b274a72a2f5213ae964fb98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc2fc4c8b2ab410eb95dfa58bd086335"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Trovate 38 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 38\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8575 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita_trovate.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Aggiungi questa parte nella funzione export_entities_to_txt\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "    if transformers_entities:\n",
        "        for entity in transformers_entities:\n",
        "            lines.append(f\"- '{entity['text']}' → Etichetta: {entity.get('label', 'N/A')}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "    else:\n",
        "        lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "            # Carica tokenizer e modello\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Crea pipeline NER\n",
        "            self.ner_pipeline = pipeline(\n",
        "                \"ner\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            self.is_available = True\n",
        "            print(f\"Modello Transformer NER caricato: {self.model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"dbmdz/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer'):\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile\")\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo da Transformer NER\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "        print(f\"DEBUG: Riconoscitore Transformer NER ha trovato {len(transformers_entities)} entità nel testo\")\n",
        "\n",
        "        # Stampa dettagli delle entità trovate\n",
        "        for entity in transformers_entities:\n",
        "            print(f\"DEBUG: Entità trovata - Testo: {entity['text']}, Etichetta: {entity['label']}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        # Stampa testi esistenti\n",
        "        print(f\"DEBUG: Testi esistenti: {existing_texts}\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            print(f\"DEBUG: Verifica entità - Testo: {entity_text}, Lowercase: {entity_lower}\")\n",
        "\n",
        "            # Rimuovi la condizione di validità temporaneamente\n",
        "            if entity_lower not in existing_texts:\n",
        "                print(f\"DEBUG: Entità unica dal riconoscitore Transformer NER: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                transformers_type = self._map_transformers_label_to_type(entity['label'])\n",
        "\n",
        "                transformers_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [transformers_type],\n",
        "                    'source': 'transformers_ner',\n",
        "                    'label': entity['label'],\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "                transformers_only.append(transformers_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"DEBUG: Entità Transformer NER uniche trovate: {len(transformers_only)}\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "56fa5b786c9547beae467638a6887b03",
            "f253de36031a487688b8c3394b8fecb3",
            "950a15c315e54f4a8e7d9f26ee9c72f7",
            "914580895a3a4d3c8bdc2216ed9737a3",
            "34aaf234ebba4e4e84d3e49c0f14789d",
            "cfb7edb43a3d4c22ae587cfc90eab650",
            "9e36db42b58b4b8fa1faf1a8310b83a1",
            "ea16871ee4a94f68aeab9203eea49451",
            "6875c961e6b142f7842ba4c5869e2d25",
            "ecee372f9da74648b0d002e985f28a72",
            "289ef29f74814dc1aac2908cd6759870",
            "e25e37f7743148909e366968eed138f8",
            "421fd2ada2214240810db5d0e431e451",
            "6233535aaeab483b8f7023b51457d184",
            "4b5b13c7cbab40429f32ddaed90de202",
            "54b605a2890f4be1b855797f7efdb598",
            "db9dddea903d430184b57088cae426b2",
            "998c3e99bb254ff4bb5087c66aee0b95",
            "68042e42b1e44e65a8627da0aec3f560",
            "c9a99e66755c4c11a74e253389dd7c9d",
            "4824d4d85ebb4b13a244df954788e965",
            "9560df99c2094f7cb63dd5975c69124f"
          ]
        },
        "id": "DrgAp6oKnT5f",
        "outputId": "38bb9cf6-d0fb-4d7b-9add-7f822420e787"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "56fa5b786c9547beae467638a6887b03"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e25e37f7743148909e366968eed138f8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Errore nel caricamento del modello Transformer NER: dbmdz/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Assicurati di avere installato transformers e torch.\n",
            "Prova a installare i modelli con:\n",
            "pip install transformers torch\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: False\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Riconoscitore Transformer NER non disponibile\n",
            "\n",
            "Trovate 38 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: False\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Riconoscitore Transformer NER non disponibile\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 38\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8688 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Lista di modelli italiani alternativi\n",
        "            alternative_models = [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"Davlan/bert-base-multilingual-cased-ner-hrl\"\n",
        "            ]\n",
        "\n",
        "            # Prova a caricare uno dei modelli\n",
        "            for model_name in alternative_models:\n",
        "                try:\n",
        "                    # Determina il device\n",
        "                    device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "                    # Carica tokenizer e modello\n",
        "                    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "                    model = AutoModelForTokenClassification.from_pretrained(model_name)\n",
        "\n",
        "                    # Crea pipeline NER\n",
        "                    self.ner_pipeline = pipeline(\n",
        "                        \"ner\",\n",
        "                        model=model,\n",
        "                        tokenizer=tokenizer,\n",
        "                        device=device\n",
        "                    )\n",
        "\n",
        "                    self.is_available = True\n",
        "                    self.model_name = model_name\n",
        "                    print(f\"Modello Transformer NER caricato: {model_name}\")\n",
        "                    return\n",
        "\n",
        "                except Exception as inner_e:\n",
        "                    print(f\"Tentativo fallito con modello {model_name}: {inner_e}\")\n",
        "\n",
        "            # Se nessun modello funziona\n",
        "            raise Exception(\"Nessun modello NER disponibile\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"dbmdz/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Debug del riconoscitore Transformer\n",
        "        print(f\"DEBUG: Stato riconoscitore Transformer - Disponibile: {hasattr(self, 'transformers_recognizer')}\")\n",
        "        if hasattr(self, 'transformers_recognizer'):\n",
        "            print(f\"DEBUG: Riconoscitore Transformer - Is Available: {self.transformers_recognizer.is_available}\")\n",
        "\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if hasattr(self, 'transformers_recognizer') and self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            print(\"DEBUG: Tentativo di trovare entità Transformer NER\")\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            print(f\"DEBUG: Entità Transformer NER trovate: {len(transformers_only_entities)}\")\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "        else:\n",
        "            print(\"DEBUG: Riconoscitore Transformer NER non disponibile\")\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo da Transformer NER\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "        print(f\"DEBUG: Riconoscitore Transformer NER ha trovato {len(transformers_entities)} entità nel testo\")\n",
        "\n",
        "        # Stampa dettagli delle entità trovate\n",
        "        for entity in transformers_entities:\n",
        "            print(f\"DEBUG: Entità trovata - Testo: {entity['text']}, Etichetta: {entity['label']}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        # Stampa testi esistenti\n",
        "        print(f\"DEBUG: Testi esistenti: {existing_texts}\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            print(f\"DEBUG: Verifica entità - Testo: {entity_text}, Lowercase: {entity_lower}\")\n",
        "\n",
        "            # Rimuovi la condizione di validità temporaneamente\n",
        "            if entity_lower not in existing_texts:\n",
        "                print(f\"DEBUG: Entità unica dal riconoscitore Transformer NER: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                transformers_type = self._map_transformers_label_to_type(entity['label'])\n",
        "\n",
        "                transformers_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [transformers_type],\n",
        "                    'source': 'transformers_ner',\n",
        "                    'label': entity['label'],\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "                transformers_only.append(transformers_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"DEBUG: Entità Transformer NER uniche trovate: {len(transformers_only)}\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "125a82f05f1d446a9acbc905b5920f2d",
            "376f7c94992a4c0d8c30132ff6b60582",
            "bfdd0cd7d31a493bbba82ee1c915c322",
            "cad65ef1c7e04c878a61fdd3738f72f9",
            "03858db06c134d53bb64d57ebf41061f",
            "f6a7275240914264a137e6fd494494af",
            "6f5f418350f24b1c93a8d40940e9631a",
            "45dd27ca7da64abba701bac781ecb2b2",
            "7292ec3cc67e478c87a78d50cd20210d",
            "c0e792397b884b94a60ae279896e4744",
            "b4665fa2a28041558db4578cd0ccacc6",
            "d7628c1a518e448cb3b6831288bc118a",
            "8350b5345a924735ba99c6ad08573790",
            "f4aee7509d4843438d5b1dc129c87ba1",
            "80cae6ab93784136bf089d82493e5dc4",
            "4eaeabcf6c174d728bb9ecb7cebfdc7e",
            "c713c7a97b4f49cf9f1ff5fd57a38c1a",
            "7bf31623893a4b5ba6858456abc1b43c",
            "895f8bbee858458b8911b61ae1a44f86",
            "38ab564917a24a22967b1ff42e6858a6",
            "8968f10d49b0492da351dfecff981720",
            "f90521b4fed84656becb8e3770be405c"
          ]
        },
        "id": "F_TV5LftrL_o",
        "outputId": "7a4d807f-fadd-4492-8f48-0b28049eb919"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "125a82f05f1d446a9acbc905b5920f2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d7628c1a518e448cb3b6831288bc118a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "Tentativo fallito con modello 5had3/bert-base-italian-cased-ner: 5had3/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Tentativo fallito con modello MilaNLProc/bert-italian-cased-ner: MilaNLProc/bert-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello Transformer NER caricato: Davlan/bert-base-multilingual-cased-ner-hrl\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Riconoscitore Transformer NER ha trovato 17 entità nel testo\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998433589935303\n",
            "DEBUG: Entità trovata - Testo: Roma, Etichetta: LOC, Punteggio: 0.9995030164718628\n",
            "DEBUG: Entità trovata - Testo: Vaticano, Etichetta: LOC, Punteggio: 0.6520248651504517\n",
            "DEBUG: Entità trovata - Testo: Firenze, Etichetta: LOC, Punteggio: 0.9997918009757996\n",
            "DEBUG: Entità trovata - Testo: Milano, Etichetta: LOC, Punteggio: 0.9998107552528381\n",
            "DEBUG: Entità trovata - Testo: Napoli, Etichetta: LOC, Punteggio: 0.9998220801353455\n",
            "DEBUG: Entità trovata - Testo: Sapienza, Etichetta: ORG, Punteggio: 0.9991324543952942\n",
            "DEBUG: Entità trovata - Testo: PolitecnicodiMilano, Etichetta: ORG, Punteggio: 0.9991716146469116\n",
            "DEBUG: Entità trovata - Testo: Europa, Etichetta: LOC, Punteggio: 0.9998190999031067\n",
            "DEBUG: Entità trovata - Testo: Ferrari, Etichetta: ORG, Punteggio: 0.5556326508522034\n",
            "DEBUG: Entità trovata - Testo: Lamborghini, Etichetta: ORG, Punteggio: 0.665400505065918\n",
            "DEBUG: Entità trovata - Testo: Mediterraneo, Etichetta: LOC, Punteggio: 0.9982662796974182\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998801946640015\n",
            "DEBUG: Entità trovata - Testo: DanteAlighieri, Etichetta: PER, Punteggio: 0.999840497970581\n",
            "DEBUG: Entità trovata - Testo: LeonardodaVinci, Etichetta: PER, Punteggio: 0.9998635053634644\n",
            "DEBUG: Entità trovata - Testo: GalileoGalilei, Etichetta: PER, Punteggio: 0.9998313188552856\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998799562454224\n",
            "DEBUG: Testi esistenti: {'mediterraneo', 'dante', 'galileo', 'galilei', 'italia', 'sapienza', 'europa', 'vaticano', 'pizza', 'firenze', 'politecnico', 'alighieri', 'galileo galilei', 'napoli', 'il mediterraneo', 'vinci', 'colosseo', 'dante alighieri', 'la ferrari', 'barolo', 'milano', 'la sapienza', 'leonardo', 'ferrari', 'roma', 'politecnico di milano', 'chianti', 'rinascimento', 'lamborghini', 'vesuvio', 'leonardo da vinci'}\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: Roma, Lowercase: roma\n",
            "DEBUG: Verifica entità - Testo: Vaticano, Lowercase: vaticano\n",
            "DEBUG: Verifica entità - Testo: Firenze, Lowercase: firenze\n",
            "DEBUG: Verifica entità - Testo: Milano, Lowercase: milano\n",
            "DEBUG: Verifica entità - Testo: Napoli, Lowercase: napoli\n",
            "DEBUG: Verifica entità - Testo: Sapienza, Lowercase: sapienza\n",
            "DEBUG: Verifica entità - Testo: PolitecnicodiMilano, Lowercase: politecnicodimilano\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: PolitecnicodiMilano (ORG)\n",
            "DEBUG: Verifica entità - Testo: Europa, Lowercase: europa\n",
            "DEBUG: Verifica entità - Testo: Ferrari, Lowercase: ferrari\n",
            "DEBUG: Verifica entità - Testo: Lamborghini, Lowercase: lamborghini\n",
            "DEBUG: Verifica entità - Testo: Mediterraneo, Lowercase: mediterraneo\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: DanteAlighieri, Lowercase: dantealighieri\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: DanteAlighieri (PER)\n",
            "DEBUG: Verifica entità - Testo: LeonardodaVinci, Lowercase: leonardodavinci\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: LeonardodaVinci (PER)\n",
            "DEBUG: Verifica entità - Testo: GalileoGalilei, Lowercase: galileogalilei\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: GalileoGalilei (PER)\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Entità Transformer NER uniche trovate: 4\n",
            "DEBUG: Entità Transformer NER trovate: 4\n",
            "\n",
            "Trovate 42 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "- 'PolitecnicodiMilano' → N/A\n",
            "  Tipo: Organization\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'DanteAlighieri' → N/A\n",
            "  Tipo: Person\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'LeonardodaVinci' → N/A\n",
            "  Tipo: Person\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "- 'GalileoGalilei' → N/A\n",
            "  Tipo: Person\n",
            "  Fonte: transformers_ner\n",
            "\n",
            "DEBUG: Stato riconoscitore Transformer - Disponibile: True\n",
            "DEBUG: Riconoscitore Transformer - Is Available: True\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "DEBUG: Tentativo di trovare entità Transformer NER\n",
            "DEBUG: Riconoscitore Transformer NER ha trovato 17 entità nel testo\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998433589935303\n",
            "DEBUG: Entità trovata - Testo: Roma, Etichetta: LOC, Punteggio: 0.9995030164718628\n",
            "DEBUG: Entità trovata - Testo: Vaticano, Etichetta: LOC, Punteggio: 0.6520248651504517\n",
            "DEBUG: Entità trovata - Testo: Firenze, Etichetta: LOC, Punteggio: 0.9997918009757996\n",
            "DEBUG: Entità trovata - Testo: Milano, Etichetta: LOC, Punteggio: 0.9998107552528381\n",
            "DEBUG: Entità trovata - Testo: Napoli, Etichetta: LOC, Punteggio: 0.9998220801353455\n",
            "DEBUG: Entità trovata - Testo: Sapienza, Etichetta: ORG, Punteggio: 0.9991324543952942\n",
            "DEBUG: Entità trovata - Testo: PolitecnicodiMilano, Etichetta: ORG, Punteggio: 0.9991716146469116\n",
            "DEBUG: Entità trovata - Testo: Europa, Etichetta: LOC, Punteggio: 0.9998190999031067\n",
            "DEBUG: Entità trovata - Testo: Ferrari, Etichetta: ORG, Punteggio: 0.5556326508522034\n",
            "DEBUG: Entità trovata - Testo: Lamborghini, Etichetta: ORG, Punteggio: 0.665400505065918\n",
            "DEBUG: Entità trovata - Testo: Mediterraneo, Etichetta: LOC, Punteggio: 0.9982662796974182\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998801946640015\n",
            "DEBUG: Entità trovata - Testo: DanteAlighieri, Etichetta: PER, Punteggio: 0.999840497970581\n",
            "DEBUG: Entità trovata - Testo: LeonardodaVinci, Etichetta: PER, Punteggio: 0.9998635053634644\n",
            "DEBUG: Entità trovata - Testo: GalileoGalilei, Etichetta: PER, Punteggio: 0.9998313188552856\n",
            "DEBUG: Entità trovata - Testo: Italia, Etichetta: LOC, Punteggio: 0.9998799562454224\n",
            "DEBUG: Testi esistenti: {'mediterraneo', 'dante', 'galileo', 'galilei', 'italia', 'sapienza', 'europa', 'vaticano', 'pizza', 'firenze', 'politecnico', 'alighieri', 'galileo galilei', 'napoli', 'il mediterraneo', 'vinci', 'colosseo', 'dante alighieri', 'la ferrari', 'barolo', 'milano', 'la sapienza', 'leonardo', 'ferrari', 'roma', 'politecnico di milano', 'chianti', 'rinascimento', 'lamborghini', 'vesuvio', 'leonardo da vinci'}\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: Roma, Lowercase: roma\n",
            "DEBUG: Verifica entità - Testo: Vaticano, Lowercase: vaticano\n",
            "DEBUG: Verifica entità - Testo: Firenze, Lowercase: firenze\n",
            "DEBUG: Verifica entità - Testo: Milano, Lowercase: milano\n",
            "DEBUG: Verifica entità - Testo: Napoli, Lowercase: napoli\n",
            "DEBUG: Verifica entità - Testo: Sapienza, Lowercase: sapienza\n",
            "DEBUG: Verifica entità - Testo: PolitecnicodiMilano, Lowercase: politecnicodimilano\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: PolitecnicodiMilano (ORG)\n",
            "DEBUG: Verifica entità - Testo: Europa, Lowercase: europa\n",
            "DEBUG: Verifica entità - Testo: Ferrari, Lowercase: ferrari\n",
            "DEBUG: Verifica entità - Testo: Lamborghini, Lowercase: lamborghini\n",
            "DEBUG: Verifica entità - Testo: Mediterraneo, Lowercase: mediterraneo\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Verifica entità - Testo: DanteAlighieri, Lowercase: dantealighieri\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: DanteAlighieri (PER)\n",
            "DEBUG: Verifica entità - Testo: LeonardodaVinci, Lowercase: leonardodavinci\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: LeonardodaVinci (PER)\n",
            "DEBUG: Verifica entità - Testo: GalileoGalilei, Lowercase: galileogalilei\n",
            "DEBUG: Entità unica dal riconoscitore Transformer NER: GalileoGalilei (PER)\n",
            "DEBUG: Verifica entità - Testo: Italia, Lowercase: italia\n",
            "DEBUG: Entità Transformer NER uniche trovate: 4\n",
            "DEBUG: Entità Transformer NER trovate: 4\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 42\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- transformers_only_entities: 4\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8731 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita_trovate.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Aggiungi questa parte nella funzione export_entities_to_txt\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "    if transformers_entities:\n",
        "        for entity in transformers_entities:\n",
        "            lines.append(f\"- '{entity['text']}' → Etichetta: {entity.get('label', 'N/A')}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "    else:\n",
        "        lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "            # Carica tokenizer e modello\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Crea pipeline NER\n",
        "            self.ner_pipeline = pipeline(\n",
        "                \"ner\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            self.is_available = True\n",
        "            print(f\"Modello Transformer NER caricato: {self.model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"dbmdz/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo da Transformer NER\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore Transformer NER ha trovato {len(transformers_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore Transformer NER: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                transformers_type = self._map_transformers_label_to_type(entity['label'])\n",
        "\n",
        "                transformers_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [transformers_type],\n",
        "                    'source': 'transformers_ner',\n",
        "                    'label': entity['label'],\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "                transformers_only.append(transformers_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(transformers_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "33c27647f7e74193a37d241be940571b",
            "462f06ca806e4365a30e2fcb079cf026",
            "ff5adce707d34e948e8aec223b80c23b",
            "7cb541b4f6024f2e966a8e37a5cd093c",
            "538ccc648a2f40228e1e588a6249fd87",
            "c6e6b85d44d940848c9d95cc24f814dd",
            "660b6030ddfa4acd806517b4a8ba2da9",
            "c3c37a4607044b4998041e6a55cbba1c",
            "ac5636907393468d90ae5ca40c9b67cb",
            "6f925dc41b3641a5ad94244732dad5cc",
            "43a347010fc745c7a13638d4a53fbf95",
            "9dd45e1efb824a9fa64eed0a2cb28683",
            "734d841bcfe944bfa28d479df00b624b",
            "3269d9ef5be0400da3d8f232c799c7b6",
            "3c5ba092d043479d843c97314af67e4c",
            "effb0092e34941188768b5a8765ec472",
            "584be2e248a94313b814df876b1e32e1",
            "c3a15d887b38402c8a84769b0d9663cc",
            "3508ede777ae4e11a83ff6e80415a3fd",
            "9ad5e73b7cef402e96a15f5b36bb4c51",
            "9bb827146b8649719ca3a0961e997ced",
            "d026a7d56aac4d549e8c900b8e789975"
          ]
        },
        "id": "VR0ujPN4A2O2",
        "outputId": "b56655d3-d51c-44b1-a19f-166e629706b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello spaCy it_core_news_lg caricato con successo.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "33c27647f7e74193a37d241be940571b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: it (Italian) ...\n",
            "INFO:stanza:File exists: /root/stanza_resources/it/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9dd45e1efb824a9fa64eed0a2cb28683"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: it (Italian):\n",
            "=================================\n",
            "| Processor | Package           |\n",
            "---------------------------------\n",
            "| tokenize  | combined          |\n",
            "| mwt       | combined          |\n",
            "| pos       | combined_charlm   |\n",
            "| lemma     | combined_nocharlm |\n",
            "| ner       | fbk               |\n",
            "=================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Riconoscitore NLP italiano inizializzato correttamente con Stanza.\n",
            "Errore nel caricamento del modello Transformer NER: dbmdz/bert-base-italian-cased-ner is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
            "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
            "Assicurati di avere installato transformers e torch.\n",
            "Prova a installare i modelli con:\n",
            "pip install transformers torch\n",
            "Analisi del testo:\n",
            "L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico. Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento. Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio. Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export. Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa. Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo. La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo. Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia. Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana. Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Trovate 38 entità:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "  Tipo: paese, stato sovrano, stato sociale, Stato unitario, repubblica, paese mediterraneo, grande potenza\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q38\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "  Tipo: città di confine, comune italiano soppresso, destinazione turistica, metropoli, città più grande, città universitaria, grande città, comune italiano, capitale di Stato\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q220\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "  Tipo: anfiteatro romano, sito archeologico, attrazione turistica, stadio, manufatto archeologico museo, museo nazionale italiano, edificio civile storico museo, museo del Ministero della Cultura italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q10285\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "  Tipo: stato sovrano, città-Stato, enclave, Paese senza affaccio al mare, attrazione turistica, paese mediterraneo, paese, complesso religioso, complesso istituzionale, area urbana, destinazione turistica, Stato confessionale, Q7396640\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q237\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "  Tipo: comune italiano, grande città, capitale o capoluogo, Città-stato italiane\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2044\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "  Tipo: movimento artistico, movimento culturale\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4692\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "  Tipo: città, capoluogo, capoluogo, capoluogo, Città-stato italiane, grande città, metropoli, comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q490\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "  Tipo: città, comune italiano, grande città\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2634\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "  Tipo: vulcano attivo, stratovulcano, attrazione turistica, montagna\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q524\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "  Tipo: politecnico, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q392904\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "  Tipo: area continentale e isole limitrofe, continente, regione geografica\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q46\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "  Tipo: vino rosso, vino da tavola\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q750979\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q18356\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "  Tipo: scuderia di Formula 1\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q169898\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: casa automobilistica, impresa, società controllata\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q35886\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "  Tipo: mare interno, mare mediterraneo, bacino idrografico\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q4918\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1067\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "  Tipo: singolo\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q16570172\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "  Tipo: quotidiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3792796\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "  Tipo: università statale, organizzazione\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q209344\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "  Tipo: \n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1371037\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "  Tipo: famiglia nobile italiana\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q3611786\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q762\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "  Tipo: comune italiano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q82884\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "  Tipo: umano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q307\n",
            "  Fonte: wikidata+spacy\n",
            "\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q1151817\n",
            "  Fonte: local\n",
            "\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q26378\n",
            "  Fonte: local\n",
            "\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "  Tipo: University\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q213485\n",
            "  Fonte: local\n",
            "\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "  Tipo: Volcano\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q2292\n",
            "  Fonte: local\n",
            "\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "  Tipo: Company\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q8402\n",
            "  Fonte: local\n",
            "\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q662785\n",
            "  Fonte: local\n",
            "\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "  Tipo: Wine\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q812394\n",
            "  Fonte: local\n",
            "\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "  Tipo: Food\n",
            "  Wikidata: https://www.wikidata.org/wiki/Q177\n",
            "  Fonte: local\n",
            "\n",
            "- 'Dante' → N/A\n",
            "  Tipo: ProperNoun\n",
            "  Fonte: italian_nlp\n",
            "\n",
            "spaCy ha trovato 22 entità nel testo\n",
            "Di cui 0 sono uniche (non trovate da Wikidata o dal dizionario locale)\n",
            "Riconoscitore italiano ha trovato 27 entità nel testo\n",
            "Entità unica dal riconoscitore italiano: Dante (PROPN)\n",
            "Di cui 1 sono uniche (non trovate da altre fonti)\n",
            "\n",
            "Verifica URI:\n",
            "- La parola 'Italia' → URI: http://dbpedia.org/resource/Italia esiste\n",
            "- La parola 'Roma' → URI: http://dbpedia.org/resource/Roma esiste\n",
            "- La parola 'Colosseo' → URI: http://dbpedia.org/resource/Colosseo esiste\n",
            "- La parola 'Vaticano' → URI: http://dbpedia.org/resource/Città_del_Vaticano esiste\n",
            "- La parola 'Firenze' → URI: http://dbpedia.org/resource/Firenze esiste\n",
            "- La parola 'Rinascimento' → URI: http://dbpedia.org/resource/Rinascimento esiste\n",
            "- La parola 'Milano' → URI: http://dbpedia.org/resource/Milano esiste\n",
            "- La parola 'Napoli' → URI: http://dbpedia.org/resource/Napoli esiste\n",
            "- La parola 'Vesuvio' → URI: http://dbpedia.org/resource/Mount_Vesuvius esiste\n",
            "- La parola 'la Sapienza' → URI: http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\" esiste\n",
            "- La parola 'Politecnico di Milano' → URI: http://dbpedia.org/resource/Polytechnic_University_of_Milan esiste\n",
            "- La parola 'Europa' → URI: http://dbpedia.org/resource/Europa esiste\n",
            "- La parola 'Chianti' → URI: http://dbpedia.org/resource/Chianti esiste\n",
            "- La parola 'Barolo' → URI: http://dbpedia.org/resource/Barolo esiste\n",
            "- La parola 'Ferrari' → URI: http://dbpedia.org/resource/Ferrari esiste\n",
            "- La parola 'Lamborghini' → URI: http://dbpedia.org/resource/Lamborghini esiste\n",
            "- La parola 'Mediterraneo' → URI: http://dbpedia.org/resource/Mar_Mediterraneo esiste\n",
            "- La parola 'Dante Alighieri' → URI: http://dbpedia.org/resource/Dante_Alighieri esiste\n",
            "- La parola 'Leonardo da Vinci' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Galileo Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'La Ferrari' → URI: http://dbpedia.org/resource/La_Ferrari_è_lei esiste\n",
            "- La parola 'Il Mediterraneo' → URI: http://dbpedia.org/resource/Il_Mediterraneo esiste\n",
            "- La parola 'Sapienza' → URI: http://dbpedia.org/resource/Sapienza_University_of_Rome esiste\n",
            "- La parola 'Politecnico' → URI: http://dbpedia.org/resource/Institute_of_technology esiste\n",
            "- La parola 'Alighieri' → URI: http://dbpedia.org/resource/Alighieri esiste\n",
            "- La parola 'Leonardo' → URI: http://dbpedia.org/resource/Leonardo_da_Vinci esiste\n",
            "- La parola 'Vinci' → URI: http://dbpedia.org/resource/Vinci esiste\n",
            "- La parola 'Galileo' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'Galilei' → URI: http://dbpedia.org/resource/Galileo_Galilei esiste\n",
            "- La parola 'pizza' → URI: http://dbpedia.org/resource/Pizza esiste\n",
            "\n",
            "Entità con URI DBpedia:\n",
            "- 'Italia' → http://dbpedia.org/resource/Italia\n",
            "- 'Roma' → http://dbpedia.org/resource/Roma\n",
            "- 'Colosseo' → http://dbpedia.org/resource/Colosseo\n",
            "- 'Vaticano' → http://dbpedia.org/resource/Città_del_Vaticano\n",
            "- 'Firenze' → http://dbpedia.org/resource/Firenze\n",
            "- 'Rinascimento' → http://dbpedia.org/resource/Rinascimento\n",
            "- 'Milano' → http://dbpedia.org/resource/Milano\n",
            "- 'Napoli' → http://dbpedia.org/resource/Napoli\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Vesuvio\n",
            "- 'la Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Politecnico_di_Milano\n",
            "- 'Europa' → http://dbpedia.org/resource/Europa\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti_(vino)\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo_(Italia)\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Scuderia_Ferrari\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Mediterraneo' → http://dbpedia.org/resource/Mar_Mediterraneo\n",
            "- 'Dante Alighieri' → http://dbpedia.org/resource/Dante_Alighieri\n",
            "- 'Leonardo da Vinci' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Galileo Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'La Ferrari' → http://dbpedia.org/resource/La_Ferrari_è_lei\n",
            "- 'Il Mediterraneo' → http://dbpedia.org/resource/Il_Mediterraneo\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Università_degli_Studi_di_Roma_\"La_Sapienza\"\n",
            "- 'Politecnico' → http://dbpedia.org/resource/Institute_of_technology\n",
            "- 'Alighieri' → http://dbpedia.org/resource/Alighieri\n",
            "- 'Leonardo' → http://dbpedia.org/resource/Leonardo_da_Vinci\n",
            "- 'Vinci' → http://dbpedia.org/resource/Vinci\n",
            "- 'Galileo' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Galilei' → http://dbpedia.org/resource/Galileo_Galilei\n",
            "- 'Politecnico di Milano' → http://dbpedia.org/resource/Polytechnic_University_of_Milan\n",
            "- 'Lamborghini' → http://dbpedia.org/resource/Lamborghini\n",
            "- 'Sapienza' → http://dbpedia.org/resource/Sapienza_University_of_Rome\n",
            "- 'Vesuvio' → http://dbpedia.org/resource/Mount_Vesuvius\n",
            "- 'Ferrari' → http://dbpedia.org/resource/Ferrari\n",
            "- 'Chianti' → http://dbpedia.org/resource/Chianti\n",
            "- 'Barolo' → http://dbpedia.org/resource/Barolo\n",
            "- 'pizza' → http://dbpedia.org/resource/Pizza\n",
            "File salvato in: /content/entita.txt\n",
            "\n",
            "File di output salvato in: /content/entita.txt\n",
            "\n",
            "Statistiche:\n",
            "- total_entities: 38\n",
            "- wikidata_entities: 37\n",
            "- spacy_only_entities: 0\n",
            "- local_entities: 8\n",
            "- entities_with_dbpedia: 37\n",
            "- entities_with_types: 36\n",
            "- italian_only_entities: 1\n",
            "\n",
            "Verifica file: 'entita.txt' esiste e ha dimensione 8688 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Verifica caricamento del modello\n",
        "recognizer = TransformersNERRecognizer(language=\"it\", use_gpu=False)\n",
        "\n",
        "# Testo di test più esplicito\n",
        "test_text = \"Mario Rossi lavora per la Banca d'Italia a Roma e ha fondato una startup tecnologica a Milano.\"\n",
        "entities = recognizer.recognize_entities(test_text)\n",
        "\n",
        "print(\"Entità trovate:\")\n",
        "for entity in entities:\n",
        "    print(f\"- {entity['text']} (Tipo: {entity['label']}, Punteggio: {entity['score']})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365,
          "referenced_widgets": [
            "c90dde9098534fea853d095ccb84eb19",
            "0c1dc55bdcdf4e41b5d013902b8e9d39",
            "403c1e63a4ea437f99d9ec89f11cc0a3",
            "d96bf999477e4ef9b8bc234b98145c8a",
            "a5433829f47442fd94c5684af30193aa",
            "eae40eb33c5a470c88b258d55b1c813f",
            "5a332f3c39294dd1a6e630e98801cc05",
            "f6ab92969e164615b1ee83b07e712ab6",
            "698382eaa87749a6bdebfdc5b1b4750b",
            "fcbbba22dd4f488fa17c3833095487ef",
            "183fd4438f53425fb48e09b774b659f6",
            "4f3a66cfa9284ebaaa9b725a3c441481",
            "76f6bff5db694fc199a9cac79e9ee059",
            "0adcacf5046248e2a2755ccfae8a81dd",
            "329e85b08c974eab8489919c4f19f779",
            "a3e64a9a84374c6aa882c3d392787de4",
            "ece0841d4413440882c3f9ade6c96aa5",
            "a411a57a8c664ab292ba61db4b1c8b97",
            "6f1f94c3f06249a5a5ddba8612a4540f",
            "e62bc58dc5314d8f8ce3d281414f5ef6",
            "29961dda9b2340c2910dae9a7362ef95",
            "45f7c233516c44ed92fdf1b827824b5f",
            "52edbf406d994662a0f9b2d502c50e05",
            "25fb7569935d45c492b1e40f42beb924",
            "4499685d5da4456dba01c7ba715fb2fd",
            "9a2ae8819e1847ad86ca1e2b6854fa63",
            "ac7c72e2e80547dc983b8b525959001f",
            "ff9cdc3ea5324b0fb6ea24ac464918af",
            "357d96cf6a8b49d6835b3c3561405289",
            "3a48d2f8041b4548bd144c53d5388615",
            "e50f2ccb731f49079f1e5686f044b514",
            "2355602a97024827b88855f72f8b0999",
            "f9073c358088495c8421f0052e52445f",
            "ba1b64a2f4214ebdb9fd8e6656f20c95",
            "5d7b2bc39f014c9c871bddbd5270a21a",
            "223bf4622ffa492c985d665cf36cc0dd",
            "0260d2c7f71149bd88c35a938eda887d",
            "85ff0058b3f842608d587c28b68662c8",
            "5bff531258344601927d91bca35dd246",
            "351680af59cd420da6f79816f68b80e4",
            "99173a3ae2024f5982ac860d30c9402d",
            "a312de900c68435d9a6e2e6ffc7c4936",
            "9fbf602a21744e188a063083a75ba833",
            "130db07b81e64547b13fb78641f1fb31",
            "7a198d9f0cac418389ef3405d35a3fb1",
            "92d5f9952d9b4688ab646d0f630c67f6",
            "7ec6c714d5834b5f990bbd8d561e1943",
            "a8cb44ae119c44be807fc3acac3921dd",
            "5adae7dcf5d54bdb97edd4e549deb2f9",
            "e9484de7528449058d309e1e0def2999",
            "157039af7a8d44299f97f27dd8ccd7f1",
            "0d92e96f491b442194aa70e8a6b4ccee",
            "664d04ccfef8460984418f04e71a9007",
            "c17b7744158a448896aad8812b327bda",
            "52931570e243479896f8f791693f9bf8",
            "dda7a5a2a3c04dc4aed55cc9bc7bd5db",
            "19f96645145c46de863d24f62802ee89",
            "45c4627054544a7abe82a232dbbdb270",
            "cb5596c133094bbb9518ba4754fd5844",
            "293b590c9d10454f89c619c9ea7a032d",
            "ef39645116f2433dafbdc067f58cb9e2",
            "2a4498b52be64694b221a361a7a88b8d",
            "4bff3ae090b442f68932a09076df7a66",
            "533fce5ea7324727ade621adb06f4855",
            "6e590f0043064a06b7e1bda995b6336d",
            "6111b1be06644e06b1e798e23e804a9d"
          ]
        },
        "id": "fO36zAXeNZae",
        "outputId": "76be2b16-8ca1-4a60-c7d7-2c677f5e8bc9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/59.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c90dde9098534fea853d095ccb84eb19"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/829 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4f3a66cfa9284ebaaa9b725a3c441481"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/213k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "52edbf406d994662a0f9b2d502c50e05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "added_tokens.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba1b64a2f4214ebdb9fd8e6656f20c95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7a198d9f0cac418389ef3405d35a3fb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/433M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dda7a5a2a3c04dc4aed55cc9bc7bd5db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
            "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modello Transformer NER caricato: dslim/bert-base-NER\n",
            "Entità trovate:\n",
            "- MarioRossi (Tipo: PER, Punteggio: 0.9997310042381287)\n",
            "- Roma (Tipo: LOC, Punteggio: 0.9841041564941406)\n",
            "- Milano (Tipo: LOC, Punteggio: 0.9881179928779602)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "import re\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set\n",
        "import os\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
        "from typing import List, Dict, Any, Optional, Tuple, Set # Added missing Any type hint\n",
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import torch\n",
        "\n",
        "\n",
        "# Importazione di spaCy\n",
        "import spacy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per assicurare la corretta inclusione delle entità spaCy\n",
        "\n",
        "# Modifica alla funzione export_entities_to_txt per includere le entità del riconoscitore italiano\n",
        "#def export_entities_to_txt(entities, stats, output_file=\"entita_trovate.txt\", spacy_recognizer=None, italian_recognizer=None, original_text=None):\n",
        "\n",
        "def export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    output_file=\"entita_trovate.txt\",\n",
        "    spacy_recognizer=None,\n",
        "    italian_recognizer=None,\n",
        "    transformers_recognizer=None,  # Nuovo parametro\n",
        "    original_text=None\n",
        "    ):\n",
        "    # ... resto del codice invariato ...\n",
        "    \"\"\"\n",
        "    Esporta le entità trovate in un file di testo semplice con le categorie richieste.\n",
        "    Include anche le entità riconosciute da spaCy e dal riconoscitore italiano.\n",
        "\n",
        "    Args:\n",
        "        entities: Lista di entità trovate\n",
        "        stats: Statistiche sulle entità\n",
        "        output_file: Nome del file di output\n",
        "        spacy_recognizer: Riconoscitore spaCy (opzionale)\n",
        "        italian_recognizer: Riconoscitore italiano (opzionale)\n",
        "        original_text: Testo originale da analizzare (opzionale)\n",
        "\n",
        "    Returns:\n",
        "        Percorso completo del file creato\n",
        "    \"\"\"\n",
        "    import os\n",
        "\n",
        "    # Filtra le entità per categoria\n",
        "    wikidata_entities = [e for e in entities if e.get('source') in ['wikidata', 'wikidata+spacy']]\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    entities_with_types = [e for e in entities if 'types' in e and e['types']]\n",
        "    spacy_entities = [e for e in entities if e.get('source') == 'spacy']\n",
        "    italian_entities = [e for e in entities if e.get('source') in ['italian_nlp', 'stanza', 'stanza_pos', 'geo_rule']]\n",
        "#    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "\n",
        "    # Prepara i contenuti\n",
        "    lines = []\n",
        "\n",
        "    # Intestazione\n",
        "    lines.append(\"ELENCO ENTITÀ TROVATE NEL TESTO\")\n",
        "    lines.append(\"============================\\n\")\n",
        "\n",
        "    # Entità Wikidata\n",
        "    lines.append(\"WIKIDATA ENTITIES:\")\n",
        "    lines.append(\"------------------\")\n",
        "    for entity in wikidata_entities:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity.get('wikidata_id', 'N/A')} (URL: {entity.get('wikidata_url', 'N/A')})\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con URI DBpedia\n",
        "    lines.append(\"ENTITIES WITH DBPEDIA:\")\n",
        "    lines.append(\"----------------------\")\n",
        "    for entity in entities_with_dbpedia:\n",
        "        lines.append(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Entità con tipi definiti\n",
        "    lines.append(\"ENTITIES WITH TYPES:\")\n",
        "    lines.append(\"-------------------\")\n",
        "    for entity in entities_with_types:\n",
        "        types_str = \", \".join(entity.get('types', []))\n",
        "        lines.append(f\"- '{entity['text']}' → Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità spaCy\n",
        "    lines.append(\"SPACY ENTITIES:\")\n",
        "    lines.append(\"--------------\")\n",
        "\n",
        "    # Aggiungi questa parte nella funzione export_entities_to_txt\n",
        "    # Sezione entità del riconoscitore Transformer NER\n",
        "    transformers_entities = [e for e in entities if e.get('source') == 'transformers_ner']\n",
        "\n",
        "    lines.append(\"TRANSFORMER NER ENTITIES:\")\n",
        "    lines.append(\"-------------------------\")\n",
        "    if transformers_entities:\n",
        "        for entity in transformers_entities:\n",
        "            lines.append(f\"- '{entity['text']}' → Etichetta: {entity.get('label', 'N/A')}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "    else:\n",
        "        lines.append(\"Nessuna entità rilevata dal riconoscitore Transformer NER.\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore spaCy e un testo originale, ottieni tutte le entità spaCy\n",
        "    if spacy_recognizer and original_text:\n",
        "        try:\n",
        "            spacy_all_entities = spacy_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not spacy_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata da spaCy nel testo.\")\n",
        "            else:\n",
        "                for entity in spacy_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi spaCy: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità spaCy già filtrate\n",
        "        if not spacy_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità spaCy, è necessario passare il riconoscitore spaCy e il testo originale.\")\n",
        "        else:\n",
        "            for entity in spacy_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "    lines.append(\"\")\n",
        "\n",
        "    # Sezione entità del riconoscitore italiano\n",
        "    lines.append(\"ITALIAN NLP ENTITIES:\")\n",
        "    lines.append(\"--------------------\")\n",
        "\n",
        "    # Se abbiamo un riconoscitore italiano e un testo originale, ottieni tutte le entità\n",
        "    if italian_recognizer and original_text:\n",
        "        try:\n",
        "            italian_all_entities = italian_recognizer.recognize_entities(original_text)\n",
        "\n",
        "            if not italian_all_entities:\n",
        "                lines.append(\"Nessuna entità rilevata dal riconoscitore italiano nel testo.\")\n",
        "            else:\n",
        "                for entity in italian_all_entities:\n",
        "                    label = entity.get('label', '')\n",
        "                    lines.append(f\"- '{entity['text']}' → Etichetta: {label}\")\n",
        "        except Exception as e:\n",
        "            lines.append(f\"Errore nell'analisi del riconoscitore italiano: {e}\")\n",
        "    else:\n",
        "        # Fallback alle entità italiane già filtrate\n",
        "        if not italian_entities:\n",
        "            lines.append(\"Per mostrare tutte le entità del riconoscitore italiano, è necessario passare il riconoscitore e il testo originale.\")\n",
        "        else:\n",
        "            for entity in italian_entities:\n",
        "                label = entity.get('label', '')\n",
        "                types_str = \", \".join(entity.get('types', []))\n",
        "                lines.append(f\"- '{entity['text']}' → Etichetta: {label}, Tipi: {types_str}\")\n",
        "\n",
        "    # Scrivi il file\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        f.write('\\n'.join(lines))\n",
        "\n",
        "    # Ottieni e restituisci il percorso completo\n",
        "    output_path = os.path.abspath(output_file)\n",
        "    print(f\"File salvato in: {output_path}\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "\n",
        "class SpacyRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità basato su spaCy.\n",
        "    Riconosce entità in testo italiano e altre lingue supportate.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua del testo da analizzare (it, en, ecc.)\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.model = None\n",
        "        self.is_available = False\n",
        "\n",
        "        # Mappa codici lingua a modelli spaCy\n",
        "        self.language_models = {\n",
        "            \"it\": \"it_core_news_lg\",\n",
        "            \"en\": \"en_core_web_lg\",\n",
        "            \"fr\": \"fr_core_news_lg\",\n",
        "            \"de\": \"de_core_news_lg\",\n",
        "            \"es\": \"es_core_news_lg\"\n",
        "        }\n",
        "\n",
        "        # Inizializza il modello appropriato\n",
        "        self._load_model()\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"Carica il modello spaCy per la lingua specificata.\"\"\"\n",
        "        try:\n",
        "            # Ottieni il nome del modello per la lingua specificata\n",
        "            model_name = self.language_models.get(self.language)\n",
        "\n",
        "            if not model_name:\n",
        "                print(f\"Lingua {self.language} non supportata da spaCy. Utilizzo del modello inglese.\")\n",
        "                model_name = \"en_core_web_sm\"\n",
        "\n",
        "            # Carica il modello\n",
        "            self.model = spacy.load(model_name)\n",
        "            self.is_available = True\n",
        "            print(f\"Modello spaCy {model_name} caricato con successo.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello spaCy: {e}\")\n",
        "            print(\"Assicurati di aver installato spaCy e i modelli linguistici con:\")\n",
        "            print(f\"python -m spacy download {self.language_models.get(self.language, 'en_core_web_sm')}\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.model:\n",
        "            print(\"SpacyRecognizer non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con spaCy\n",
        "            doc = self.model(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for ent in doc.ents:\n",
        "                entity = {\n",
        "                    'text': ent.text,\n",
        "                    'label': ent.label_,\n",
        "                    'start_char': ent.start_char,\n",
        "                    'end_char': ent.end_char,\n",
        "                    'source': 'spacy'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con spaCy: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "class ItalianNLPRecognizer:\n",
        "    \"\"\"\n",
        "    Riconoscitore di entità ottimizzato per l'italiano in ambiente Colab.\n",
        "    Usa una combinazione di Stanza (versione Python di Stanford CoreNLP)\n",
        "    e altre librerie per l'italiano.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        self.use_gpu = use_gpu\n",
        "        self.is_available = False\n",
        "        self.stanza_nlp = None\n",
        "        self.initialized = False\n",
        "\n",
        "        # Inizializza le risorse\n",
        "        self._initialize_resources()\n",
        "\n",
        "    def _initialize_resources(self):\n",
        "        \"\"\"Inizializza Stanza e altre risorse per l'italiano.\"\"\"\n",
        "        try:\n",
        "            import stanza\n",
        "            import simplemma\n",
        "            import nltk\n",
        "            from nltk.tokenize import word_tokenize\n",
        "\n",
        "            # Scarica i modelli di Stanza per l'italiano\n",
        "            stanza.download('it')\n",
        "\n",
        "            # Inizializza il pipeline Stanza per l'italiano\n",
        "            self.stanza_nlp = stanza.Pipeline(\n",
        "                lang='it',\n",
        "                processors='tokenize,mwt,pos,lemma,ner',\n",
        "                use_gpu=self.use_gpu\n",
        "            )\n",
        "\n",
        "            # Scarica le risorse NLTK necessarie\n",
        "            nltk.download('punkt')\n",
        "            nltk.download('stopwords')\n",
        "            nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "            self.is_available = True\n",
        "            self.initialized = True\n",
        "            print(\"Riconoscitore NLP italiano inizializzato correttamente con Stanza.\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'inizializzazione del riconoscitore italiano: {e}\")\n",
        "            print(\"Assicurati di aver eseguito le installazioni necessarie in Colab.\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo italiano utilizzando Stanza.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.stanza_nlp:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Analizza il testo con Stanza\n",
        "            doc = self.stanza_nlp(text)\n",
        "\n",
        "            # Estrai le entità\n",
        "            for sent in doc.sentences:\n",
        "                for ent in sent.ents:\n",
        "                    entity = {\n",
        "                        'text': ent.text,\n",
        "                        'label': ent.type,\n",
        "                        'start_char': -1,  # Stanza non fornisce direttamente le posizioni dei caratteri\n",
        "                        'end_char': -1,\n",
        "                        'source': 'stanza'\n",
        "                    }\n",
        "                    entities.append(entity)\n",
        "\n",
        "                # Aggiungi anche i nomi propri (POS tag == PROPN)\n",
        "                for token in sent.tokens:\n",
        "                    for word in token.words:\n",
        "                        if word.upos == 'PROPN' and not any(e['text'] == word.text for e in entities):\n",
        "                            entity = {\n",
        "                                'text': word.text,\n",
        "                                'label': 'PROPN',\n",
        "                                'start_char': -1,\n",
        "                                'end_char': -1,\n",
        "                                'source': 'stanza_pos'\n",
        "                            }\n",
        "                            entities.append(entity)\n",
        "\n",
        "            # Cerca anche entità geografiche italiane specifiche usando regole\n",
        "            entities.extend(self._extract_italian_geo_entities(text))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Stanza: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def _extract_italian_geo_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Estrae entità geografiche italiane usando regole.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità geografiche\n",
        "        \"\"\"\n",
        "        import re\n",
        "        entities = []\n",
        "\n",
        "        # Regioni italiane\n",
        "        regions = [\n",
        "            \"Abruzzo\", \"Basilicata\", \"Calabria\", \"Campania\", \"Emilia-Romagna\",\n",
        "            \"Friuli Venezia Giulia\", \"Lazio\", \"Liguria\", \"Lombardia\", \"Marche\",\n",
        "            \"Molise\", \"Piemonte\", \"Puglia\", \"Sardegna\", \"Sicilia\", \"Toscana\",\n",
        "            \"Trentino-Alto Adige\", \"Umbria\", \"Valle d'Aosta\", \"Veneto\"\n",
        "        ]\n",
        "\n",
        "        # Pattern per laghi, montagne, fiumi italiani\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Lago|Monte|Fiume|Isola|Golfo|Capo|Valle|Parco Nazionale|Mar) [A-Z][a-zàèéìòù]+\\b',\n",
        "            r'\\b(?:Alpi|Appennini|Dolomiti|Maremma|Laguna|Costa|Riviera) [A-Z][a-zàèéìòù]*\\b'\n",
        "        ]\n",
        "\n",
        "        # Cerca regioni\n",
        "        for region in regions:\n",
        "            if re.search(r'\\b' + re.escape(region) + r'\\b', text):\n",
        "                entity = {\n",
        "                    'text': region,\n",
        "                    'label': 'GPE',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        # Cerca pattern geografici\n",
        "        for pattern in geo_patterns:\n",
        "            matches = re.finditer(pattern, text)\n",
        "            for match in matches:\n",
        "                entity = {\n",
        "                    'text': match.group(0),\n",
        "                    'label': 'LOC',\n",
        "                    'start_char': -1,\n",
        "                    'end_char': -1,\n",
        "                    'source': 'geo_rule'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "\n",
        "# Alternative per modelli italiani NER\n",
        "model_options = [\n",
        "    \"5had3/bert-base-italian-cased-ner\",  # Un'alternativa affidabile\n",
        "    \"MilaNLProc/bert-italian-cased-ner\",  # Altro modello italiano per NER\n",
        "    \"dslim/bert-base-NER\"  # Modello generico che funziona bene\n",
        "]\n",
        "\n",
        "\n",
        "class TransformersNERRecognizer:\n",
        "    def __init__(self,\n",
        "                 model_name: str = None,\n",
        "                 language: str = \"it\",\n",
        "                 use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il riconoscitore di entità con un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            model_name: Nome del modello pre-addestrato per NER\n",
        "            language: Lingua del modello (default: italiano)\n",
        "            use_gpu: Se utilizzare l'accelerazione GPU\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.use_gpu = use_gpu\n",
        "\n",
        "        # Dizionario di mapping per modelli in diverse lingue\n",
        "        self.language_models = {\n",
        "            \"it\": [\n",
        "                \"5had3/bert-base-italian-cased-ner\",\n",
        "                \"MilaNLProc/bert-italian-cased-ner\",\n",
        "                \"dslim/bert-base-NER\"\n",
        "            ],\n",
        "            \"en\": [\"dslim/bert-base-NER\"],\n",
        "            \"de\": [\"RafLorem/bert-base-german-NER\"],\n",
        "            \"fr\": [\"Jean-Baptiste/camembert-ner\"]\n",
        "        }\n",
        "\n",
        "        # Se non specificato, scegli un modello predefinito\n",
        "        if model_name is None:\n",
        "            model_name = self._select_best_model()\n",
        "\n",
        "        self.model_name = model_name\n",
        "        self.is_available = False\n",
        "        self.ner_pipeline = None\n",
        "\n",
        "        # Inizializza il modello\n",
        "        self._load_model()\n",
        "\n",
        "    def _select_best_model(self) -> str:\n",
        "        \"\"\"\n",
        "        Seleziona il miglior modello disponibile per la lingua.\n",
        "\n",
        "        Returns:\n",
        "            Nome del modello\n",
        "        \"\"\"\n",
        "        models = self.language_models.get(self.language, self.language_models[\"en\"])\n",
        "\n",
        "        for model in models:\n",
        "            try:\n",
        "                # Prova a caricare il modello\n",
        "                AutoTokenizer.from_pretrained(model)\n",
        "                return model\n",
        "            except Exception:\n",
        "                continue\n",
        "\n",
        "        # Fallback\n",
        "        return \"dslim/bert-base-NER\"\n",
        "\n",
        "    def _load_model(self):\n",
        "        \"\"\"\n",
        "        Carica il modello Transformer per il riconoscimento di entità.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Determina il device\n",
        "            device = 0 if self.use_gpu and torch.cuda.is_available() else -1\n",
        "\n",
        "            # Carica tokenizer e modello\n",
        "            tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "            model = AutoModelForTokenClassification.from_pretrained(self.model_name)\n",
        "\n",
        "            # Crea pipeline NER\n",
        "            self.ner_pipeline = pipeline(\n",
        "                \"ner\",\n",
        "                model=model,\n",
        "                tokenizer=tokenizer,\n",
        "                device=device\n",
        "            )\n",
        "\n",
        "            self.is_available = True\n",
        "            print(f\"Modello Transformer NER caricato: {self.model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel caricamento del modello Transformer NER: {e}\")\n",
        "            print(\"Assicurati di avere installato transformers e torch.\")\n",
        "            print(\"Prova a installare i modelli con:\")\n",
        "            print(\"pip install transformers torch\")\n",
        "            self.is_available = False\n",
        "\n",
        "    def recognize_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Riconosce entità in un testo utilizzando un modello Transformer.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute\n",
        "        \"\"\"\n",
        "        if not self.is_available or not self.ner_pipeline:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        try:\n",
        "            # Esegui NER\n",
        "            ner_results = self.ner_pipeline(text)\n",
        "\n",
        "            # Raggruppa entità consecutive con lo stesso tipo\n",
        "            grouped_entities = []\n",
        "            current_entity = None\n",
        "\n",
        "            for result in ner_results:\n",
        "                if result['entity'].startswith('B-'):\n",
        "                    # Nuova entità\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "\n",
        "                    current_entity = {\n",
        "                        'text': result['word'],\n",
        "                        'label': result['entity'][2:],\n",
        "                        'start_char': result['start'],\n",
        "                        'end_char': result['end'],\n",
        "                        'score': result['score']\n",
        "                    }\n",
        "\n",
        "                elif result['entity'].startswith('I-') and current_entity:\n",
        "                    # Continua l'entità precedente\n",
        "                    if result['entity'][2:] == current_entity['label']:\n",
        "                        current_entity['text'] += result['word'].replace('##', '')\n",
        "                        current_entity['end_char'] = result['end']\n",
        "                        current_entity['score'] = max(current_entity['score'], result['score'])\n",
        "\n",
        "                else:\n",
        "                    # Caso di fallback\n",
        "                    if current_entity:\n",
        "                        grouped_entities.append(current_entity)\n",
        "                        current_entity = None\n",
        "\n",
        "            # Aggiungi ultima entità\n",
        "            if current_entity:\n",
        "                grouped_entities.append(current_entity)\n",
        "\n",
        "            # Aggiungi source\n",
        "            for entity in grouped_entities:\n",
        "                entity['source'] = 'transformers_ner'\n",
        "                entities.append(entity)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel riconoscimento delle entità con Transformer NER: {e}\")\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def set_language(self, language: str):\n",
        "        \"\"\"\n",
        "        Cambia la lingua del riconoscitore.\n",
        "\n",
        "        Args:\n",
        "            language: Nuova lingua da impostare\n",
        "        \"\"\"\n",
        "        if language != self.language:\n",
        "            self.language = language\n",
        "            self._load_model()\n",
        "\n",
        "    def get_supported_languages(self) -> List[str]:\n",
        "        \"\"\"\n",
        "        Restituisce le lingue supportate.\n",
        "\n",
        "        Returns:\n",
        "            Lista di codici lingua supportati\n",
        "        \"\"\"\n",
        "        return list(self.language_models.keys())\n",
        "\n",
        "\n",
        "\n",
        "# Aggiungi questo prima della definizione di WikidataConnectorWithSpacy\n",
        "class WikidataConnector:\n",
        "    \"\"\"\n",
        "    Classe per l'integrazione con Wikidata per il riconoscimento e l'arricchimento di entità.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore a Wikidata.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.delay = delay\n",
        "        self.cache = {}  # Cache per risultati\n",
        "\n",
        "        # Italiano stop words\n",
        "        self.stop_words = {\n",
        "            \"a\", \"al\", \"alla\", \"allo\", \"ai\", \"agli\", \"alle\", \"con\", \"col\", \"coi\", \"da\", \"dal\", \"dallo\",\n",
        "            \"dalla\", \"dai\", \"dagli\", \"dalle\", \"di\", \"del\", \"dello\", \"della\", \"dei\", \"degli\", \"delle\",\n",
        "            \"in\", \"nel\", \"nello\", \"nella\", \"nei\", \"negli\", \"nelle\", \"su\", \"sul\", \"sullo\", \"sulla\",\n",
        "            \"sui\", \"sugli\", \"sulle\", \"per\", \"tra\", \"fra\", \"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\",\n",
        "            \"uno\", \"una\", \"un'\", \"che\", \"chi\", \"cui\", \"è\", \"sono\", \"sei\", \"siamo\", \"siete\", \"ha\",\n",
        "            \"ho\", \"hai\", \"abbiamo\", \"avete\", \"hanno\", \"e\", \"o\", \"ma\", \"se\", \"anche\", \"però\", \"come\",\n",
        "            \"dove\", \"quando\", \"mentre\", \"ed\", \"od\", \"né\", \"più\", \"meno\", \"molto\", \"poco\", \"tanto\",\n",
        "            \"ogni\", \"questo\", \"questa\", \"questi\", \"queste\", \"quello\", \"quella\", \"quelli\", \"quelle\"\n",
        "        }\n",
        "\n",
        "    def search_entity(self, term: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Cerca entità su Wikidata in base a un termine.\n",
        "\n",
        "        Args:\n",
        "            term: Il termine da cercare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"search_{self.language}_{term}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per la ricerca su Wikidata\n",
        "        params = {\n",
        "            'action': 'wbsearchentities',\n",
        "            'search': term,\n",
        "            'language': self.language,\n",
        "            'format': 'json',\n",
        "            'limit': 5  # Limita a 5 risultati per efficienza\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua ricerca\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            results = data.get('search', [])\n",
        "            self.cache[cache_key] = results\n",
        "\n",
        "            # Rispetta i rate limit\n",
        "            time.sleep(self.delay)\n",
        "\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nella ricerca Wikidata: {e}\")\n",
        "            return []\n",
        "\n",
        "    def get_entity_by_id(self, entity_id: str) -> Optional[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene i dettagli completi di un'entità Wikidata tramite ID.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata (es. Q220)\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con i dettagli dell'entità o None in caso di errore\n",
        "        \"\"\"\n",
        "        # Controlla cache\n",
        "        cache_key = f\"entity_{entity_id}\"\n",
        "        if cache_key in self.cache:\n",
        "            return self.cache[cache_key]\n",
        "\n",
        "        # Parametri per l'API Wikidata\n",
        "        params = {\n",
        "            'action': 'wbgetentities',\n",
        "            'ids': entity_id,\n",
        "            'languages': self.language,\n",
        "            'format': 'json'\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            # Effettua richiesta\n",
        "            response = requests.get(\n",
        "                'https://www.wikidata.org/w/api.php',\n",
        "                params=params,\n",
        "                timeout=10\n",
        "            )\n",
        "            response.raise_for_status()\n",
        "            data = response.json()\n",
        "\n",
        "            if 'entities' in data and entity_id in data['entities']:\n",
        "                entity = data['entities'][entity_id]\n",
        "                self.cache[cache_key] = entity\n",
        "\n",
        "                # Rispetta i rate limit\n",
        "                time.sleep(self.delay)\n",
        "\n",
        "                return entity\n",
        "\n",
        "            return None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nel recupero dell'entità Wikidata {entity_id}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def get_entity_types(self, entity_id: str) -> List[Dict[str, str]]:\n",
        "        \"\"\"\n",
        "        Ottiene i tipi di un'entità Wikidata tramite la proprietà 'instance of' (P31).\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            Lista di tipi dell'entità con ID e label\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity or 'claims' not in entity:\n",
        "            return []\n",
        "\n",
        "        # P31 è la proprietà \"instance of\" in Wikidata\n",
        "        if 'P31' not in entity['claims']:\n",
        "            return []\n",
        "\n",
        "        types = []\n",
        "        for claim in entity['claims']['P31']:\n",
        "            if 'mainsnak' in claim and 'datavalue' in claim['mainsnak']:\n",
        "                datavalue = claim['mainsnak']['datavalue']\n",
        "                if datavalue['type'] == 'wikibase-entityid':\n",
        "                    type_id = datavalue['value']['id']\n",
        "\n",
        "                    # Ottieni l'etichetta del tipo\n",
        "                    type_entity = self.get_entity_by_id(type_id)\n",
        "                    type_label = None\n",
        "\n",
        "                    if type_entity and 'labels' in type_entity:\n",
        "                        if self.language in type_entity['labels']:\n",
        "                            type_label = type_entity['labels'][self.language]['value']\n",
        "                        elif 'en' in type_entity['labels']:\n",
        "                            # Fallback all'inglese\n",
        "                            type_label = type_entity['labels']['en']['value']\n",
        "\n",
        "                    types.append({\n",
        "                        'id': type_id,\n",
        "                        'label': type_label or type_id\n",
        "                    })\n",
        "\n",
        "        return types\n",
        "\n",
        "    def get_equivalent_dbpedia_uri(self, entity_id: str) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Trova l'URI DBpedia equivalente per un'entità Wikidata.\n",
        "\n",
        "        Args:\n",
        "            entity_id: L'ID dell'entità Wikidata\n",
        "\n",
        "        Returns:\n",
        "            URI DBpedia equivalente o None se non trovato\n",
        "        \"\"\"\n",
        "        entity = self.get_entity_by_id(entity_id)\n",
        "        if not entity:\n",
        "            return None\n",
        "\n",
        "        # Cerca l'equivalente Wikipedia\n",
        "        if 'sitelinks' in entity:\n",
        "            # Prima cerca nella lingua specificata\n",
        "            wiki_key = f\"{self.language}wiki\"\n",
        "            if wiki_key in entity['sitelinks']:\n",
        "                title = entity['sitelinks'][wiki_key]['title']\n",
        "                # Converti spazi in underscore e codifica per URI\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "            # Fallback all'inglese\n",
        "            elif 'enwiki' in entity['sitelinks']:\n",
        "                title = entity['sitelinks']['enwiki']['title']\n",
        "                title = title.replace(' ', '_')\n",
        "                return f\"http://dbpedia.org/resource/{title}\"\n",
        "\n",
        "        return None\n",
        "\n",
        "    def extract_candidates(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità da un testo, ripulendoli dalle stop words.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        candidates = []\n",
        "\n",
        "        # 1. Estrai frasi multi-parola con iniziali maiuscole (nomi propri complessi)\n",
        "        multi_word_entities = re.findall(r'\\b[A-Z][a-zA-Z]*(?: [A-Z][a-zA-Z]*)+\\b', text)\n",
        "        candidates.extend(multi_word_entities)\n",
        "\n",
        "        # 2. Estrai sequenze \"nome e nome\" con iniziali maiuscole (es. \"Dante Alighieri\")\n",
        "        name_with_surname = re.findall(r'\\b[A-Z][a-zA-Z]* (?:di |da |de |del |della |degli |dei |van |von |)[A-Z][a-zA-Z]*\\b', text)\n",
        "        candidates.extend(name_with_surname)\n",
        "\n",
        "        # 3. Estrai singole parole con iniziale maiuscola (nomi propri)\n",
        "        proper_nouns = re.findall(r'\\b[A-Z][a-zA-Z]{3,}\\b', text)\n",
        "\n",
        "        # Filtra nomi propri per rimuovere quelli all'inizio di frase e parole comuni\n",
        "        for noun in proper_nouns:\n",
        "            # Esclude parole che iniziano frasi\n",
        "            pattern = r'(?:\\. |^)' + re.escape(noun)\n",
        "            if not re.search(pattern, text):\n",
        "                candidates.append(noun)\n",
        "\n",
        "        # 4. Coppie di sostantivi che potrebbero essere entità\n",
        "        # Es. \"Politecnico di Milano\", \"Università di Bologna\"\n",
        "        institution_patterns = [\n",
        "            r'\\b(?:Università|Politecnico|Accademia|Istituto|Teatro|Museo|Galleria) (?:di|del|della|degli|dei) [A-Z][a-zA-Z]+\\b',\n",
        "            r'\\b[A-Z][a-zA-Z]+ (?:University|College|Institute|Museum|Gallery)\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in institution_patterns:\n",
        "            institutions = re.findall(pattern, text)\n",
        "            candidates.extend(institutions)\n",
        "\n",
        "        # 5. Nomi di luoghi geografici\n",
        "        geo_patterns = [\n",
        "            r'\\b(?:Mar|Monte|Lago|Fiume|Golfo|Isola|Isole|Monti|Catena|Arcipelago|Oceano) [A-Z][a-zA-Z]+\\b'\n",
        "        ]\n",
        "\n",
        "        for pattern in geo_patterns:\n",
        "            geo_entities = re.findall(pattern, text)\n",
        "            candidates.extend(geo_entities)\n",
        "\n",
        "        # Rimuovi duplicati mantenendo l'ordine\n",
        "        seen = set()\n",
        "        filtered_candidates = []\n",
        "\n",
        "        for candidate in candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                # Verifica che il candidato non sia composto solo da stop words\n",
        "                words = candidate.lower().split()\n",
        "                if any(word not in self.stop_words for word in words):\n",
        "                    filtered_candidates.append(candidate)\n",
        "                    seen.add(candidate.lower())\n",
        "\n",
        "        return filtered_candidates\n",
        "\n",
        "    def analyze_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Estrai candidati entità\n",
        "        candidates = self.extract_candidates(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe WikidataConnector esistente per integrare spaCy\n",
        "class WikidataConnectorWithSpacy(WikidataConnector):\n",
        "    \"\"\"\n",
        "    Versione estesa di WikidataConnector che integra spaCy\n",
        "    per un riconoscimento delle entità più accurato.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", delay: float = 0.3):\n",
        "        \"\"\"\n",
        "        Inizializza il connettore Wikidata con integrazione spaCy.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (it, en, fr, de, es, ecc.)\n",
        "            delay: Ritardo tra le richieste per evitare limiti di rate\n",
        "        \"\"\"\n",
        "        super().__init__(language, delay)\n",
        "        self.spacy_recognizer = SpacyRecognizer(language)\n",
        "\n",
        "    def extract_candidates_with_spacy(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Estrae candidati entità usando spaCy e regole euristiche.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di candidati entità\n",
        "        \"\"\"\n",
        "        # Ottieni entità da spaCy\n",
        "        spacy_entities = self.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "        # Estrai testi delle entità\n",
        "        spacy_texts = [entity['text'] for entity in spacy_entities]\n",
        "\n",
        "        # Combina con candidati estratti dal metodo originale\n",
        "        rule_based_candidates = self.extract_candidates(text)\n",
        "\n",
        "        # Unisci i candidati eliminando duplicati, mantenendo l'ordine\n",
        "        all_candidates = []\n",
        "        seen = set()\n",
        "\n",
        "        # Prima aggiungi entità spaCy (hanno priorità)\n",
        "        for candidate in spacy_texts:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        # Poi aggiungi i candidati basati su regole\n",
        "        for candidate in rule_based_candidates:\n",
        "            if candidate.lower() not in seen:\n",
        "                all_candidates.append(candidate)\n",
        "                seen.add(candidate.lower())\n",
        "\n",
        "        return all_candidates\n",
        "\n",
        "    def analyze_text_with_spacy(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Analizza un testo ed estrae entità usando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con le entità trovate e metadati\n",
        "        \"\"\"\n",
        "        # Se spaCy non è disponibile, usa il metodo originale\n",
        "        if not self.spacy_recognizer.is_available:\n",
        "            return self.analyze_text(text)\n",
        "\n",
        "        # Estrai candidati combinando spaCy e regole\n",
        "        candidates = self.extract_candidates_with_spacy(text)\n",
        "\n",
        "        entities = []\n",
        "\n",
        "        # Cerca ogni candidato su Wikidata\n",
        "        for candidate in candidates:\n",
        "            search_results = self.search_entity(candidate)\n",
        "            if search_results:\n",
        "                # Prendi il primo risultato (più rilevante)\n",
        "                top_result = search_results[0]\n",
        "                entity_id = top_result['id']\n",
        "\n",
        "                # Ottieni tipi\n",
        "                types = self.get_entity_types(entity_id)\n",
        "                type_labels = [t['label'] for t in types if 'label' in t]\n",
        "\n",
        "                # Ottieni URI DBpedia equivalente\n",
        "                dbpedia_uri = self.get_equivalent_dbpedia_uri(entity_id)\n",
        "\n",
        "                entity = {\n",
        "                    'text': candidate,\n",
        "                    'wikidata_id': entity_id,\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{entity_id}\",\n",
        "                    'dbpedia_uri': dbpedia_uri,\n",
        "                    'label': top_result.get('label', candidate),\n",
        "                    'description': top_result.get('description', ''),\n",
        "                    'types': type_labels,\n",
        "                    'score': top_result.get('score', 0),\n",
        "                    'source': 'wikidata+spacy'\n",
        "                }\n",
        "\n",
        "                entities.append(entity)\n",
        "\n",
        "        return {\n",
        "            'text_length': len(text),\n",
        "            'entities_count': len(entities),\n",
        "            'entities': entities\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "class EntityLinker:\n",
        "    \"\"\"\n",
        "    Classe per collegare entità tra diversi sistemi (DBpedia, Wikidata, testo).\n",
        "    Combina diverse strategie per massimizzare la copertura.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati\n",
        "        \"\"\"\n",
        "        self.language = language\n",
        "        self.wikidata = WikidataConnector(language=language)\n",
        "\n",
        "        # Dizionario ampliato di entità comuni italiane\n",
        "        self.local_entities = {\n",
        "            # Paesi e continenti\n",
        "            \"italia\": {\"uri\": \"http://dbpedia.org/resource/Italy\", \"type\": \"Country\", \"wikidata_id\": \"Q38\"},\n",
        "            \"europa\": {\"uri\": \"http://dbpedia.org/resource/Europe\", \"type\": \"Continent\", \"wikidata_id\": \"Q46\"},\n",
        "            \"stati uniti\": {\"uri\": \"http://dbpedia.org/resource/United_States\", \"type\": \"Country\", \"wikidata_id\": \"Q30\"},\n",
        "            \"vaticano\": {\"uri\": \"http://dbpedia.org/resource/Vatican_City\", \"type\": \"Country\", \"wikidata_id\": \"Q237\"},\n",
        "\n",
        "            # Città italiane\n",
        "            \"roma\": {\"uri\": \"http://dbpedia.org/resource/Rome\", \"type\": \"City\", \"wikidata_id\": \"Q220\"},\n",
        "            \"milano\": {\"uri\": \"http://dbpedia.org/resource/Milan\", \"type\": \"City\", \"wikidata_id\": \"Q490\"},\n",
        "            \"napoli\": {\"uri\": \"http://dbpedia.org/resource/Naples\", \"type\": \"City\", \"wikidata_id\": \"Q2634\"},\n",
        "            \"firenze\": {\"uri\": \"http://dbpedia.org/resource/Florence\", \"type\": \"City\", \"wikidata_id\": \"Q2044\"},\n",
        "            \"venezia\": {\"uri\": \"http://dbpedia.org/resource/Venice\", \"type\": \"City\", \"wikidata_id\": \"Q641\"},\n",
        "            \"torino\": {\"uri\": \"http://dbpedia.org/resource/Turin\", \"type\": \"City\", \"wikidata_id\": \"Q495\"},\n",
        "            \"bologna\": {\"uri\": \"http://dbpedia.org/resource/Bologna\", \"type\": \"City\", \"wikidata_id\": \"Q1891\"},\n",
        "\n",
        "            # Monumenti e luoghi\n",
        "            \"colosseo\": {\"uri\": \"http://dbpedia.org/resource/Colosseum\", \"type\": \"Monument\", \"wikidata_id\": \"Q10285\"},\n",
        "            \"vesuvio\": {\"uri\": \"http://dbpedia.org/resource/Mount_Vesuvius\", \"type\": \"Volcano\", \"wikidata_id\": \"Q2292\"},\n",
        "            \"mediterraneo\": {\"uri\": \"http://dbpedia.org/resource/Mediterranean_Sea\", \"type\": \"Sea\", \"wikidata_id\": \"Q4918\"},\n",
        "\n",
        "            # Università\n",
        "            \"sapienza\": {\"uri\": \"http://dbpedia.org/resource/Sapienza_University_of_Rome\", \"type\": \"University\", \"wikidata_id\": \"Q213485\"},\n",
        "            \"politecnico di milano\": {\"uri\": \"http://dbpedia.org/resource/Polytechnic_University_of_Milan\", \"type\": \"University\", \"wikidata_id\": \"Q1151817\"},\n",
        "\n",
        "            # Prodotti e marchi\n",
        "            \"ferrari\": {\"uri\": \"http://dbpedia.org/resource/Ferrari\", \"type\": \"Company\", \"wikidata_id\": \"Q8402\"},\n",
        "            \"lamborghini\": {\"uri\": \"http://dbpedia.org/resource/Lamborghini\", \"type\": \"Company\", \"wikidata_id\": \"Q26378\"},\n",
        "            \"chianti\": {\"uri\": \"http://dbpedia.org/resource/Chianti\", \"type\": \"Wine\", \"wikidata_id\": \"Q662785\"},\n",
        "            \"barolo\": {\"uri\": \"http://dbpedia.org/resource/Barolo\", \"type\": \"Wine\", \"wikidata_id\": \"Q812394\"},\n",
        "            \"pizza\": {\"uri\": \"http://dbpedia.org/resource/Pizza\", \"type\": \"Food\", \"wikidata_id\": \"Q177\"},\n",
        "\n",
        "            # Personaggi storici\n",
        "            \"dante alighieri\": {\"uri\": \"http://dbpedia.org/resource/Dante_Alighieri\", \"type\": \"Person\", \"wikidata_id\": \"Q1067\"},\n",
        "            \"leonardo da vinci\": {\"uri\": \"http://dbpedia.org/resource/Leonardo_da_Vinci\", \"type\": \"Person\", \"wikidata_id\": \"Q762\"},\n",
        "            \"galileo galilei\": {\"uri\": \"http://dbpedia.org/resource/Galileo_Galilei\", \"type\": \"Person\", \"wikidata_id\": \"Q307\"}\n",
        "        }\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando molteplici strategie.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Inizia con Wikidata\n",
        "        wikidata_results = self.wikidata.analyze_text(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Usa anche il metodo locale per avere una copertura completa\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _is_valid_entity(self, text: str) -> bool:\n",
        "        \"\"\"\n",
        "        Verifica se un testo rappresenta un'entità valida (non una stopword o articolo).\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da verificare\n",
        "\n",
        "        Returns:\n",
        "            True se l'entità è valida, False altrimenti\n",
        "        \"\"\"\n",
        "        if not text:\n",
        "            return False\n",
        "\n",
        "        text_lower = text.lower()\n",
        "\n",
        "        # Stopwords più comuni\n",
        "        stopwords = self.wikidata.stop_words\n",
        "\n",
        "        # Articoli in italiano\n",
        "        articles = {\"il\", \"lo\", \"la\", \"i\", \"gli\", \"le\", \"un\", \"uno\", \"una\"}\n",
        "\n",
        "        # Congiunzioni e preposizioni\n",
        "        conjunctions = {\"e\", \"o\", \"ma\", \"se\", \"mentre\", \"per\", \"con\", \"su\", \"in\", \"da\", \"di\", \"a\"}\n",
        "\n",
        "        # Verifica se il testo è composto solo da stopwords, articoli o congiunzioni\n",
        "        words = text_lower.split()\n",
        "\n",
        "        # Se è una singola parola e fa parte delle stopwords\n",
        "        if len(words) == 1 and (words[0] in stopwords or words[0] in articles or words[0] in conjunctions):\n",
        "            return False\n",
        "\n",
        "        # Se tutte le parole sono stopwords\n",
        "        if all(word in stopwords or word in articles or word in conjunctions for word in words):\n",
        "            return False\n",
        "\n",
        "        # Se è una parola troppo corta (meno di 3 caratteri)\n",
        "        if len(text) < 3:\n",
        "            return False\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _find_local_entities(self, text: str) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità usando il dizionario locale.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità trovate\n",
        "        \"\"\"\n",
        "        text_lower = text.lower()\n",
        "        entities = []\n",
        "\n",
        "        # Ordina le chiavi del dizionario per lunghezza (decrescente)\n",
        "        # Questo assicura che \"politecnico di milano\" venga trovato prima di \"milano\"\n",
        "        sorted_keys = sorted(self.local_entities.keys(), key=len, reverse=True)\n",
        "\n",
        "        for entity_name in sorted_keys:\n",
        "            if entity_name in text_lower:\n",
        "                info = self.local_entities[entity_name]\n",
        "\n",
        "                # Trova la posizione dell'entità nel testo\n",
        "                start = text_lower.find(entity_name)\n",
        "\n",
        "                # Controlla se l'entità è parte di un'altra parola\n",
        "                if start > 0 and text_lower[start-1].isalnum():\n",
        "                    continue  # Skip se l'entità è parte di una parola più lunga\n",
        "\n",
        "                if start + len(entity_name) < len(text_lower) and text_lower[start + len(entity_name)].isalnum():\n",
        "                    continue  # Skip se l'entità è seguita da altri caratteri alfanumerici\n",
        "\n",
        "                # Estrai il testo originale con la capitalizzazione originale\n",
        "                original_text = text[start:start+len(entity_name)]\n",
        "\n",
        "                entity = {\n",
        "                    'text': original_text,\n",
        "                    'dbpedia_uri': info['uri'],\n",
        "                    'types': [info['type']],\n",
        "                    'wikidata_id': info.get('wikidata_id'),\n",
        "                    'wikidata_url': f\"https://www.wikidata.org/wiki/{info.get('wikidata_id')}\" if info.get('wikidata_id') else None,\n",
        "                    'score': 1.0,\n",
        "                    'source': 'local'\n",
        "                }\n",
        "                entities.append(entity)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinker per utilizzare il WikidataConnector migliorato\n",
        "class EntityLinkerWithSpacy(EntityLinker):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker che utilizza il WikidataConnector con integrazione spaCy.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\"):\n",
        "        # Prima chiamiamo il costruttore base per inizializzare il dizionario locale\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Poi sostituiamo l'oggetto wikidata con la versione estesa\n",
        "        self.wikidata = WikidataConnectorWithSpacy(language=language)\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando spaCy e Wikidata.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Usa il metodo migliorato che integra spaCy\n",
        "        wikidata_results = self.wikidata.analyze_text_with_spacy(text)\n",
        "        entities = wikidata_results['entities']\n",
        "\n",
        "        # Il resto del metodo è identico alla versione originale\n",
        "        local_entities = self._find_local_entities(text)\n",
        "\n",
        "        # Combina i risultati, evitando duplicati\n",
        "        existing_ids = {e.get('wikidata_id') for e in entities if 'wikidata_id' in e}\n",
        "        existing_texts = {e.get('text').lower() for e in entities if 'text' in e}\n",
        "\n",
        "        for local_entity in local_entities:\n",
        "            wikidata_id = local_entity.get('wikidata_id')\n",
        "            text_lower = local_entity.get('text', '').lower()\n",
        "\n",
        "            if (wikidata_id and wikidata_id not in existing_ids) or (text_lower and text_lower not in existing_texts):\n",
        "                entities.append(local_entity)\n",
        "\n",
        "        # Rimuovi le entità che sono stopwords o articoli\n",
        "        entities = [e for e in entities if self._is_valid_entity(e.get('text', ''))]\n",
        "\n",
        "        # Arricchisci le entità con dati aggiuntivi se necessario\n",
        "        enriched_entities = []\n",
        "        for entity in entities:\n",
        "            # Assicurati che abbiamo un URI DBpedia\n",
        "            if 'dbpedia_uri' not in entity or not entity['dbpedia_uri']:\n",
        "                wikidata_id = entity.get('wikidata_id')\n",
        "                if wikidata_id:\n",
        "                    dbpedia_uri = self.wikidata.get_equivalent_dbpedia_uri(wikidata_id)\n",
        "                    if dbpedia_uri:\n",
        "                        entity['dbpedia_uri'] = dbpedia_uri\n",
        "\n",
        "            enriched_entities.append(entity)\n",
        "\n",
        "        # Aggiungi le entità spaCy non trovate in Wikidata\n",
        "        spacy_only_entities = self._get_spacy_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(spacy_only_entities)\n",
        "\n",
        "        # Raccogli statistiche\n",
        "        stats = {\n",
        "            'total_entities': len(enriched_entities),\n",
        "            'wikidata_entities': len(wikidata_results['entities']),\n",
        "            'spacy_only_entities': len(spacy_only_entities),\n",
        "            'local_entities': sum(1 for e in enriched_entities if e.get('source') == 'local'),\n",
        "            'entities_with_dbpedia': sum(1 for e in enriched_entities if 'dbpedia_uri' in e and e['dbpedia_uri']),\n",
        "            'entities_with_types': sum(1 for e in enriched_entities if 'types' in e and e['types'])\n",
        "        }\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_spacy_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "            \"\"\"\n",
        "            Ottiene entità riconosciute solo da spaCy e non da Wikidata o dal dizionario locale.\n",
        "\n",
        "            Args:\n",
        "                text: Il testo da analizzare\n",
        "                existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "            Returns:\n",
        "                Lista di entità riconosciute solo da spaCy\n",
        "            \"\"\"\n",
        "            # CORREZIONE: Verifica che il riconoscitore spaCy sia disponibile\n",
        "            if not hasattr(self.wikidata, 'spacy_recognizer') or not self.wikidata.spacy_recognizer.is_available:\n",
        "                print(\"Riconoscitore spaCy non disponibile.\")\n",
        "                return []\n",
        "\n",
        "            # Ottieni tutte le entità da spaCy\n",
        "            spacy_entities = self.wikidata.spacy_recognizer.recognize_entities(text)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy sono state trovate\n",
        "            print(f\"spaCy ha trovato {len(spacy_entities)} entità nel testo\")\n",
        "\n",
        "            # Lista per entità trovate solo da spaCy\n",
        "            spacy_only = []\n",
        "\n",
        "            # CORREZIONE: Itera su tutte le entità spaCy\n",
        "            for entity in spacy_entities:\n",
        "                entity_text = entity['text']\n",
        "                entity_lower = entity_text.lower()\n",
        "\n",
        "                # Verifica se l'entità è già stata trovata da altre fonti\n",
        "                if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                    # CORREZIONE: Debug per ogni entità spaCy considerata unica\n",
        "                    print(f\"Entità unica da spaCy: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                    # Converti l'etichetta spaCy in un tipo più generico\n",
        "                    spacy_type = self._map_spacy_label_to_type(entity['label'])\n",
        "\n",
        "                    # Crea un'entità formattata\n",
        "                    spacy_entity = {\n",
        "                        'text': entity_text,\n",
        "                        'types': [spacy_type],\n",
        "                        'source': 'spacy',\n",
        "                        'label': entity['label']\n",
        "                    }\n",
        "\n",
        "                    # Aggiungi alla lista e al set di testi esistenti\n",
        "                    spacy_only.append(spacy_entity)\n",
        "                    existing_texts.add(entity_lower)\n",
        "\n",
        "            # CORREZIONE: Debug per verificare quante entità spaCy uniche sono state trovate\n",
        "            print(f\"Di cui {len(spacy_only)} sono uniche (non trovate da Wikidata o dal dizionario locale)\")\n",
        "\n",
        "            return spacy_only\n",
        "\n",
        "    def _map_spacy_label_to_type(self, spacy_label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette spaCy a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            spacy_label: L'etichetta spaCy\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette spaCy a tipi più generali\n",
        "        # Questa mappatura dipende dal modello spaCy utilizzato\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'PERSON': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'FAC': 'Facility',\n",
        "            'FACILITY': 'Facility',\n",
        "            'PRODUCT': 'Product',\n",
        "            'EVENT': 'Event',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'LAW': 'Law',\n",
        "            'LANGUAGE': 'Language',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'PERCENT': 'Percent',\n",
        "            'MONEY': 'Money',\n",
        "            'QUANTITY': 'Quantity',\n",
        "            'ORDINAL': 'Ordinal',\n",
        "            'CARDINAL': 'Cardinal'\n",
        "        }\n",
        "\n",
        "        return mapping.get(spacy_label, spacy_label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estensione del metodo originale per includere i risultati di spaCy.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        entities, stats = self.find_entities(text)\n",
        "\n",
        "        # Organizza i risultati per parola/frase\n",
        "        results = {}\n",
        "        for entity in entities:\n",
        "            text_key = entity['text']\n",
        "\n",
        "            # Ottieni l'URI disponibile (preferibilmente DBpedia)\n",
        "            uri = entity.get('dbpedia_uri')\n",
        "\n",
        "            if text_key and uri:\n",
        "                results[text_key] = {\n",
        "                    'uri': uri,\n",
        "                    'exists': True,  # Assumiamo che esista perché l'abbiamo trovato\n",
        "                    'wikidata_id': entity.get('wikidata_id'),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una nuova sezione per le entità spaCy senza URI\n",
        "        spacy_results = {}\n",
        "        for entity in entities:\n",
        "            if entity.get('source') == 'spacy' and entity['text'] not in results:\n",
        "                spacy_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        return {\n",
        "            'results': results,\n",
        "            'spacy_results': spacy_results,\n",
        "            'stats': stats,\n",
        "            'text': text\n",
        "        }\n",
        "\n",
        "\n",
        "\n",
        "# Estendi la classe EntityLinkerWithSpacy per incorporare il riconoscitore italiano\n",
        "class EntityLinkerItalian(EntityLinkerWithSpacy):\n",
        "    \"\"\"\n",
        "    Versione estesa di EntityLinker ottimizzata per l'italiano in ambiente Colab.\n",
        "    Integra riconoscitori multipli: Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, language: str = \"it\", use_gpu: bool = False):\n",
        "        \"\"\"\n",
        "        Inizializza il linker di entità con supporto NLP per l'italiano.\n",
        "\n",
        "        Args:\n",
        "            language: Lingua preferita per i risultati (deve essere \"it\")\n",
        "            use_gpu: Se True, utilizza GPU per l'accelerazione (se disponibile in Colab)\n",
        "        \"\"\"\n",
        "        # Controlla che la lingua sia italiana\n",
        "        if language != \"it\":\n",
        "            print(\"Attenzione: questo riconoscitore è ottimizzato per l'italiano. Impostiamo language='it'.\")\n",
        "            language = \"it\"\n",
        "\n",
        "        # Inizializza la classe base (con spaCy)\n",
        "        super().__init__(language)\n",
        "\n",
        "        # Aggiungi il riconoscitore italiano Stanza NLP\n",
        "        self.italian_recognizer = ItalianNLPRecognizer(use_gpu=use_gpu)\n",
        "\n",
        "        # Aggiungi il riconoscitore Transformer NER\n",
        "        try:\n",
        "            from transformers import AutoTokenizer  # Verifica disponibilità\n",
        "            self.transformers_recognizer = TransformersNERRecognizer(\n",
        "                model_name=\"dbmdz/bert-base-italian-cased-ner\",\n",
        "                language=language,\n",
        "                use_gpu=use_gpu\n",
        "            )\n",
        "        except ImportError:\n",
        "            print(\"Transformer NER non disponibile. Installare 'transformers' e 'torch'.\")\n",
        "            self.transformers_recognizer = None\n",
        "\n",
        "    def find_entities(self, text: str) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Trova entità nel testo utilizzando Wikidata, spaCy, Stanza NLP e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Tuple con lista di entità e statistiche\n",
        "        \"\"\"\n",
        "        # Prima ottieni i risultati dalla classe base (Wikidata + spaCy)\n",
        "        enriched_entities, stats = super().find_entities(text)\n",
        "        existing_texts = {e.get('text', '').lower() for e in enriched_entities if 'text' in e}\n",
        "\n",
        "        # Aggiungi le entità del riconoscitore italiano se disponibile\n",
        "        italian_only_entities = self._get_italian_only_entities(text, existing_texts)\n",
        "        enriched_entities.extend(italian_only_entities)\n",
        "\n",
        "        # Aggiungi le entità dal riconoscitore Transformer NER\n",
        "        if self.transformers_recognizer and self.transformers_recognizer.is_available:\n",
        "            transformers_only_entities = self._get_transformers_only_entities(text, existing_texts)\n",
        "            enriched_entities.extend(transformers_only_entities)\n",
        "            stats['transformers_only_entities'] = len(transformers_only_entities)\n",
        "\n",
        "        # Aggiorna le statistiche\n",
        "        stats['italian_only_entities'] = len(italian_only_entities)\n",
        "        stats['total_entities'] = len(enriched_entities)\n",
        "\n",
        "        return enriched_entities, stats\n",
        "\n",
        "    def _get_italian_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore italiano e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo dal riconoscitore italiano\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore italiano sia disponibile\n",
        "        if not self.italian_recognizer.is_available:\n",
        "            print(\"Riconoscitore NLP italiano non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore italiano\n",
        "        italian_entities = self.italian_recognizer.recognize_entities(text)\n",
        "        print(f\"Riconoscitore italiano ha trovato {len(italian_entities)} entità nel testo\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        italian_only = []\n",
        "        for entity in italian_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            if entity_lower not in existing_texts and self._is_valid_entity(entity_text):\n",
        "                print(f\"Entità unica dal riconoscitore italiano: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                italian_type = self._map_italian_label_to_type(entity['label'])\n",
        "\n",
        "                italian_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [italian_type],\n",
        "                    'source': 'italian_nlp',\n",
        "                    'label': entity['label']\n",
        "                }\n",
        "\n",
        "                italian_only.append(italian_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"Di cui {len(italian_only)} sono uniche (non trovate da altre fonti)\")\n",
        "        return italian_only\n",
        "\n",
        "    def _get_transformers_only_entities(self, text: str, existing_texts: Set[str]) -> List[Dict[str, Any]]:\n",
        "        \"\"\"\n",
        "        Ottiene entità riconosciute solo dal riconoscitore Transformer NER\n",
        "        e non da altre fonti.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "            existing_texts: Set di testi di entità già trovate\n",
        "\n",
        "        Returns:\n",
        "            Lista di entità riconosciute solo da Transformer NER\n",
        "        \"\"\"\n",
        "        # Verifica che il riconoscitore Transformer sia disponibile\n",
        "        if not self.transformers_recognizer or not self.transformers_recognizer.is_available:\n",
        "            print(\"Riconoscitore Transformer NER non disponibile.\")\n",
        "            return []\n",
        "\n",
        "        # Ottieni tutte le entità dal riconoscitore Transformer NER\n",
        "        transformers_entities = self.transformers_recognizer.recognize_entities(text)\n",
        "        print(f\"DEBUG: Riconoscitore Transformer NER ha trovato {len(transformers_entities)} entità nel testo\")\n",
        "\n",
        "        # Stampa dettagli delle entità trovate\n",
        "        for entity in transformers_entities:\n",
        "            print(f\"DEBUG: Entità trovata - Testo: {entity['text']}, Etichetta: {entity['label']}, Punteggio: {entity.get('score', 'N/A')}\")\n",
        "\n",
        "        # Stampa testi esistenti\n",
        "        print(f\"DEBUG: Testi esistenti: {existing_texts}\")\n",
        "\n",
        "        # Filtra per entità uniche non trovate da altre fonti\n",
        "        transformers_only = []\n",
        "        for entity in transformers_entities:\n",
        "            entity_text = entity['text']\n",
        "            entity_lower = entity_text.lower()\n",
        "\n",
        "            print(f\"DEBUG: Verifica entità - Testo: {entity_text}, Lowercase: {entity_lower}\")\n",
        "\n",
        "            # Rimuovi la condizione di validità temporaneamente\n",
        "            if entity_lower not in existing_texts:\n",
        "                print(f\"DEBUG: Entità unica dal riconoscitore Transformer NER: {entity_text} ({entity['label']})\")\n",
        "\n",
        "                # Converti l'etichetta in un tipo più generico\n",
        "                transformers_type = self._map_transformers_label_to_type(entity['label'])\n",
        "\n",
        "                transformers_entity = {\n",
        "                    'text': entity_text,\n",
        "                    'types': [transformers_type],\n",
        "                    'source': 'transformers_ner',\n",
        "                    'label': entity['label'],\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "                transformers_only.append(transformers_entity)\n",
        "                existing_texts.add(entity_lower)\n",
        "\n",
        "        print(f\"DEBUG: Entità Transformer NER uniche trovate: {len(transformers_only)}\")\n",
        "        return transformers_only\n",
        "\n",
        "    def _map_italian_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore italiano a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        mapping = {\n",
        "            'PERSON': 'Person',\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'PROPN': 'ProperNoun',\n",
        "            'WORK_OF_ART': 'Artwork',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def _map_transformers_label_to_type(self, label: str) -> str:\n",
        "        \"\"\"\n",
        "        Mappa le etichette del riconoscitore Transformer NER a tipi più generici.\n",
        "\n",
        "        Args:\n",
        "            label: L'etichetta originale\n",
        "\n",
        "        Returns:\n",
        "            Tipo generico\n",
        "        \"\"\"\n",
        "        # Mappatura delle etichette a tipi più generali\n",
        "        # (simile a quella di altri riconoscitori)\n",
        "        mapping = {\n",
        "            'PER': 'Person',\n",
        "            'LOC': 'Location',\n",
        "            'ORG': 'Organization',\n",
        "            'MISC': 'Miscellaneous',\n",
        "            'GPE': 'Location',\n",
        "            'LOCATION': 'Location',\n",
        "            'PERSON': 'Person',\n",
        "            'ORGANIZATION': 'Organization',\n",
        "            'EVENT': 'Event',\n",
        "            'DATE': 'Date',\n",
        "            'TIME': 'Time',\n",
        "            'MONEY': 'Money',\n",
        "            'PERCENT': 'Percent',\n",
        "            'QUANTITY': 'Quantity'\n",
        "        }\n",
        "\n",
        "        return mapping.get(label, label)\n",
        "\n",
        "    def verify_uris_for_text(self, text: str) -> Dict[str, Any]:\n",
        "        \"\"\"\n",
        "        Verifica l'esistenza di URI per tutte le entità in un testo.\n",
        "        Estende il metodo della classe base per includere i risultati\n",
        "        del riconoscitore italiano e Transformer NER.\n",
        "\n",
        "        Args:\n",
        "            text: Il testo da analizzare\n",
        "\n",
        "        Returns:\n",
        "            Dizionario con risultati e statistiche\n",
        "        \"\"\"\n",
        "        # Ottieni i risultati di base\n",
        "        uri_results = super().verify_uris_for_text(text)\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore italiano senza URI\n",
        "        italian_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'italian_nlp' and entity['text'] not in uri_results['results']:\n",
        "                italian_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', [])\n",
        "                }\n",
        "\n",
        "        # Aggiungi una sezione per le entità del riconoscitore Transformer NER senza URI\n",
        "        transformers_results = {}\n",
        "        for entity in uri_results.get('entities', []):\n",
        "            if entity.get('source') == 'transformers_ner' and entity['text'] not in uri_results['results']:\n",
        "                transformers_results[entity['text']] = {\n",
        "                    'label': entity.get('label', ''),\n",
        "                    'types': entity.get('types', []),\n",
        "                    'score': entity.get('score', 0)\n",
        "                }\n",
        "\n",
        "        # Aggiungi le sezioni ai risultati\n",
        "        uri_results['italian_results'] = italian_results\n",
        "        uri_results['transformers_results'] = transformers_results\n",
        "\n",
        "        return uri_results\n",
        "\n",
        "\n",
        "# Aggiungi questo codice alla fine dello script o modifica la sezione principale:\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Crea un'istanza del linker di entità migliorato\n",
        "#    linker = EntityLinkerWithSpacy(language=\"it\")\n",
        "    linker = EntityLinkerItalian(language=\"it\", use_gpu=False)\n",
        "\n",
        "\n",
        "    # Testo di esempio esteso - usa lo stesso testo che stavi analizzando prima\n",
        "    text = \"L'Italia è uno dei paesi più visitati al mondo, con una storia millenaria e un patrimonio culturale unico.\" \\\n",
        "           \" Roma, la capitale, ospita il Colosseo e il Vaticano, mentre Firenze è considerata la culla del Rinascimento.\" \\\n",
        "           \" Milano è un centro globale della moda e dell'economia, e Napoli è famosa per la sua pizza e il Vesuvio.\" \\\n",
        "           \" Il settore turistico è una delle principali fonti di reddito, insieme all'industria manifatturiera e all'export.\" \\\n",
        "           \" Le università italiane, come la Sapienza e il Politecnico di Milano, sono tra le più antiche d'Europa.\" \\\n",
        "           \" Il paese è noto anche per la sua tradizione enogastronomica, con vini rinomati come il Chianti e il Barolo.\" \\\n",
        "           \" La Ferrari e la Lamborghini sono simboli del design automobilistico italiano apprezzati in tutto il mondo.\" \\\n",
        "           \" Il Mediterraneo ha sempre avuto un ruolo strategico nella storia e nell'economia dell'Italia.\" \\\n",
        "           \" Dante Alighieri, Leonardo da Vinci e Galileo Galilei sono tra i più grandi esponenti della cultura italiana.\" \\\n",
        "           \" Negli ultimi anni, l'Italia ha investito molto nella digitalizzazione e nell'innovazione tecnologica.\"\n",
        "\n",
        "    print(f\"Analisi del testo:\\n{text}\\n\")\n",
        "\n",
        "    # Trova entità\n",
        "    entities, stats = linker.find_entities(text)\n",
        "\n",
        "    print(f\"\\nTrovate {stats['total_entities']} entità:\")\n",
        "    for entity in entities:\n",
        "        source = entity.get('source', 'sconosciuta')\n",
        "        uri = entity.get('dbpedia_uri', 'N/A')\n",
        "        types = ', '.join(entity.get('types', ['Sconosciuto']))\n",
        "\n",
        "        print(f\"- '{entity['text']}' → {uri}\")\n",
        "        print(f\"  Tipo: {types}\")\n",
        "        if 'wikidata_id' in entity:\n",
        "            print(f\"  Wikidata: {entity.get('wikidata_url', 'N/A')}\")\n",
        "        print(f\"  Fonte: {source}\")\n",
        "        print(\"\")\n",
        "\n",
        "    # Verifica URI\n",
        "    uri_results = linker.verify_uris_for_text(text)\n",
        "\n",
        "    print(\"\\nVerifica URI:\")\n",
        "    for word, info in uri_results['results'].items():\n",
        "        print(f\"- La parola '{word}' → URI: {info['uri']} esiste\")\n",
        "\n",
        "    # Mostra entità spaCy senza URI\n",
        "    if 'spacy_results' in uri_results and uri_results['spacy_results']:\n",
        "        print(\"\\nEntità riconosciute solo da spaCy (senza URI):\")\n",
        "        for word, info in uri_results['spacy_results'].items():\n",
        "            print(f\"- '{word}' → Etichetta: {info.get('label', '')}, Tipi: {', '.join(info.get('types', []))}\")\n",
        "\n",
        "    # Mostra solo le entità con URI DBpedia\n",
        "    print(\"\\nEntità con URI DBpedia:\")\n",
        "    entities_with_dbpedia = [e for e in entities if 'dbpedia_uri' in e and e['dbpedia_uri']]\n",
        "    for entity in entities_with_dbpedia:\n",
        "        print(f\"- '{entity['text']}' → {entity['dbpedia_uri']}\")\n",
        "\n",
        "    # Esporta le entità\n",
        "    output_file_path = export_entities_to_txt(\n",
        "    entities,\n",
        "    stats,\n",
        "    \"entita.txt\",\n",
        "    spacy_recognizer=linker.wikidata.spacy_recognizer,\n",
        "    italian_recognizer=linker.italian_recognizer,\n",
        "    transformers_recognizer=linker.transformers_recognizer,  # Aggiungi questo\n",
        "    original_text=text\n",
        "    )\n",
        "\n",
        "    print(f\"\\nFile di output salvato in: {output_file_path}\")\n",
        "\n",
        "    print(\"\\nStatistiche:\")\n",
        "    for key, value in stats.items():\n",
        "        print(f\"- {key}: {value}\")\n",
        "\n",
        "    # Verifica aggiuntiva per il file\n",
        "    import os\n",
        "    if os.path.exists(\"entita.txt\"):\n",
        "        file_size = os.path.getsize(\"entita.txt\")\n",
        "        print(f\"\\nVerifica file: 'entita.txt' esiste e ha dimensione {file_size} bytes\")\n",
        "    else:\n",
        "        print(\"\\nATTENZIONE: Il file 'entita.txt' non è stato creato!\")\n",
        "        # Prova a identificare il problema\n",
        "        try:\n",
        "            with open(\"test_write_permission.txt\", \"w\") as f:\n",
        "                f.write(\"Test\")\n",
        "            print(\"La directory è scrivibile, il problema è altrove\")\n",
        "            os.remove(\"test_write_permission.txt\")\n",
        "        except Exception as e:\n",
        "            print(f\"Errore nell'accesso alla directory: {e}\")\n",
        "            print(\"Prova a specificare un percorso completo per il file di output\")\n",
        "            # Tenta con percorso nella home directory dell'utente\n",
        "            home_dir = os.path.expanduser(\"~\")\n",
        "            output_path = os.path.join(home_dir, \"entita.txt\")\n",
        "            try:\n",
        "                export_entities_to_txt(entities, stats, output_path)\n",
        "                print(f\"File salvato con successo in: {output_path}\")\n",
        "            except Exception as e2:\n",
        "                print(f\"Errore anche con percorso alternativo: {e2}\")"
      ],
      "metadata": {
        "id": "ib3r2kyHEJ01"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "99b2c7037a41432b9c4ca5082c3d7a04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3086d26b5924bbdae57674d26283897",
              "IPY_MODEL_4c900e21b11b411ab7c455f04e1b85dc",
              "IPY_MODEL_7a5cdc20c07e4816a3a0fd19a61e804b"
            ],
            "layout": "IPY_MODEL_56047f68127946569aada5452404e4e1"
          }
        },
        "d3086d26b5924bbdae57674d26283897": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d2d6713ad9e34197b91a68f69132c349",
            "placeholder": "​",
            "style": "IPY_MODEL_79392c1fd4734882828b6c5da77dd33f",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "4c900e21b11b411ab7c455f04e1b85dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15364e7e9ef242e9b8951c76da22f619",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3571070609e241efb626541028b1e09c",
            "value": 52557
          }
        },
        "7a5cdc20c07e4816a3a0fd19a61e804b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef78ffb6569f44be8b40aed193d3ac2b",
            "placeholder": "​",
            "style": "IPY_MODEL_46d4463a40ab48749959d0a2f8f0c044",
            "value": " 424k/? [00:00&lt;00:00, 26.6MB/s]"
          }
        },
        "56047f68127946569aada5452404e4e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2d6713ad9e34197b91a68f69132c349": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79392c1fd4734882828b6c5da77dd33f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "15364e7e9ef242e9b8951c76da22f619": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3571070609e241efb626541028b1e09c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ef78ffb6569f44be8b40aed193d3ac2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46d4463a40ab48749959d0a2f8f0c044": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "befca2b67b274a72a2f5213ae964fb98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb8d2a11d3a84dd79a4f3f308aa465d4",
              "IPY_MODEL_857f3bcab111422f9e30442ba135460d",
              "IPY_MODEL_acdc3319110d496ab68ac4b0b9e0d638"
            ],
            "layout": "IPY_MODEL_727d64886002405a923d4407af0c4209"
          }
        },
        "fb8d2a11d3a84dd79a4f3f308aa465d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43fef5bc8a9b4d1abda35a54026e44bb",
            "placeholder": "​",
            "style": "IPY_MODEL_06315793acb0496b9fe3946783d10b04",
            "value": "Downloading https://huggingface.co/stanfordnlp/stanza-it/resolve/v1.10.0/models/default.zip: 100%"
          }
        },
        "857f3bcab111422f9e30442ba135460d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d6f61c84b8604421a831513077391927",
            "max": 455516818,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_32b257cc71514839b256ca7e3830f83f",
            "value": 455516818
          }
        },
        "acdc3319110d496ab68ac4b0b9e0d638": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3607370f349249d6b8acfd001ff8b198",
            "placeholder": "​",
            "style": "IPY_MODEL_909766e612dd453cbfa79cdf15fbb2be",
            "value": " 456M/456M [00:02&lt;00:00, 183MB/s]"
          }
        },
        "727d64886002405a923d4407af0c4209": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43fef5bc8a9b4d1abda35a54026e44bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06315793acb0496b9fe3946783d10b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d6f61c84b8604421a831513077391927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "32b257cc71514839b256ca7e3830f83f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3607370f349249d6b8acfd001ff8b198": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "909766e612dd453cbfa79cdf15fbb2be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc2fc4c8b2ab410eb95dfa58bd086335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_37eb79cbd8504854b848074be4a59921",
              "IPY_MODEL_f84fab5f290045bbbada5b979cdc3cd5",
              "IPY_MODEL_2016dd4b64694c78a4eb7bc816aed6ad"
            ],
            "layout": "IPY_MODEL_6a293547cc7b47fcb6c0dab36c12c7c7"
          }
        },
        "37eb79cbd8504854b848074be4a59921": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8fe12d941444d04b202ff08e772526f",
            "placeholder": "​",
            "style": "IPY_MODEL_ef3b30f1c661461d9c63dc9561bcb9ef",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "f84fab5f290045bbbada5b979cdc3cd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb82a3d5031942ce84a5c714270036b5",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3793086f93704ad292403ead367c6073",
            "value": 52557
          }
        },
        "2016dd4b64694c78a4eb7bc816aed6ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e80ef0d369f04de199b2352c26b61111",
            "placeholder": "​",
            "style": "IPY_MODEL_987be07332014d6a8c1a27c07d5e7876",
            "value": " 424k/? [00:00&lt;00:00, 19.7MB/s]"
          }
        },
        "6a293547cc7b47fcb6c0dab36c12c7c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8fe12d941444d04b202ff08e772526f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef3b30f1c661461d9c63dc9561bcb9ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb82a3d5031942ce84a5c714270036b5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3793086f93704ad292403ead367c6073": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e80ef0d369f04de199b2352c26b61111": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "987be07332014d6a8c1a27c07d5e7876": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56fa5b786c9547beae467638a6887b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f253de36031a487688b8c3394b8fecb3",
              "IPY_MODEL_950a15c315e54f4a8e7d9f26ee9c72f7",
              "IPY_MODEL_914580895a3a4d3c8bdc2216ed9737a3"
            ],
            "layout": "IPY_MODEL_34aaf234ebba4e4e84d3e49c0f14789d"
          }
        },
        "f253de36031a487688b8c3394b8fecb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cfb7edb43a3d4c22ae587cfc90eab650",
            "placeholder": "​",
            "style": "IPY_MODEL_9e36db42b58b4b8fa1faf1a8310b83a1",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "950a15c315e54f4a8e7d9f26ee9c72f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ea16871ee4a94f68aeab9203eea49451",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6875c961e6b142f7842ba4c5869e2d25",
            "value": 52557
          }
        },
        "914580895a3a4d3c8bdc2216ed9737a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecee372f9da74648b0d002e985f28a72",
            "placeholder": "​",
            "style": "IPY_MODEL_289ef29f74814dc1aac2908cd6759870",
            "value": " 424k/? [00:00&lt;00:00, 24.7MB/s]"
          }
        },
        "34aaf234ebba4e4e84d3e49c0f14789d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cfb7edb43a3d4c22ae587cfc90eab650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e36db42b58b4b8fa1faf1a8310b83a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ea16871ee4a94f68aeab9203eea49451": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6875c961e6b142f7842ba4c5869e2d25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecee372f9da74648b0d002e985f28a72": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "289ef29f74814dc1aac2908cd6759870": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e25e37f7743148909e366968eed138f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_421fd2ada2214240810db5d0e431e451",
              "IPY_MODEL_6233535aaeab483b8f7023b51457d184",
              "IPY_MODEL_4b5b13c7cbab40429f32ddaed90de202"
            ],
            "layout": "IPY_MODEL_54b605a2890f4be1b855797f7efdb598"
          }
        },
        "421fd2ada2214240810db5d0e431e451": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db9dddea903d430184b57088cae426b2",
            "placeholder": "​",
            "style": "IPY_MODEL_998c3e99bb254ff4bb5087c66aee0b95",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "6233535aaeab483b8f7023b51457d184": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68042e42b1e44e65a8627da0aec3f560",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c9a99e66755c4c11a74e253389dd7c9d",
            "value": 52557
          }
        },
        "4b5b13c7cbab40429f32ddaed90de202": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4824d4d85ebb4b13a244df954788e965",
            "placeholder": "​",
            "style": "IPY_MODEL_9560df99c2094f7cb63dd5975c69124f",
            "value": " 424k/? [00:00&lt;00:00, 27.2MB/s]"
          }
        },
        "54b605a2890f4be1b855797f7efdb598": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db9dddea903d430184b57088cae426b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "998c3e99bb254ff4bb5087c66aee0b95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "68042e42b1e44e65a8627da0aec3f560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c9a99e66755c4c11a74e253389dd7c9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4824d4d85ebb4b13a244df954788e965": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9560df99c2094f7cb63dd5975c69124f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "125a82f05f1d446a9acbc905b5920f2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_376f7c94992a4c0d8c30132ff6b60582",
              "IPY_MODEL_bfdd0cd7d31a493bbba82ee1c915c322",
              "IPY_MODEL_cad65ef1c7e04c878a61fdd3738f72f9"
            ],
            "layout": "IPY_MODEL_03858db06c134d53bb64d57ebf41061f"
          }
        },
        "376f7c94992a4c0d8c30132ff6b60582": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6a7275240914264a137e6fd494494af",
            "placeholder": "​",
            "style": "IPY_MODEL_6f5f418350f24b1c93a8d40940e9631a",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "bfdd0cd7d31a493bbba82ee1c915c322": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_45dd27ca7da64abba701bac781ecb2b2",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7292ec3cc67e478c87a78d50cd20210d",
            "value": 52557
          }
        },
        "cad65ef1c7e04c878a61fdd3738f72f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c0e792397b884b94a60ae279896e4744",
            "placeholder": "​",
            "style": "IPY_MODEL_b4665fa2a28041558db4578cd0ccacc6",
            "value": " 424k/? [00:00&lt;00:00, 18.4MB/s]"
          }
        },
        "03858db06c134d53bb64d57ebf41061f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6a7275240914264a137e6fd494494af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f5f418350f24b1c93a8d40940e9631a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "45dd27ca7da64abba701bac781ecb2b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7292ec3cc67e478c87a78d50cd20210d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c0e792397b884b94a60ae279896e4744": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4665fa2a28041558db4578cd0ccacc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d7628c1a518e448cb3b6831288bc118a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8350b5345a924735ba99c6ad08573790",
              "IPY_MODEL_f4aee7509d4843438d5b1dc129c87ba1",
              "IPY_MODEL_80cae6ab93784136bf089d82493e5dc4"
            ],
            "layout": "IPY_MODEL_4eaeabcf6c174d728bb9ecb7cebfdc7e"
          }
        },
        "8350b5345a924735ba99c6ad08573790": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c713c7a97b4f49cf9f1ff5fd57a38c1a",
            "placeholder": "​",
            "style": "IPY_MODEL_7bf31623893a4b5ba6858456abc1b43c",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "f4aee7509d4843438d5b1dc129c87ba1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_895f8bbee858458b8911b61ae1a44f86",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_38ab564917a24a22967b1ff42e6858a6",
            "value": 52557
          }
        },
        "80cae6ab93784136bf089d82493e5dc4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8968f10d49b0492da351dfecff981720",
            "placeholder": "​",
            "style": "IPY_MODEL_f90521b4fed84656becb8e3770be405c",
            "value": " 424k/? [00:00&lt;00:00, 25.3MB/s]"
          }
        },
        "4eaeabcf6c174d728bb9ecb7cebfdc7e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c713c7a97b4f49cf9f1ff5fd57a38c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf31623893a4b5ba6858456abc1b43c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "895f8bbee858458b8911b61ae1a44f86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38ab564917a24a22967b1ff42e6858a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8968f10d49b0492da351dfecff981720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f90521b4fed84656becb8e3770be405c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33c27647f7e74193a37d241be940571b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_462f06ca806e4365a30e2fcb079cf026",
              "IPY_MODEL_ff5adce707d34e948e8aec223b80c23b",
              "IPY_MODEL_7cb541b4f6024f2e966a8e37a5cd093c"
            ],
            "layout": "IPY_MODEL_538ccc648a2f40228e1e588a6249fd87"
          }
        },
        "462f06ca806e4365a30e2fcb079cf026": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6e6b85d44d940848c9d95cc24f814dd",
            "placeholder": "​",
            "style": "IPY_MODEL_660b6030ddfa4acd806517b4a8ba2da9",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "ff5adce707d34e948e8aec223b80c23b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3c37a4607044b4998041e6a55cbba1c",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac5636907393468d90ae5ca40c9b67cb",
            "value": 52557
          }
        },
        "7cb541b4f6024f2e966a8e37a5cd093c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f925dc41b3641a5ad94244732dad5cc",
            "placeholder": "​",
            "style": "IPY_MODEL_43a347010fc745c7a13638d4a53fbf95",
            "value": " 424k/? [00:00&lt;00:00, 20.6MB/s]"
          }
        },
        "538ccc648a2f40228e1e588a6249fd87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6e6b85d44d940848c9d95cc24f814dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "660b6030ddfa4acd806517b4a8ba2da9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3c37a4607044b4998041e6a55cbba1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5636907393468d90ae5ca40c9b67cb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6f925dc41b3641a5ad94244732dad5cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43a347010fc745c7a13638d4a53fbf95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9dd45e1efb824a9fa64eed0a2cb28683": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_734d841bcfe944bfa28d479df00b624b",
              "IPY_MODEL_3269d9ef5be0400da3d8f232c799c7b6",
              "IPY_MODEL_3c5ba092d043479d843c97314af67e4c"
            ],
            "layout": "IPY_MODEL_effb0092e34941188768b5a8765ec472"
          }
        },
        "734d841bcfe944bfa28d479df00b624b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_584be2e248a94313b814df876b1e32e1",
            "placeholder": "​",
            "style": "IPY_MODEL_c3a15d887b38402c8a84769b0d9663cc",
            "value": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: "
          }
        },
        "3269d9ef5be0400da3d8f232c799c7b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3508ede777ae4e11a83ff6e80415a3fd",
            "max": 52557,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9ad5e73b7cef402e96a15f5b36bb4c51",
            "value": 52557
          }
        },
        "3c5ba092d043479d843c97314af67e4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9bb827146b8649719ca3a0961e997ced",
            "placeholder": "​",
            "style": "IPY_MODEL_d026a7d56aac4d549e8c900b8e789975",
            "value": " 424k/? [00:00&lt;00:00, 17.1MB/s]"
          }
        },
        "effb0092e34941188768b5a8765ec472": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "584be2e248a94313b814df876b1e32e1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3a15d887b38402c8a84769b0d9663cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3508ede777ae4e11a83ff6e80415a3fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad5e73b7cef402e96a15f5b36bb4c51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9bb827146b8649719ca3a0961e997ced": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d026a7d56aac4d549e8c900b8e789975": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c90dde9098534fea853d095ccb84eb19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0c1dc55bdcdf4e41b5d013902b8e9d39",
              "IPY_MODEL_403c1e63a4ea437f99d9ec89f11cc0a3",
              "IPY_MODEL_d96bf999477e4ef9b8bc234b98145c8a"
            ],
            "layout": "IPY_MODEL_a5433829f47442fd94c5684af30193aa"
          }
        },
        "0c1dc55bdcdf4e41b5d013902b8e9d39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eae40eb33c5a470c88b258d55b1c813f",
            "placeholder": "​",
            "style": "IPY_MODEL_5a332f3c39294dd1a6e630e98801cc05",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "403c1e63a4ea437f99d9ec89f11cc0a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6ab92969e164615b1ee83b07e712ab6",
            "max": 59,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_698382eaa87749a6bdebfdc5b1b4750b",
            "value": 59
          }
        },
        "d96bf999477e4ef9b8bc234b98145c8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fcbbba22dd4f488fa17c3833095487ef",
            "placeholder": "​",
            "style": "IPY_MODEL_183fd4438f53425fb48e09b774b659f6",
            "value": " 59.0/59.0 [00:00&lt;00:00, 3.90kB/s]"
          }
        },
        "a5433829f47442fd94c5684af30193aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eae40eb33c5a470c88b258d55b1c813f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5a332f3c39294dd1a6e630e98801cc05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6ab92969e164615b1ee83b07e712ab6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "698382eaa87749a6bdebfdc5b1b4750b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "fcbbba22dd4f488fa17c3833095487ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "183fd4438f53425fb48e09b774b659f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4f3a66cfa9284ebaaa9b725a3c441481": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76f6bff5db694fc199a9cac79e9ee059",
              "IPY_MODEL_0adcacf5046248e2a2755ccfae8a81dd",
              "IPY_MODEL_329e85b08c974eab8489919c4f19f779"
            ],
            "layout": "IPY_MODEL_a3e64a9a84374c6aa882c3d392787de4"
          }
        },
        "76f6bff5db694fc199a9cac79e9ee059": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ece0841d4413440882c3f9ade6c96aa5",
            "placeholder": "​",
            "style": "IPY_MODEL_a411a57a8c664ab292ba61db4b1c8b97",
            "value": "config.json: 100%"
          }
        },
        "0adcacf5046248e2a2755ccfae8a81dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f1f94c3f06249a5a5ddba8612a4540f",
            "max": 829,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e62bc58dc5314d8f8ce3d281414f5ef6",
            "value": 829
          }
        },
        "329e85b08c974eab8489919c4f19f779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_29961dda9b2340c2910dae9a7362ef95",
            "placeholder": "​",
            "style": "IPY_MODEL_45f7c233516c44ed92fdf1b827824b5f",
            "value": " 829/829 [00:00&lt;00:00, 78.8kB/s]"
          }
        },
        "a3e64a9a84374c6aa882c3d392787de4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ece0841d4413440882c3f9ade6c96aa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a411a57a8c664ab292ba61db4b1c8b97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f1f94c3f06249a5a5ddba8612a4540f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e62bc58dc5314d8f8ce3d281414f5ef6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "29961dda9b2340c2910dae9a7362ef95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45f7c233516c44ed92fdf1b827824b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52edbf406d994662a0f9b2d502c50e05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_25fb7569935d45c492b1e40f42beb924",
              "IPY_MODEL_4499685d5da4456dba01c7ba715fb2fd",
              "IPY_MODEL_9a2ae8819e1847ad86ca1e2b6854fa63"
            ],
            "layout": "IPY_MODEL_ac7c72e2e80547dc983b8b525959001f"
          }
        },
        "25fb7569935d45c492b1e40f42beb924": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ff9cdc3ea5324b0fb6ea24ac464918af",
            "placeholder": "​",
            "style": "IPY_MODEL_357d96cf6a8b49d6835b3c3561405289",
            "value": "vocab.txt: 100%"
          }
        },
        "4499685d5da4456dba01c7ba715fb2fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3a48d2f8041b4548bd144c53d5388615",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e50f2ccb731f49079f1e5686f044b514",
            "value": 213450
          }
        },
        "9a2ae8819e1847ad86ca1e2b6854fa63": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2355602a97024827b88855f72f8b0999",
            "placeholder": "​",
            "style": "IPY_MODEL_f9073c358088495c8421f0052e52445f",
            "value": " 213k/213k [00:00&lt;00:00, 1.56MB/s]"
          }
        },
        "ac7c72e2e80547dc983b8b525959001f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff9cdc3ea5324b0fb6ea24ac464918af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "357d96cf6a8b49d6835b3c3561405289": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3a48d2f8041b4548bd144c53d5388615": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e50f2ccb731f49079f1e5686f044b514": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2355602a97024827b88855f72f8b0999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9073c358088495c8421f0052e52445f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ba1b64a2f4214ebdb9fd8e6656f20c95": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d7b2bc39f014c9c871bddbd5270a21a",
              "IPY_MODEL_223bf4622ffa492c985d665cf36cc0dd",
              "IPY_MODEL_0260d2c7f71149bd88c35a938eda887d"
            ],
            "layout": "IPY_MODEL_85ff0058b3f842608d587c28b68662c8"
          }
        },
        "5d7b2bc39f014c9c871bddbd5270a21a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5bff531258344601927d91bca35dd246",
            "placeholder": "​",
            "style": "IPY_MODEL_351680af59cd420da6f79816f68b80e4",
            "value": "added_tokens.json: 100%"
          }
        },
        "223bf4622ffa492c985d665cf36cc0dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99173a3ae2024f5982ac860d30c9402d",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a312de900c68435d9a6e2e6ffc7c4936",
            "value": 2
          }
        },
        "0260d2c7f71149bd88c35a938eda887d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9fbf602a21744e188a063083a75ba833",
            "placeholder": "​",
            "style": "IPY_MODEL_130db07b81e64547b13fb78641f1fb31",
            "value": " 2.00/2.00 [00:00&lt;00:00, 60.5B/s]"
          }
        },
        "85ff0058b3f842608d587c28b68662c8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5bff531258344601927d91bca35dd246": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "351680af59cd420da6f79816f68b80e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99173a3ae2024f5982ac860d30c9402d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a312de900c68435d9a6e2e6ffc7c4936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9fbf602a21744e188a063083a75ba833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "130db07b81e64547b13fb78641f1fb31": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7a198d9f0cac418389ef3405d35a3fb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_92d5f9952d9b4688ab646d0f630c67f6",
              "IPY_MODEL_7ec6c714d5834b5f990bbd8d561e1943",
              "IPY_MODEL_a8cb44ae119c44be807fc3acac3921dd"
            ],
            "layout": "IPY_MODEL_5adae7dcf5d54bdb97edd4e549deb2f9"
          }
        },
        "92d5f9952d9b4688ab646d0f630c67f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e9484de7528449058d309e1e0def2999",
            "placeholder": "​",
            "style": "IPY_MODEL_157039af7a8d44299f97f27dd8ccd7f1",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "7ec6c714d5834b5f990bbd8d561e1943": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d92e96f491b442194aa70e8a6b4ccee",
            "max": 112,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_664d04ccfef8460984418f04e71a9007",
            "value": 112
          }
        },
        "a8cb44ae119c44be807fc3acac3921dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c17b7744158a448896aad8812b327bda",
            "placeholder": "​",
            "style": "IPY_MODEL_52931570e243479896f8f791693f9bf8",
            "value": " 112/112 [00:00&lt;00:00, 4.16kB/s]"
          }
        },
        "5adae7dcf5d54bdb97edd4e549deb2f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e9484de7528449058d309e1e0def2999": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "157039af7a8d44299f97f27dd8ccd7f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0d92e96f491b442194aa70e8a6b4ccee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "664d04ccfef8460984418f04e71a9007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c17b7744158a448896aad8812b327bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52931570e243479896f8f791693f9bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dda7a5a2a3c04dc4aed55cc9bc7bd5db": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_19f96645145c46de863d24f62802ee89",
              "IPY_MODEL_45c4627054544a7abe82a232dbbdb270",
              "IPY_MODEL_cb5596c133094bbb9518ba4754fd5844"
            ],
            "layout": "IPY_MODEL_293b590c9d10454f89c619c9ea7a032d"
          }
        },
        "19f96645145c46de863d24f62802ee89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ef39645116f2433dafbdc067f58cb9e2",
            "placeholder": "​",
            "style": "IPY_MODEL_2a4498b52be64694b221a361a7a88b8d",
            "value": "model.safetensors: 100%"
          }
        },
        "45c4627054544a7abe82a232dbbdb270": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bff3ae090b442f68932a09076df7a66",
            "max": 433292294,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_533fce5ea7324727ade621adb06f4855",
            "value": 433292294
          }
        },
        "cb5596c133094bbb9518ba4754fd5844": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e590f0043064a06b7e1bda995b6336d",
            "placeholder": "​",
            "style": "IPY_MODEL_6111b1be06644e06b1e798e23e804a9d",
            "value": " 433M/433M [00:09&lt;00:00, 32.6MB/s]"
          }
        },
        "293b590c9d10454f89c619c9ea7a032d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef39645116f2433dafbdc067f58cb9e2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4498b52be64694b221a361a7a88b8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bff3ae090b442f68932a09076df7a66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533fce5ea7324727ade621adb06f4855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e590f0043064a06b7e1bda995b6336d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6111b1be06644e06b1e798e23e804a9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}